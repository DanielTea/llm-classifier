Date,Repository-Link,Github-Link,Summary,Readme-Text,Classification,Image-Links,Video-Links,Stars,Suitable-Image-Links,Suitable-Video-Links,Repository-Creation-Date
2024-02-25,https://github.com/karpathy/minbpe,https://raw.githubusercontent.com/karpathy/minbpe/master/README.md,"Minbpe offers a streamlined implementation of the Byte Pair Encoding (BPE) algorithm for tokenization in large language models (LLMs), functioning at the byte-level on UTF-8 encoded strings. Made popular by GPT-2 and subsequent models, BPE plays a critical role in modern LLM tokenization processes, including GPT, Llama, and Mistral. The repository includes two versions of the tokenizer capable of training vocabularies, encoding text to tokens, and decoding tokens to text. It features a basic tokenizer directly processing text and a regex tokenizer introducing preprocessing stages for more sophisticated splitting, including handling special tokens in a method akin to GPT-4‚Äôs tokenizer. Additionally, users can train their tokenizer using this code, contrasting with tiktoken which doesn‚Äôt offer training capabilities. The codebase is designed to be clean, comprehensible, and modifiable, supported by thorough comments and examples for user guidance. It also encourages educational engagement through provided exercises and video lectures. Licensing for minbpe is covered under MIT.","# minbpe

Minimal, clean code for the (byte-level) Byte Pair Encoding (BPE) algorithm commonly used in LLM tokenization. The BPE algorithm is ""byte-level"" because it runs on UTF-8 encoded strings.

This algorithm was popularized for LLMs by the [GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) and the associated GPT-2 [code release](https://github.com/openai/gpt-2) from OpenAI. [Sennrich et al. 2015](https://arxiv.org/abs/1508.07909) is cited as the original reference for the use of BPE in NLP applications. Today, all modern LLMs (e.g. GPT, Llama, Mistral) use this algorithm to train their tokenizers.

There are two Tokenizers in this repository, both of which can perform the 3 primary functions of a Tokenizer: 1) train the tokenizer vocabulary and merges on a given text, 2) encode from text to tokens, 3) decode from tokens to text. The files of the repo are as follows:

1. [minbpe/base.py](minbpe/base.py): Implements the `Tokenizer` class, which is the base class. It contains the `train`, `encode`, and `decode` stubs, save/load functionality, and there are also a few common utility functions. This class is not meant to be used directly, but rather to be inherited from.
2. [minbpe/basic.py](minbpe/basic.py): Implements the `BasicTokenizer`, the simplest implementation of the BPE algorithm that runs directly on text.
3. [minbpe/regex.py](minbpe/regex.py): Implements the `RegexTokenizer` that further splits the input text by a regex pattern, which is a preprocessing stage that splits up the input text by categories (think: letters, numbers, punctuation) before tokenization. This ensures that no merges will happen across category boundaries. This was introduced in the GPT-2 paper and continues to be in use as of GPT-4. This class also handles special tokens, if any.
4. [minbpe/gpt4.py](minbpe/gpt4.py): Implements the `GPT4Tokenizer`. This class is a light wrapper around the `RegexTokenizer` (2, above) that exactly reproduces the tokenization of GPT-4 in the [tiktoken](https://github.com/openai/tiktoken) library. The wrapping handles some details around recovering the exact merges in the tokenizer, and the handling of some unfortunate (and likely historical?) 1-byte token permutations.

Finally, the script [train.py](train.py) trains the two major tokenizers on the input text [tests/taylorswift.txt](tests/taylorswift.txt) (this is the Wikipedia entry for her kek) and saves the vocab to disk for visualization. This script runs in about 25 seconds on my (M1) MacBook.

All of the files above are very short and thoroughly commented, and also contain a usage example on the bottom of the file.

## quick start

As the simplest example, we can reproduce the [Wikipedia article on BPE](https://en.wikipedia.org/wiki/Byte_pair_encoding) as follows:

```python
from minbpe import BasicTokenizer
tokenizer = BasicTokenizer()
text = ""aaabdaaabac""
tokenizer.train(text, 256 + 3) # 256 are the byte tokens, then do 3 merges
print(tokenizer.encode(text))
# [258, 100, 258, 97, 99]
print(tokenizer.decode([258, 100, 258, 97, 99]))
# aaabdaaabac
tokenizer.save(""toy"")
# writes two files: toy.model (for loading) and toy.vocab (for viewing)
```

According to Wikipedia, running bpe on the input string: ""aaabdaaabac"" for 3 merges results in the string: ""XdXac"" where  X=ZY, Y=ab, and Z=aa. The tricky thing to note is that minbpe always allocates the 256 individual bytes as tokens, and then merges bytes as needed from there. So for us a=97, b=98, c=99, d=100 (their [ASCII](https://www.asciitable.com) values). Then when (a,a) is merged to Z, Z will become 256. Likewise Y will become 257 and X 258. So we start with the 256 bytes, and do 3 merges to get to the result above, with the expected output of [258, 100, 258, 97, 99].

## inference: GPT-4 comparison

We can verify that the `RegexTokenizer` has feature parity with the GPT-4 tokenizer from [tiktoken](https://github.com/openai/tiktoken) as follows:

```python
text = ""hello123!!!? (ÏïàÎÖïÌïòÏÑ∏Ïöî!) üòâ""

# tiktoken
import tiktoken
enc = tiktoken.get_encoding(""cl100k_base"")
print(enc.encode(text))
# [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037]

# ours
from minbpe import GPT4Tokenizer
tokenizer = GPT4Tokenizer()
print(tokenizer.encode(text))
# [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037]
```

(you'll have to `pip install tiktoken` to run). Under the hood, the `GPT4Tokenizer` is just a light wrapper around `RegexTokenizer`, passing in the merges and the special tokens of GPT-4. We can also ensure the special tokens are handled correctly:

```python
text = ""<|endoftext|>hello world""

# tiktoken
import tiktoken
enc = tiktoken.get_encoding(""cl100k_base"")
print(enc.encode(text, allowed_special=""all""))
# [100257, 15339, 1917]

# ours
from minbpe import GPT4Tokenizer
tokenizer = GPT4Tokenizer()
print(tokenizer.encode(text, allowed_special=""all""))
# [100257, 15339, 1917]
```

Note that just like tiktoken, we have to explicitly declare our intent to use and parse special tokens in the call to encode. Otherwise this can become a major footgun, unintentionally tokenizing attacker-controlled data (e.g. user prompts) with special tokens. The `allowed_special` parameter can be set to ""all"", ""none"", or a list of special tokens to allow.

## training

Unlike tiktoken, this code allows you to train your own tokenizer. In principle and to my knowledge, if you train the `RegexTokenizer` on a large dataset with a vocabulary size of 100K, you would reproduce the GPT-4 tokenizer.

There are two paths you can follow. First, you can decide that you don't want the complexity of splitting and preprocessing text with regex patterns, and you also don't care for special tokens. In that case, reach for the `BasicTokenizer`. You can train it, and then encode and decode for example as follows:

```python
from minbpe import BasicTokenizer
tokenizer = BasicTokenizer()
tokenizer.train(very_long_training_string, vocab_size=4096)
tokenizer.encode(""hello world"") # string -> tokens
tokenizer.decode([1000, 2000, 3000]) # tokens -> string
tokenizer.save(""mymodel"") # writes mymodel.model and mymodel.vocab
tokenizer.load(""mymodel.model"") # loads the model back, the vocab is just for vis
```

If you instead want to follow along with OpenAI did for their text tokenizer, it's a good idea to adopt their approach of using regex pattern to split the text by categories. The GPT-4 pattern is a default with the `RegexTokenizer`, so you'd simple do something like:

```python
from minbpe import RegexTokenizer
tokenizer = RegexTokenizer()
tokenizer.train(very_long_training_string, vocab_size=32768)
tokenizer.encode(""hello world"") # string -> tokens
tokenizer.decode([1000, 2000, 3000]) # tokens -> string
tokenizer.save(""tok32k"") # writes tok32k.model and tok32k.vocab
tokenizer.load(""tok32k.model"") # loads the model back from disk
```

Where, of course, you'd want to change around the vocabulary size depending on the size of your dataset.

**Special tokens**. Finally, you might wish to add special tokens to your tokenizer. Register these using the `register_special_tokens` function. For example if you train with vocab_size of 32768, then the first 256 tokens are raw byte tokens, the next 32768-256 are merge tokens, and after those you can add the special tokens. The last ""real"" merge token will have id of 32767 (vocab_size - 1), so your first special token should come right after that, with an id of exactly 32768. So:

```python
from minbpe import RegexTokenizer
tokenizer = RegexTokenizer()
tokenizer.train(very_long_training_string, vocab_size=32768)
tokenizer.register_special_tokens({""<|endoftext|>"": 32768})
tokenizer.encode(""<|endoftext|>hello world"", allowed_special=""all"")
```

You can of course add more tokens after that as well, as you like. Finally, I'd like to stress that I tried hard to keep the code itself clean, readable and hackable. You should not have feel scared to read the code and understand how it works. The tests are also a nice place to look for more usage examples. That reminds me:

## tests

We use the pytest library for tests. All of them are located in the `tests/` directory. First `pip install pytest` if you haven't already, then:

```bash
$ pytest -v .
```

to run the tests. (-v is verbose, slightly prettier).

## exercise

For those trying to study BPE, here is the advised progression exercise for how you can build your own minbpe step by step. See [exercise.md](exercise.md).

## lecture

I built the code in this repository in this [YouTube video](https://www.youtube.com/watch?v=zduSFxRajkE). You can also find this lecture in text form in [lecture.md](lecture.md).

## todos

- write a more optimized Python version that could run over large files and big vocabs
- write an even more optimized C or Rust version (think through)
- rename GPT4Tokenizer to GPTTokenizer and support GPT-2/GPT-3/GPT-3.5 as well?
- write a LlamaTokenizer similar to GPT4Tokenizer (i.e. attempt sentencepiece equivalent)

## License

MIT
",,,https://www.youtube.com/watch?v=zduSFxRajkE,7050,,,2024-02-16T16:18:15Z
2024-02-25,https://github.com/LargeWorldModel/LWM,https://raw.githubusercontent.com/LargeWorldModel/LWM/main/README.md,"The Large World Model (LWM) is a pioneering general-purpose, multimodal autoregressive model that excels in understanding and generating language, images, and videos. It achieves this by being trained on an extensive dataset of long videos and books, utilizing innovative techniques like RingAttention to handle the enormous context sizes involved. This model sets new standards in neural network context sizes, being able to understand and generate content over long sequences, thereby overcoming significant challenges in memory constraints, computational complexity, and dataset limitations. LWM can retrieve facts with high accuracy, answer questions from lengthy videos, chat using images, and even generate videos and images from textual descriptions. The models, ranging from language-only to vision-language versions, are fully open-sourced, catering to various applications with context sizes up to 1 million tokens. It is optimized for training and inference on TPUs but also supports GPUs. The project underscores a significant leap towards models that can process and understand complex, long-form multimodal data, paving the way for more advanced AI capabilities in understanding both human knowledge and the physical world.","# Large World Model (LWM)

[[Project]](https://largeworldmodel.github.io/)
[[Paper]](https://arxiv.org/abs/2402.08268)
[[Models]](https://huggingface.co/LargeWorldModel)

**Large World Model (LWM)** is a general-purpose large-context multimodal autoregressive model. It is trained on a large dataset of diverse long videos and books using RingAttention, and can perform language, image, and video understanding and generation.


## Approach

<div align=""center"">
  <img src=""./imgs/data.png""/>
</div>

Current language models fall short in understanding aspects of the world not easily described in words, and struggle with complex, long-form tasks. Video sequences offer valuable temporal information absent in language and static images, making them attractive for joint modeling with language. Such models could develop a understanding of both human textual knowledge and the physical world, enabling broader AI capabilities for assisting humans. However, learning from millions of tokens of video and language sequences poses challenges due to memory constraints, computational complexity, and limited datasets. To address these challenges, we curate a large dataset of diverse videos and books, utilize the RingAttention technique to scalably train on long sequences, and gradually increase context size from 4K to 1M tokens. This paper makes the following contributions: (a) Largest context size neural network: We train one of the largest context size transformers on long video and language sequences, setting new benchmarks in difficult retrieval tasks and long video understanding. (b) Solutions for overcoming vision-language training challenges, including using masked sequence packing for mixing different sequence lengths, loss weighting to balance language and vision, and model-generated QA dataset for long sequence chat. (c) A highly-optimized implementation with RingAttention, masked sequence packing, and other key features for training on millions-length multimodal sequences. (d) Fully open-sourced a family of 7B parameter models capable of processing long text documents (LWM-Text, LWM-Text-Chat) and videos (LWM, LWM-Chat) of over 1M tokens.
This work paves the way for training on massive datasets of long video and language to develop understanding of both human knowledge and the multimodal world, and broader capabilities.

## LWM Capabilities

<div align=""center"">
  <img src=""./imgs/single_needle_1M.png""/>
  <p>
  LWM can retrieval facts across 1M context with high accuracy.
  </p>
</div>

<br />

<div align=""center"">
  <img src=""./imgs/long_video_chat_main.png""/>
  <p>
  LWM can answer questions over 1 hour YouTube video.
  </p>
</div>

<br />

<div align=""center"">
  <img src=""./imgs/image_chat.png""/>
  <p>
  LWM can chat with images.
  </p>
</div>

<br />

<div align=""center"">
  <img src=""./imgs/image_video_gen.png""/>
  <p>
  LWM can generate videos and images from text.
  </p>
</div>


## Setup

This codebase is supported on Ubuntu and has not been tested on Windows or macOS. We recommend using TPUs for training and inference, although it is also possible to use GPUs. On TPU, the code is highly optimized with Jax's Pallas and can achieve high MFUs with RingAttention at very large context sizes. On GPU, the code is based on XLA and is not as optimized as it is for TPU.

Install the requirements with:
```
conda create -n lwm python=3.10
pip install -U ""jax[cuda12_pip]==0.4.23"" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
pip install -r requirements.txt
```
or set up TPU VM with:
```
sh tpu_vm_setup.sh
```


## Available models

There are language-only and video-language versions, offering context sizes from 32K, to 128K, 256K and 1M tokens. The vision-language models are available only in Jax, and the language-only models are available in both PyTorch and Jax. Below are the names of the available models and their corresponding context sizes and capabilities:

| Model Name         | Context Size | Language or Vision-Language | Chat or Base | URL                                                                                                                                          |
|--------------------|--------------|-----------------------------|--------------|----------------------------------------------------------------------------------------------------------------------------------------------|
| LWM-Text-Chat-128K | 128K         | Language                    | Chat         | [[Pytorch](https://huggingface.co/LargeWorldModel/LWM-Text-Chat-128K)][[Jax](https://huggingface.co/LargeWorldModel/LWM-Text-Chat-128K-Jax)] |
| LWM-Text-Chat-256K | 256K         | Language                    | Chat         | [[Pytorch](https://huggingface.co/LargeWorldModel/LWM-Text-Chat-256K)][[Jax](https://huggingface.co/LargeWorldModel/LWM-Text-Chat-256K-Jax)] |
| LWM-Text-Chat-512K | 512K         | Language                    | Chat         | [[Pytorch](https://huggingface.co/LargeWorldModel/LWM-Text-Chat-512K)][[Jax](https://huggingface.co/LargeWorldModel/LWM-Text-Chat-512K-Jax)] |
| LWM-Text-Chat-1M   | 1M           | Language                    | Chat         | [[Pytorch](https://huggingface.co/LargeWorldModel/LWM-Text-Chat-1M)][[Jax](https://huggingface.co/LargeWorldModel/LWM-Text-Chat-1M-Jax)]     |
| LWM-Text-128K      | 128K         | Language                    | Base         | [[Pytorch](https://huggingface.co/LargeWorldModel/LWM-Text-128K)][[Jax](https://huggingface.co/LargeWorldModel/LWM-Text-128K-Jax)]           |
| LWM-Text-256K      | 256K         | Language                    | Base         | [[Pytorch](https://huggingface.co/LargeWorldModel/LWM-Text-256K)][[Jax](https://huggingface.co/LargeWorldModel/LWM-Text-256K-Jax)]           |
| LWM-Text-512K      | 512K         | Language                    | Base         | [[Pytorch](https://huggingface.co/LargeWorldModel/LWM-Text-512K)][[Jax](https://huggingface.co/LargeWorldModel/LWM-Text-512K-Jax)]           |
| LWM-Text-1M        | 1M           | Language                    | Base         | [[Pytorch](https://huggingface.co/LargeWorldModel/LWM-Text-1M)][[Jax](https://huggingface.co/LargeWorldModel/LWM-Text-1M-Jax)]               |
| LWM-Chat-32K       | 32K          | Vision-Language             | Chat         | [[Jax](https://huggingface.co/LargeWorldModel/LWM-32K-Jax)]                                                                                  |
| LWM-Chat-128K      | 128K         | Vision-Language             | Chat         | [[Jax](https://huggingface.co/LargeWorldModel/LWM-128K-Jax)]                                                                                 |
| LWM-Chat-1M        | 1M           | Vision-Language             | Chat         | [[Jax](https://huggingface.co/LargeWorldModel/LWM-1M-Jax)]                                                                                   |


## Code structure
Use `scan_query_chunk_size` and `scan_key_chunk_size` to control the block size in blockwise compute of the self-attention. Use `scan_mlp_chunk_size` to control the block size in blockwise compute of the feedforward network. Use `scan_attention=True` and `scan_mlp=True` to enable/disable blockwise compute in the self-attention and feed-forward network. Use `remat_attention` and `remat_mlp` to control the rematerialization policy with `nothing_saveable` recommended.

You can use `mesh_dim=dp, fsdp, tp, sp` to control the degree of parallelism and RingAttention. It is a string of 4 integers separated by commas, representing the number of data parallelism, fully sharded data parallelism, tensor parallelism, and sequence parallelism.
For example, `mesh_dim='1,64,4,1'` means 1 data parallelism, 64 fully sharded data parallelism, 4 tensor parallelism, and 1 sequence parallelism. `mesh_dim='1,1,4,64'` means 1 data parallelism, 1 fully sharded data parallelism, 4 tensor parallelism, and 64 sequence parallelism for RingAttention.


## Running Jax Models
In this section, we provide instructions on how to run each of the provided scripts. For each script, you may need to fill in your own paths and values in the variables described in the beginning of each script. 

To run each of the following scripts, use `bash <script_name>.sh`:
- Language model training: `bash scripts/run_train_text.sh`
- Vision-Language model training: `bash scripts/run_train_vision_text.sh`
- Single Needle Evals (Language Model): `bash scripts/run_eval_needle.sh`
- Multi Needle Evals (Language Model): `bash scripts/run_eval_needle_multi.sh`
- Sampling images (Vision-Language Model): `bash scripts/run_sample_image.sh`
- Sampling videos (Vision-LanguageModel): `bash scripts/run_sample_video.sh`
- Image / Video understanding (Vision-Language Model): `bash scripts/run_vision_chat.sh`

By default the `mesh_dim` argument puts all devices on `tp` (tensor parallelism). For longer sequences, you may want to include `sp`, which is the last dimension in the `mesh_dim`.

When running needle evals, you may need to adjust the `theta` and `max_sequence_length` arguments in the scripts depending on the model. Below shows the correct values for each model.

|                     | LWM-Text-128K /  LWM-Text-Chat-128K | LWM-Text-256K /  LWM-Text-Chat-256K | LWM-Text-512K / LWM-Text-Chat-512K | LWM-Text-1M / LWM-Text-Chat-1M |
|---------------------|:-----------------------------------:|:-----------------------------------:|:----------------------------------:|:------------------------------:|
| theta               |               10000000              |               10000000              |              25000000              |            50000000            |
| max_sequence_length |                131072               |                262144               |               524288               |             1048576            |


An example of filling out a script (`run_sample_video.sh`) is as follows
```bash
#! /bin/bash

export SCRIPT_DIR=""$( cd -- ""$( dirname -- ""${BASH_SOURCE[0]}"" )"" &> /dev/null && pwd )""
export PROJECT_DIR=""$( cd -- ""$( dirname -- ""$SCRIPT_DIR"" )"" &> /dev/null && pwd )""
cd $PROJECT_DIR
export PYTHONPATH=""$PYTHONPATH:$PROJECT_DIR""

export llama_tokenizer_path=""/path/to/ckpt/folder/tokenizer.model""
export vqgan_checkpoint=""/path/to/ckpt/folder/vqgan""
export lwm_checkpoint=""/path/to/ckpt/folder/params""

python3 -u -m lwm.vision_generation \
    --prompt='Fireworks over the city' \
    --output_file='fireworks.mp4' \
    --temperature_image=1.0 \
    --temperature_video=1.0 \
    --top_k_image=8192 \
    --top_k_video=1000 \
    --cfg_scale_image=5.0 \
    --cfg_scale_video=1.0 \
    --vqgan_checkpoint=""$vqgan_checkpoint"" \
    --n_frames=8 \
    --mesh_dim='!1,1,-1,1' \
    --dtype='fp32' \
    --load_llama_config='7b' \
    --update_llama_config=""dict(sample_mode='vision',theta=50000000,max_sequence_length=32768,use_flash_attention=True,scan_attention=False,scan_query_chunk_size=128,scan_key_chunk_size=128,scan_mlp=False,scan_mlp_chunk_size=8192,scan_layers=True)"" \
    --load_checkpoint=""params::$lwm_checkpoint"" \
    --tokenizer.vocab_file=""$llama_tokenizer_path""
read
```


## Needle Haystack Data
Run `python scripts/create_needle_data.py`


## Running PyTorch Models
Only text and text chat models are currently supported for PyTorch inference. PyTorch models can be loaded as Hugging Face `LlamaForCausalLM` models. Run `python scripts/sample_pyt.py` to sample. You may need to separately install `torch`. 


## If you have issues

This is based on the [codebase](https://github.com/lhao499/ring-attention) of BPT and RingAttention, with the necessary features for vision-language training. The training and inference have been tested on both TPUv3 and TPUv4.

If you encounter bugs, please open a GitHub issue!


## Citation

If you use this codebase, or otherwise found our work valuable, please cite:

```
@article{liu2023world,
    title={World Model on Million-Length Video and Language with RingAttention},
    author={Liu, Hao and Yan, Wilson and Zaharia, Matei and Abbeel, Pieter},
    journal={arXiv preprint},
    year={2024},
}
@article{liu2023ring,
    title={Ring Attention with Blockwise Transformers for Near-Infinite Context},
    author={Liu, Hao and Zaharia, Matei and Abbeel, Pieter},
    journal={International Conference on Learning Representations},
    year={2024}
}
@article{liu2023blockwise,
    title={Blockwise Parallel Transformer for Large Context Models},
    author={Liu, Hao and Abbeel, Pieter},
    journal={Advances in neural information processing systems},
    year={2023}
}
```

## License

LWM's code is released under the Apache 2.0 License. See [LICENSE](https://github.com/LargeWorldModel/lwm/blob/main/LICENSE) for further details. The models are released under the Llama-2 license.
",,,,5890,,,2024-02-08T04:16:42Z
2024-02-25,https://github.com/google/magika,https://raw.githubusercontent.com/google/magika/main/README.md,"Magika is an innovative AI-driven tool designed for accurate file type detection, utilizing advanced deep learning techniques. Developed with a Keras model that's both compact (1MB) and efficient, it delivers rapid identification results on minimal computing resources. It excels in precision and recall, achieving over 99% accuracy across a test suite of 1M files and 100+ file types, enhancing security measures for Google services like Gmail and Drive. Magika supports multiple operational modes, including command line and Python API, with provisions for batch processing and high configurability. Its performance is exceptional, maintaining near-constant inference times regardless of file size, thanks to evaluating only a sample of bytes. Additionally, Magika is open source, providing ample opportunities for community involvement in further development and refinement. Users can quickly experiment with Magika through a web demo, accessible online, and find comprehensive documentation and support for varying levels of user expertise.","# Magika

Magika is a novel AI powered file type detection tool that relies on the recent advance of deep learning to provide accurate detection. Under the hood, Magika employs a custom, highly optimized Keras model that only weighs about 1MB, and enables precise file identification within milliseconds, even when running on a single CPU.

In an evaluation with over 1M files and over 100 content types (covering both binary and textual file formats), Magika achieves 99%+ precision and recall. Magika is used at scale to help improve Google users‚Äô safety by routing Gmail, Drive, and Safe Browsing files to the proper security and content policy scanners.


You can try Magika without anything by using our [web demo](https://google.github.io/magika/), which runs locally in your browser!

Here is an example of what Magika command line output look like:
<p align=""center"">
    <img src=""./assets/magika-screenshot.png"" width=""600"">
</p>

For more context you can read our initial [announcement post on Google's OSS blog](https://opensource.googleblog.com/2024/02/magika-ai-powered-fast-and-efficient-file-type-identification.html)


## Highlights

- Available as a Python command line, a Python API, and an experimental TFJS version (which powers our [web demo](https://google.github.io/magika/)).
- Trained on a dataset of over 25M files across more than 100 content types.
- On our evaluation, Magika achieves 99%+ average precision and recall, outperforming existing approaches.
- More than 100 content types (see [full list](./docs/supported_content_types_list.md)).
- After the model is loaded (this is a one-off overhead), the inference time is about 5ms per file.
- Batching: You can pass to the command line and API multiple files at the same time, and Magika will use batching to speed up the inference time. You can invoke Magika with even thousands of files at the same time. You can also use `-r` for recursively scanning a directory.
- Near-constant inference time independently from the file size Magika only uses a limited subset of the file's bytes.
- Magika uses a per-content-type threshold system that determines whether to ""trust"" the prediction for the model, or whether to return a generic label, such as ""Generic text document"" or ""Unknown binary data"".
- Support three different prediction modes, which tweak the tolerance to errors: `high-confidence`, `medium-confidence`, and `best-guess`.
- It's open source! (And more is yet to come.)

For more details, see the documentation for the [python package](./docs/python.md) and for the [js package](./js/README.md) (dev [docs](./docs/js.md)).


## Table of Contents

1. [Getting Started](#getting-started)
    1. [Installation](#installation)
    1. [Running on Docker](#running-in-docker)
    1. [Usage](#usage)
        1. [Python command line](#python-command-line)
        1. [Python API](#python-api)
        1. [Experimental TFJS model & npm package](#experimental-tfjs-model--npm-package)
1. [Development Setup](#development-setup)
1. [Important Documentation](#important-documentation)
1. [Known Limitations & Contributing](#known-limitations--contributing)
1. [Frequently Asked Questions](#frequently-asked-questions)
1. [Additional Resources](#additional-resources)
1. [Citation](#citation)
1. [License](#license)
1. [Disclaimer](#disclaimer)


## Getting Started

### Installation

Magika is available as `magika` on PyPI:

```shell
$ pip install magika
```

### Running in Docker

```
git clone https://github.com/google/magika
cd magika/
docker build -t magika .
docker run -it --rm -v $(pwd):/magika magika -r /magika/tests_data
```

### Usage

#### Python command line

Examples:

```shell
$ magika -r tests_data/
tests_data/README.md: Markdown document (text)
tests_data/basic/code.asm: Assembly (code)
tests_data/basic/code.c: C source (code)
tests_data/basic/code.css: CSS source (code)
tests_data/basic/code.js: JavaScript source (code)
tests_data/basic/code.py: Python source (code)
tests_data/basic/code.rs: Rust source (code)
...
tests_data/mitra/7-zip.7z: 7-zip archive data (archive)
tests_data/mitra/bmp.bmp: BMP image data (image)
tests_data/mitra/bzip2.bz2: bzip2 compressed data (archive)
tests_data/mitra/cab.cab: Microsoft Cabinet archive data (archive)
tests_data/mitra/elf.elf: ELF executable (executable)
tests_data/mitra/flac.flac: FLAC audio bitstream data (audio)
...
```

```shell
$ magika code.py --json
[
    {
        ""path"": ""code.py"",
        ""dl"": {
            ""ct_label"": ""python"",
            ""score"": 0.9940916895866394,
            ""group"": ""code"",
            ""mime_type"": ""text/x-python"",
            ""magic"": ""Python script"",
            ""description"": ""Python source""
        },
        ""output"": {
            ""ct_label"": ""python"",
            ""score"": 0.9940916895866394,
            ""group"": ""code"",
            ""mime_type"": ""text/x-python"",
            ""magic"": ""Python script"",
            ""description"": ""Python source""
        }
    }
]
```

```shell
$ cat doc.ini | magika -
-: INI configuration file (text)
```

```help
$ magika -h
Usage: magika [OPTIONS] [FILE]...

  Magika - Determine type of FILEs with deep-learning.

Options:
  -r, --recursive                 When passing this option, magika scans every
                                  file within directories, instead of
                                  outputting ""directory""
  --json                          Output in JSON format.
  --jsonl                         Output in JSONL format.
  -i, --mime-type                 Output the MIME type instead of a verbose
                                  content type description.
  -l, --label                     Output a simple label instead of a verbose
                                  content type description. Use --list-output-
                                  content-types for the list of supported
                                  output.
  -c, --compatibility-mode        Compatibility mode: output is as close as
                                  possible to `file` and colors are disabled.
  -s, --output-score              Output the prediction score in addition to
                                  the content type.
  -m, --prediction-mode [best-guess|medium-confidence|high-confidence]
  --batch-size INTEGER            How many files to process in one batch.
  --no-dereference                This option causes symlinks not to be
                                  followed. By default, symlinks are
                                  dereferenced.
  --colors / --no-colors          Enable/disable use of colors.
  -v, --verbose                   Enable more verbose output.
  -vv, --debug                    Enable debug logging.
  --generate-report               Generate report useful when reporting
                                  feedback.
  --version                       Print the version and exit.
  --list-output-content-types     Show a list of supported content types.
  --model-dir DIRECTORY           Use a custom model.
  -h, --help                      Show this message and exit.

  Magika version: ""0.5.0""

  Default model: ""standard_v1""

  Send any feedback to magika-dev@google.com or via GitHub issues.
```

See [python documentation](./docs/python.md) for detailed documentation.


#### Python API

Examples:

```python
>>> from magika import Magika
>>> m = Magika()
>>> res = m.identify_bytes(b""# Example\nThis is an example of markdown!"")
>>> print(res.output.ct_label)
markdown
```


See [python documentation](./docs/python.md) for detailed documentation.


#### Experimental TFJS model & npm package

We also provide Magika as an experimental package for people interested in using in a web app.
Note that Magika JS implementation performance is significantly slower and you should expect to spend 100ms+ per file.

See [js documentation](./docs/js.md) for the details.


## Development Setup

We use [poetry](https://python-poetry.org/) for development and packaging:

```shell
$ git clone https://github.com/google/magika
$ cd magika/python
$ poetry shell && poetry install
$ magika -r ../tests_data
```

To run the tests:

```shell
$ cd magika/python
$ poetry shell
$ pytest tests/
```


## Important Documentation

- [Documentation about the CLI](./docs/command_line_interface.md)
- [Documentation about the bindings for different languages](./docs/bindings.md)
- [List of supported content types (for v1, more to come).](./docs/supported_content_types_list.md)
- [Documentation on how to interpret Magika's output.](./docs/magika_output.md)
- [Frequently Asked Questions](./docs/faq.md)


## Known Limitations & Contributing

Magika significantly improves over the state of the art, but there's always room for improvement! More work can be done to increase detection accuracy, support for additional content types, bindings for more languages, etc.

This initial release is not targeting polyglot detection, and we're looking forward to seeing adversarial examples from the community.
We would also love to hear from the community about encountered problems, misdetections, features requests, need for support for additional content types, etc.

Check our open GitHub issues to see what is on our roadmap and please report misdetections or feature requests by either opening GitHub issues (preferred) or by emailing us at magika-dev@google.com.

When reporting misdetections, you may want to use `$ magika --generate-report <path>` to generate a report with debug information, which you can include in your github issue.

**NOTE: Do NOT send reports about files that may contain PII, the report contains (a small) part of the file content!**

See [`CONTRIBUTING.md`](CONTRIBUTING.md) for details.


## Frequently Asked Questions

We have collected a number of FAQs [here](./docs/faq.md).


## Additional Resources

- [Google's OSS blog post](https://opensource.googleblog.com/2024/02/magika-ai-powered-fast-and-efficient-file-type-identification.html) about Magika announcement.
- Web demo: [web demo](https://google.github.io/magika/).


## Citation
If you use this software for your research, please cite it as:
```bibtex
@software{magika,
author = {Fratantonio, Yanick and Bursztein, Elie and Invernizzi, Luca and Zhang, Marina and Metitieri, Giancarlo and Kurt, Thomas and Galilee, Francois and Petit-Bianco, Alexandre and Farah, Loua and Albertini, Ange},
title = {{Magika content-type scanner}},
url = {https://github.com/google/magika}
}
```

## License

Apache 2.0 see [`LICENSE`](LICENSE) for details.

## Disclaimer

This project is not an official Google project. It is not supported by
Google and Google specifically disclaims all warranties as to its quality,
merchantability, or fitness for a particular purpose.
",,,,0,,,
2024-02-25,https://github.com/sherlock-project/sherlock,https://raw.githubusercontent.com/sherlock-project/sherlock/master/README.md,"The text introduces the Sherlock Project, a tool designed for finding social media accounts across various networks by username. It provides detailed instructions on installation, usage, and includes Docker notes for containerized execution. Users can clone the Sherlock repository from GitHub, install requirements via pip, and then use specific commands to search for usernames across social networks. The tool offers a range of options for output formatting and supports the use of proxies and Tor for anonymous searches. Additionally, the text invites contributions from the community for the development of Sherlock, including the addition of new site support and improvement of existing functionalities. Tests for validating contributions are mentioned, along with a call to participate in the project's development. The project is licensed under MIT and credits Siddharth Dushantha as the original creator.","<p align=center>
  <br>
  <a href=""https://sherlock-project.github.io/"" target=""_blank""><img src=""https://user-images.githubusercontent.com/27065646/53551960-ae4dff80-3b3a-11e9-9075-cef786c69364.png""/></a>
  <br>
  <span>Hunt down social media accounts by username across <a href=""https://github.com/sherlock-project/sherlock/blob/master/sites.md"">social networks</a></span>
  <br>
</p>

<p align=""center"">
  <a href=""#installation"">Installation</a>
  &nbsp&nbsp&nbsp|&nbsp&nbsp&nbsp
  <a href=""#usage"">Usage</a>
  &nbsp&nbsp&nbsp|&nbsp&nbsp&nbsp
  <a href=""#docker-notes"">Docker Notes</a>
  &nbsp&nbsp&nbsp|&nbsp&nbsp&nbsp
  <a href=""#contributing"">Contributing</a>
</p>

<p align=""center"">
<img width=""70%"" height=""70%"" src=""https://user-images.githubusercontent.com/27065646/219638267-a5e11090-aa6e-4e77-87f7-0e95f6ad5978.png""/>
</a>
</p>


## Installation

```console
# clone the repo
$ git clone https://github.com/sherlock-project/sherlock.git

# change the working directory to sherlock
$ cd sherlock

# install the requirements
$ python3 -m pip install -r requirements.txt
```

## Usage

```console
$ python3 sherlock --help
usage: sherlock [-h] [--version] [--verbose] [--folderoutput FOLDEROUTPUT]
                [--output OUTPUT] [--tor] [--unique-tor] [--csv]
                [--site SITE_NAME] [--proxy PROXY_URL] [--json JSON_FILE]
                [--timeout TIMEOUT] [--print-all] [--print-found] [--no-color]
                [--browse] [--local] [--nsfw]
                USERNAMES [USERNAMES ...]

Sherlock: Find Usernames Across Social Networks (Version 0.14.3)

positional arguments:
  USERNAMES             One or more usernames to check with social networks.
                        Check similar usernames using {?} (replace to '_', '-', '.').

optional arguments:
  -h, --help            show this help message and exit
  --version             Display version information and dependencies.
  --verbose, -v, -d, --debug
                        Display extra debugging information and metrics.
  --folderoutput FOLDEROUTPUT, -fo FOLDEROUTPUT
                        If using multiple usernames, the output of the results will be
                        saved to this folder.
  --output OUTPUT, -o OUTPUT
                        If using single username, the output of the result will be saved
                        to this file.
  --tor, -t             Make requests over Tor increases runtime requires Tor to be
                        installed and in system path.
  --unique-tor, -u      Make requests over Tor with new Tor circuit after each request
                        increases runtime requires Tor to be installed and in system
                        path.
  --csv                 Create Comma-Separated Values (CSV) File.
  --xlsx                Create the standard file for the modern Microsoft Excel
                        spreadsheet (xslx).
  --site SITE_NAME      Limit analysis to just the listed sites. Add multiple options to
                        specify more than one site.
  --proxy PROXY_URL, -p PROXY_URL
                        Make requests over a proxy. e.g. socks5://127.0.0.1:1080
  --json JSON_FILE, -j JSON_FILE
                        Load data from a JSON file or an online, valid, JSON file.
  --timeout TIMEOUT     Time (in seconds) to wait for response to requests (Default: 60)
  --print-all           Output sites where the username was not found.
  --print-found         Output sites where the username was found.
  --no-color            Don't color terminal output
  --browse, -b          Browse to all results on default browser.
  --local, -l           Force the use of the local data.json file.
  --nsfw                Include checking of NSFW sites from default list.
```

To search for only one user:
```
python3 sherlock user123
```

To search for more than one user:
```
python3 sherlock user1 user2 user3
```

Accounts found will be stored in an individual text file with the corresponding username (e.g ```user123.txt```).

## Anaconda (Windows) Notes

If you are using Anaconda in Windows, using `python3` might not work. Use `python` instead.

## Docker Notes

If docker is installed you can build an image and run this as a container.

```
docker build -t mysherlock-image .
```

Once the image is built, sherlock can be invoked by running the following:

```
docker run --rm -t mysherlock-image user123
```

Use the following command to access the saved results:

```
docker run --rm -t -v ""$PWD/results:/opt/sherlock/results"" mysherlock-image -o /opt/sherlock/results/text.txt user123
```

Docker is instructed to create (or use) the folder `results` in the current working directory and to mount it at `/opt/sherlock/results` on the docker container by using the ```-v ""$PWD/results:/opt/sherlock/results""``` options. `Sherlock` is instructed to export the result using the `-o /opt/sherlock/results/text.txt` option.


### Using `docker-compose`

You can use the `docker-compose.yml` file from the repository and use this command:

```
docker-compose run sherlock -o /opt/sherlock/results/text.txt user123
```

## Contributing
We would love to have you help us with the development of Sherlock. Each and every contribution is greatly valued!

Here are some things we would appreciate your help on:
- Addition of new site support ¬π
- Bringing back site support of [sites that have been removed](removed_sites.md) in the past due to false positives

[1] Please look at the Wiki entry on [adding new sites](https://github.com/sherlock-project/sherlock/wiki/Adding-Sites-To-Sherlock)
to understand the issues.

## Tests

Thank you for contributing to Sherlock!

Before creating a pull request with new development, please run the tests
to ensure that everything is working great.  It would also be a good idea to run the tests
before starting development to distinguish problems between your
environment and the Sherlock software.

The following is an example of the command line to run all the tests for
Sherlock.  This invocation hides the progress text that Sherlock normally
outputs, and instead shows the verbose output of the tests.

```console
$ cd sherlock/sherlock
$ python3 -m unittest tests.all --verbose
```

Note that we do currently have 100% test coverage.  Unfortunately, some of
the sites that Sherlock checks are not always reliable, so it is common
to get response problems.  Any problems in connection will show up as
warnings in the tests instead of true errors.

If some sites are failing due to connection problems (site is down, in maintenance, etc)
you can exclude them from tests by creating a `tests/.excluded_sites` file with a
list of sites to ignore (one site name per line).

## Stargazers over time

[![Stargazers over time](https://starchart.cc/sherlock-project/sherlock.svg)](https://starchart.cc/sherlock-project/sherlock)

## License

MIT ¬© Sherlock Project<br/>
Original Creator - [Siddharth Dushantha](https://github.com/sdushantha)
",,,,0,,,
2024-02-25,https://github.com/chatchat-space/Langchain-Chatchat,https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/README.md,"LangChain-Chatchat, originally named Langchain-ChatGLM, is an open-source, offline-deployable knowledge base project that enhances retrieval generation with large language models such as ChatGLM, utilizing frameworks like Langchain. The project seeks to establish an offline-operable knowledge base Q&A solution friendly to Chinese scenarios and open-source models. Inspired by various sources, including GanymedeNil's document.ai and AlexZhangji's ChatGLM-6B Pull Request, it aims to enable local knowledge base Q&A applications using open-source models. It supports integrating multiple models and databases, emphasizing data security and private deployment for businesses under the Apache License. The latest version, 0.2.10, marks the end of the 0.2.x series, with future efforts focused on developing Langchain-Chatchat 0.3.x for enhanced application. The project has achieved significant milestones, including over 20K GitHub stars and recognition at hackathons. For rapid deployment, it provides a straightforward setup process, including environment setup, model downloading, knowledge base and configuration initialization, and a start-up command. The project encourages deep engagement and contributions through its Wiki. To support various use-cases, it offers Docker images and maintains active engagement through Telegram groups, a WeChat group, and an official WeChat public account for updates and community interaction.","![](img/logo-long-chatchat-trans-v2.png)

üåç [READ THIS IN ENGLISH](README_en.md)
üåç [Êó•Êú¨Ë™û„ÅßË™≠„ÇÄ](README_ja.md)

üìÉ **LangChain-Chatchat** (Âéü Langchain-ChatGLM)

Âü∫‰∫é ChatGLM Á≠âÂ§ßËØ≠Ë®ÄÊ®°Âûã‰∏é Langchain Á≠âÂ∫îÁî®Ê°ÜÊû∂ÂÆûÁé∞ÔºåÂºÄÊ∫ê„ÄÅÂèØÁ¶ªÁ∫øÈÉ®ÁΩ≤ÁöÑÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàê(RAG)Â§ßÊ®°ÂûãÁü•ËØÜÂ∫ìÈ°πÁõÆ„ÄÇ

### ‚ö†Ô∏è ÈáçË¶ÅÊèêÁ§∫

`0.2.10`Â∞Ü‰ºöÊòØ`0.2.x`Á≥ªÂàóÁöÑÊúÄÂêé‰∏Ä‰∏™ÁâàÊú¨Ôºå`0.2.x`Á≥ªÂàóÁâàÊú¨Â∞Ü‰ºöÂÅúÊ≠¢Êõ¥Êñ∞ÂíåÊäÄÊúØÊîØÊåÅÔºåÂÖ®ÂäõÁ†îÂèëÂÖ∑ÊúâÊõ¥Âº∫Â∫îÁî®ÊÄßÁöÑ `Langchain-Chatchat 0.3.x`„ÄÇ
`0.2.10` ÁöÑÂêéÁª≠ bug ‰øÆÂ§çÂ∞Ü‰ºöÁõ¥Êé•Êé®ÈÄÅÂà∞`master`ÂàÜÊîØÔºåËÄå‰∏çÂú®ËøõË°åÁâàÊú¨Êõ¥Êñ∞„ÄÇ

---

## ÁõÆÂΩï

* [‰ªãÁªç](README.md#‰ªãÁªç)
* [Ëß£ÂÜ≥ÁöÑÁóõÁÇπ](README.md#Ëß£ÂÜ≥ÁöÑÁóõÁÇπ)
* [Âø´ÈÄü‰∏äÊâã](README.md#Âø´ÈÄü‰∏äÊâã)
    * [1. ÁéØÂ¢ÉÈÖçÁΩÆ](README.md#1-ÁéØÂ¢ÉÈÖçÁΩÆ)
    * [2. Ê®°Âûã‰∏ãËΩΩ](README.md#2-Ê®°Âûã‰∏ãËΩΩ)
    * [3. ÂàùÂßãÂåñÁü•ËØÜÂ∫ìÂíåÈÖçÁΩÆÊñá‰ª∂](README.md#3-ÂàùÂßãÂåñÁü•ËØÜÂ∫ìÂíåÈÖçÁΩÆÊñá‰ª∂)
    * [4. ‰∏ÄÈîÆÂêØÂä®](README.md#4-‰∏ÄÈîÆÂêØÂä®)
    * [5. ÂêØÂä®ÁïåÈù¢Á§∫‰æã](README.md#5-ÂêØÂä®ÁïåÈù¢Á§∫‰æã)
* [ËÅîÁ≥ªÊàë‰ª¨](README.md#ËÅîÁ≥ªÊàë‰ª¨)

## ‰ªãÁªç

ü§ñÔ∏è ‰∏ÄÁßçÂà©Áî® [langchain](https://github.com/langchain-ai/langchain)
ÊÄùÊÉ≥ÂÆûÁé∞ÁöÑÂü∫‰∫éÊú¨Âú∞Áü•ËØÜÂ∫ìÁöÑÈóÆÁ≠îÂ∫îÁî®ÔºåÁõÆÊ†áÊúüÊúõÂª∫Á´ã‰∏ÄÂ•óÂØπ‰∏≠ÊñáÂú∫ÊôØ‰∏éÂºÄÊ∫êÊ®°ÂûãÊîØÊåÅÂèãÂ•Ω„ÄÅÂèØÁ¶ªÁ∫øËøêË°åÁöÑÁü•ËØÜÂ∫ìÈóÆÁ≠îËß£ÂÜ≥ÊñπÊ°à„ÄÇ

üí° Âèó [GanymedeNil](https://github.com/GanymedeNil) ÁöÑÈ°πÁõÆ [document.ai](https://github.com/GanymedeNil/document.ai)
Âíå [AlexZhangji](https://github.com/AlexZhangji)
ÂàõÂª∫ÁöÑ [ChatGLM-6B Pull Request](https://github.com/THUDM/ChatGLM-6B/pull/216)
ÂêØÂèëÔºåÂª∫Á´ã‰∫ÜÂÖ®ÊµÅÁ®ãÂèØ‰ΩøÁî®ÂºÄÊ∫êÊ®°ÂûãÂÆûÁé∞ÁöÑÊú¨Âú∞Áü•ËØÜÂ∫ìÈóÆÁ≠îÂ∫îÁî®„ÄÇÊú¨È°πÁõÆÁöÑÊúÄÊñ∞ÁâàÊú¨‰∏≠ÈÄöËøá‰ΩøÁî® [FastChat](https://github.com/lm-sys/FastChat)
Êé•ÂÖ• Vicuna, Alpaca, LLaMA, Koala, RWKV Á≠âÊ®°ÂûãÔºå‰æùÊâò‰∫é [langchain](https://github.com/langchain-ai/langchain)
Ê°ÜÊû∂ÊîØÊåÅÈÄöËøáÂü∫‰∫é [FastAPI](https://github.com/tiangolo/fastapi) Êèê‰æõÁöÑ API
Ë∞ÉÁî®ÊúçÂä°ÔºåÊàñ‰ΩøÁî®Âü∫‰∫é [Streamlit](https://github.com/streamlit/streamlit) ÁöÑ WebUI ËøõË°åÊìç‰Ωú„ÄÇ

‚úÖ ‰æùÊâò‰∫éÊú¨È°πÁõÆÊîØÊåÅÁöÑÂºÄÊ∫ê LLM ‰∏é Embedding Ê®°ÂûãÔºåÊú¨È°πÁõÆÂèØÂÆûÁé∞ÂÖ®ÈÉ®‰ΩøÁî®**ÂºÄÊ∫ê**Ê®°Âûã**Á¶ªÁ∫øÁßÅÊúâÈÉ®ÁΩ≤**„ÄÇ‰∏éÊ≠§ÂêåÊó∂ÔºåÊú¨È°πÁõÆ‰πüÊîØÊåÅ
OpenAI GPT API ÁöÑË∞ÉÁî®ÔºåÂπ∂Â∞ÜÂú®ÂêéÁª≠ÊåÅÁª≠Êâ©ÂÖÖÂØπÂêÑÁ±ªÊ®°ÂûãÂèäÊ®°Âûã API ÁöÑÊé•ÂÖ•„ÄÇ

‚õìÔ∏è Êú¨È°πÁõÆÂÆûÁé∞ÂéüÁêÜÂ¶Ç‰∏ãÂõæÊâÄÁ§∫ÔºåËøáÁ®ãÂåÖÊã¨Âä†ËΩΩÊñá‰ª∂ -> ËØªÂèñÊñáÊú¨ -> ÊñáÊú¨ÂàÜÂâ≤ -> ÊñáÊú¨ÂêëÈáèÂåñ -> ÈóÆÂè•ÂêëÈáèÂåñ ->
Âú®ÊñáÊú¨ÂêëÈáè‰∏≠ÂåπÈÖçÂá∫‰∏éÈóÆÂè•ÂêëÈáèÊúÄÁõ∏‰ººÁöÑ `top k`‰∏™ -> ÂåπÈÖçÂá∫ÁöÑÊñáÊú¨‰Ωú‰∏∫‰∏ä‰∏ãÊñáÂíåÈóÆÈ¢ò‰∏ÄËµ∑Ê∑ªÂä†Âà∞ `prompt`‰∏≠ -> Êèê‰∫§Áªô `LLM`ÁîüÊàêÂõûÁ≠î„ÄÇ

üì∫ [ÂéüÁêÜ‰ªãÁªçËßÜÈ¢ë](https://www.bilibili.com/video/BV13M4y1e7cN/?share_source=copy_web&vd_source=e6c5aafe684f30fbe41925d61ca6d514)

![ÂÆûÁé∞ÂéüÁêÜÂõæ](img/langchain+chatglm.png)

‰ªéÊñáÊ°£Â§ÑÁêÜËßíÂ∫¶Êù•ÁúãÔºåÂÆûÁé∞ÊµÅÁ®ãÂ¶Ç‰∏ãÔºö

![ÂÆûÁé∞ÂéüÁêÜÂõæ2](img/langchain+chatglm2.png)

üö© Êú¨È°πÁõÆÊú™Ê∂âÂèäÂæÆË∞É„ÄÅËÆ≠ÁªÉËøáÁ®ãÔºå‰ΩÜÂèØÂà©Áî®ÂæÆË∞ÉÊàñËÆ≠ÁªÉÂØπÊú¨È°πÁõÆÊïàÊûúËøõË°å‰ºòÂåñ„ÄÇ

üåê [AutoDL ÈïúÂÉè](https://www.codewithgpu.com/i/chatchat-space/Langchain-Chatchat/Langchain-Chatchat) ‰∏≠ `0.2.10`

ÁâàÊú¨ÊâÄ‰ΩøÁî®‰ª£Á†ÅÂ∑≤Êõ¥Êñ∞Ëá≥Êú¨È°πÁõÆ `v0.2.10` ÁâàÊú¨„ÄÇ

üê≥ [Docker ÈïúÂÉè](registry.cn-beijing.aliyuncs.com/chatchat/chatchat:0.2.7) Â∑≤ÁªèÊõ¥Êñ∞Âà∞ ```0.2.7``` ÁâàÊú¨„ÄÇ

üå≤ ‰∏ÄË°åÂëΩ‰ª§ËøêË°å Docker Ôºö

```shell
docker run -d --gpus all -p 80:8501 registry.cn-beijing.aliyuncs.com/chatchat/chatchat:0.2.7
```

üß© Êú¨È°πÁõÆÊúâ‰∏Ä‰∏™ÈùûÂ∏∏ÂÆåÊï¥ÁöÑ[Wiki](https://github.com/chatchat-space/Langchain-Chatchat/wiki/) Ôºå READMEÂè™ÊòØ‰∏Ä‰∏™ÁÆÄÂçïÁöÑ‰ªãÁªçÔºå_
_‰ªÖ‰ªÖÊòØÂÖ•Èó®ÊïôÁ®ãÔºåËÉΩÂ§üÂü∫Á°ÄËøêË°å__„ÄÇ
Â¶ÇÊûú‰Ω†ÊÉ≥Ë¶ÅÊõ¥Ê∑±ÂÖ•ÁöÑ‰∫ÜËß£Êú¨È°πÁõÆÔºåÊàñËÄÖÊÉ≥ÂØπÊú¨È°πÁõÆÂÅöÂá∫Ë¥°ÁåÆ„ÄÇËØ∑ÁßªÊ≠• [Wiki](https://github.com/chatchat-space/Langchain-Chatchat/wiki/)
ÁïåÈù¢

## Ëß£ÂÜ≥ÁöÑÁóõÁÇπ

ËØ•È°πÁõÆÊòØ‰∏Ä‰∏™ÂèØ‰ª•ÂÆûÁé∞ __ÂÆåÂÖ®Êú¨Âú∞Âåñ__Êé®ÁêÜÁöÑÁü•ËØÜÂ∫ìÂ¢ûÂº∫ÊñπÊ°à, ÈáçÁÇπËß£ÂÜ≥Êï∞ÊçÆÂÆâÂÖ®‰øùÊä§ÔºåÁßÅÂüüÂåñÈÉ®ÁΩ≤ÁöÑ‰ºÅ‰∏öÁóõÁÇπ„ÄÇ
Êú¨ÂºÄÊ∫êÊñπÊ°àÈááÁî®```Apache License```ÔºåÂèØ‰ª•ÂÖçË¥πÂïÜÁî®ÔºåÊó†ÈúÄ‰ªòË¥π„ÄÇ

Êàë‰ª¨ÊîØÊåÅÂ∏ÇÈù¢‰∏ä‰∏ªÊµÅÁöÑÊú¨Âú∞Â§ßËØ≠Ë®ÄÊ®°ÂûãÂíåEmbeddingÊ®°ÂûãÔºåÊîØÊåÅÂºÄÊ∫êÁöÑÊú¨Âú∞ÂêëÈáèÊï∞ÊçÆÂ∫ì„ÄÇ
ÊîØÊåÅÂàóË°®ËØ¶ËßÅ[Wiki](https://github.com/chatchat-space/Langchain-Chatchat/wiki/)

## Âø´ÈÄü‰∏äÊâã

### 1. ÁéØÂ¢ÉÈÖçÁΩÆ

+ È¶ñÂÖàÔºåÁ°Æ‰øù‰Ω†ÁöÑÊú∫Âô®ÂÆâË£Ö‰∫Ü Python 3.8 - 3.11 (Êàë‰ª¨Âº∫ÁÉàÊé®Ëçê‰ΩøÁî® Python3.11)„ÄÇ

```
$ python --version
Python 3.11.7
```

Êé•ÁùÄÔºåÂàõÂª∫‰∏Ä‰∏™ËôöÊãüÁéØÂ¢ÉÔºåÂπ∂Âú®ËôöÊãüÁéØÂ¢ÉÂÜÖÂÆâË£ÖÈ°πÁõÆÁöÑ‰æùËµñ

```shell

# ÊãâÂèñ‰ªìÂ∫ì
$ git clone https://github.com/chatchat-space/Langchain-Chatchat.git

# ËøõÂÖ•ÁõÆÂΩï
$ cd Langchain-Chatchat

# ÂÆâË£ÖÂÖ®ÈÉ®‰æùËµñ
$ pip install -r requirements.txt 
$ pip install -r requirements_api.txt
$ pip install -r requirements_webui.txt  

# ÈªòËÆ§‰æùËµñÂåÖÊã¨Âü∫Êú¨ËøêË°åÁéØÂ¢ÉÔºàFAISSÂêëÈáèÂ∫ìÔºâ„ÄÇÂ¶ÇÊûúË¶Å‰ΩøÁî® milvus/pg_vector Á≠âÂêëÈáèÂ∫ìÔºåËØ∑Â∞Ü requirements.txt ‰∏≠Áõ∏Â∫î‰æùËµñÂèñÊ∂àÊ≥®ÈáäÂÜçÂÆâË£Ö„ÄÇ
```

ËØ∑Ê≥®ÊÑèÔºåLangChain-Chatchat `0.2.x` Á≥ªÂàóÊòØÈíàÂØπ Langchain `0.0.x` Á≥ªÂàóÁâàÊú¨ÁöÑÔºåÂ¶ÇÊûú‰Ω†‰ΩøÁî®ÁöÑÊòØ Langchain `0.1.x`
Á≥ªÂàóÁâàÊú¨ÔºåÈúÄË¶ÅÈôçÁ∫ßÊÇ®ÁöÑ`Langchain`ÁâàÊú¨„ÄÇ

### 2Ôºå Ê®°Âûã‰∏ãËΩΩ

Â¶ÇÈúÄÂú®Êú¨Âú∞ÊàñÁ¶ªÁ∫øÁéØÂ¢É‰∏ãËøêË°åÊú¨È°πÁõÆÔºåÈúÄË¶ÅÈ¶ñÂÖàÂ∞ÜÈ°πÁõÆÊâÄÈúÄÁöÑÊ®°Âûã‰∏ãËΩΩËá≥Êú¨Âú∞ÔºåÈÄöÂ∏∏ÂºÄÊ∫ê LLM ‰∏é Embedding
Ê®°ÂûãÂèØ‰ª•‰ªé [HuggingFace](https://huggingface.co/models) ‰∏ãËΩΩ„ÄÇ

‰ª•Êú¨È°πÁõÆ‰∏≠ÈªòËÆ§‰ΩøÁî®ÁöÑ LLM Ê®°Âûã [THUDM/ChatGLM3-6B](https://huggingface.co/THUDM/chatglm3-6b) ‰∏é Embedding
Ê®°Âûã [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh) ‰∏∫‰æãÔºö

‰∏ãËΩΩÊ®°ÂûãÈúÄË¶ÅÂÖà[ÂÆâË£Ö Git LFS](https://docs.github.com/zh/repositories/working-with-files/managing-large-files/installing-git-large-file-storage)
ÔºåÁÑ∂ÂêéËøêË°å

```Shell
$ git lfs install
$ git clone https://huggingface.co/THUDM/chatglm3-6b
$ git clone https://huggingface.co/BAAI/bge-large-zh
```

### 3. ÂàùÂßãÂåñÁü•ËØÜÂ∫ìÂíåÈÖçÁΩÆÊñá‰ª∂

ÊåâÁÖß‰∏ãÂàóÊñπÂºèÂàùÂßãÂåñËá™Â∑±ÁöÑÁü•ËØÜÂ∫ìÂíåÁÆÄÂçïÁöÑÂ§çÂà∂ÈÖçÁΩÆÊñá‰ª∂

```shell
$ python copy_config_example.py
$ python init_database.py --recreate-vs
 ```

### 4. ‰∏ÄÈîÆÂêØÂä®

ÊåâÁÖß‰ª•‰∏ãÂëΩ‰ª§ÂêØÂä®È°πÁõÆ

```shell
$ python startup.py -a
```

### 5. ÂêØÂä®ÁïåÈù¢Á§∫‰æã

Â¶ÇÊûúÊ≠£Â∏∏ÂêØÂä®Ôºå‰Ω†Â∞ÜËÉΩÁúãÂà∞‰ª•‰∏ãÁïåÈù¢

1. FastAPI Docs ÁïåÈù¢

![](img/fastapi_docs_026.png)

2. Web UI ÂêØÂä®ÁïåÈù¢Á§∫‰æãÔºö

- Web UI ÂØπËØùÁïåÈù¢Ôºö

![img](img/LLM_success.png)

- Web UI Áü•ËØÜÂ∫ìÁÆ°ÁêÜÈ°µÈù¢Ôºö

![](img/init_knowledge_base.jpg)

### Ê≥®ÊÑè

‰ª•‰∏äÊñπÂºèÂè™ÊòØ‰∏∫‰∫ÜÂø´ÈÄü‰∏äÊâãÔºåÂ¶ÇÊûúÈúÄË¶ÅÊõ¥Â§öÁöÑÂäüËÉΩÂíåËá™ÂÆö‰πâÂêØÂä®ÊñπÂºè
ÔºåËØ∑ÂèÇËÄÉ[Wiki](https://github.com/chatchat-space/Langchain-Chatchat/wiki/)


---

## È°πÁõÆÈáåÁ®ãÁ¢ë

+ `2023Âπ¥4Êúà`: `Langchain-ChatGLM 0.1.0` ÂèëÂ∏ÉÔºåÊîØÊåÅÂü∫‰∫é ChatGLM-6B Ê®°ÂûãÁöÑÊú¨Âú∞Áü•ËØÜÂ∫ìÈóÆÁ≠î„ÄÇ
+ `2023Âπ¥8Êúà`: `Langchain-ChatGLM` ÊîπÂêç‰∏∫ `Langchain-Chatchat`Ôºå`0.2.0` ÂèëÂ∏ÉÔºå‰ΩøÁî® `fastchat` ‰Ωú‰∏∫Ê®°ÂûãÂä†ËΩΩÊñπÊ°àÔºåÊîØÊåÅÊõ¥Â§öÁöÑÊ®°ÂûãÂíåÊï∞ÊçÆÂ∫ì„ÄÇ
+ `2023Âπ¥10Êúà`: `Langchain-Chatchat 0.2.5` ÂèëÂ∏ÉÔºåÊé®Âá∫ Agent ÂÜÖÂÆπÔºåÂºÄÊ∫êÈ°πÁõÆÂú®`Founder Park & Zhipu AI & Zilliz`
  ‰∏æÂäûÁöÑÈªëÂÆ¢È©¨ÊãâÊùæËé∑Âæó‰∏âÁ≠âÂ•ñ„ÄÇ
+ `2023Âπ¥12Êúà`: `Langchain-Chatchat` ÂºÄÊ∫êÈ°πÁõÆËé∑ÂæóË∂ÖËøá **20K** stars.
+ `2024Âπ¥1Êúà`: `LangChain 0.1.x` Êé®Âá∫Ôºå`Langchain-Chatchat 0.2.x` ÂèëÂ∏ÉÁ®≥ÂÆöÁâàÊú¨`0.2.10`
  ÂêéÂ∞ÜÂÅúÊ≠¢Êõ¥Êñ∞ÂíåÊäÄÊúØÊîØÊåÅÔºåÂÖ®ÂäõÁ†îÂèëÂÖ∑ÊúâÊõ¥Âº∫Â∫îÁî®ÊÄßÁöÑ `Langchain-Chatchat 0.3.x`„ÄÇ

+ üî• ËÆ©Êàë‰ª¨‰∏ÄËµ∑ÊúüÂæÖÊú™Êù• Chatchat ÁöÑÊïÖ‰∫ã ¬∑¬∑¬∑

---

## ËÅîÁ≥ªÊàë‰ª¨

### Telegram

[![Telegram](https://img.shields.io/badge/Telegram-2CA5E0?style=for-the-badge&logo=telegram&logoColor=white ""langchain-chatglm"")](https://t.me/+RjliQ3jnJ1YyN2E9)

### È°πÁõÆ‰∫§ÊµÅÁæ§
<img src=""img/qr_code_92.jpg"" alt=""‰∫åÁª¥Á†Å"" width=""300"" />

üéâ Langchain-Chatchat È°πÁõÆÂæÆ‰ø°‰∫§ÊµÅÁæ§ÔºåÂ¶ÇÊûú‰Ω†‰πüÂØπÊú¨È°πÁõÆÊÑüÂÖ¥Ë∂£ÔºåÊ¨¢ËøéÂä†ÂÖ•Áæ§ËÅäÂèÇ‰∏éËÆ®ËÆ∫‰∫§ÊµÅ„ÄÇ

### ÂÖ¨‰ºóÂè∑

<img src=""img/official_wechat_mp_account.png"" alt=""‰∫åÁª¥Á†Å"" width=""300"" />

üéâ Langchain-Chatchat È°πÁõÆÂÆòÊñπÂÖ¨‰ºóÂè∑ÔºåÊ¨¢ËøéÊâ´Á†ÅÂÖ≥Ê≥®„ÄÇ
",,https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/img/logo-long-chatchat-trans-v2.png; https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/img/langchain+chatglm.png; https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/img/langchain+chatglm2.png; https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/img/fastapi_docs_026.png; https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/img/LLM_success.png; https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/img/init_knowledge_base.jpg,,0,https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/img/logo-long-chatchat-trans-v2.png; https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/img/langchain+chatglm.png; https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/img/langchain+chatglm2.png; https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/img/fastapi_docs_026.png; https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/img/LLM_success.png; https://raw.githubusercontent.com/chatchat-space/Langchain-Chatchat/master/img/init_knowledge_base.jpg,,
2024-02-25,https://github.com/lllyasviel/stable-diffusion-webui-forge,https://raw.githubusercontent.com/lllyasviel/stable-diffusion-webui-forge/main/README.md,"Stable Diffusion WebUI Forge enhances the original Stable Diffusion WebUI with a focus on easier development, better resource management, and accelerated inference. Forge is designed to be the counterpart to SD WebUI, similar to Minecraft Forge, offering improvements including significant speed-ups across various GPUs, from common 8GB models to powerful 24GB variants. The introduction of Unet Patcher simplifies the implementation of methods like Self-Attention Guidance and others within approximately 100 lines of code, eliminating the need for complex patching. Forge brings new capabilities, including SVD, Z123, and various masked features while maintaining compatibility with Automatic1111 WebUI. Installation options range from Git for existing users to one-click packages for ease. Forge removes unnecessary UI changes and optimizes backend operations, cancelling old CMD flags in favor of new, simpler model management strategies. Forge enables easy development of extensions, such as ControlNet and video diffusion, enhancing the platform's extensibility. Additionally, Forge supports new samplers and removes the need for certain extensions, promoting an open environment for contributions focused on Forge-specific enhancements.","# Stable Diffusion WebUI Forge

Stable Diffusion WebUI Forge is a platform on top of [Stable Diffusion WebUI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) (based on [Gradio](https://www.gradio.app/)) to make development easier, optimize resource management, and speed up inference.

The name ""Forge"" is inspired from ""Minecraft Forge"". This project is aimed at becoming SD WebUI's Forge.

Compared to original WebUI (for SDXL inference at 1024px), you can expect the below speed-ups:

1. If you use common GPU like 8GB vram, you can expect to get about **30~45% speed up** in inference speed (it/s), the GPU memory peak (in task manager) will drop about 700MB to 1.3GB, the maximum diffusion resolution (that will not OOM) will increase about 2x to 3x, and the maximum diffusion batch size (that will not OOM) will increase about 4x to 6x.

2. If you use less powerful GPU like 6GB vram, you can expect to get about **60~75% speed up** in inference speed (it/s), the GPU memory peak (in task manager) will drop about 800MB to 1.5GB, the maximum diffusion resolution (that will not OOM) will increase about 3x, and the maximum diffusion batch size (that will not OOM) will increase about 4x.

3. If you use powerful GPU like 4090 with 24GB vram, you can expect to get about **3~6% speed up** in inference speed (it/s), the GPU memory peak (in task manager) will drop about 1GB to 1.4GB, the maximum diffusion resolution (that will not OOM) will increase about 1.6x, and the maximum diffusion batch size (that will not OOM) will increase about 2x.

4. If you use ControlNet for SDXL, the maximum ControlNet count (that will not OOM) will increase about 2x, the speed with SDXL+ControlNet will **speed up about 30~45%**.

Another very important change that Forge brings is **Unet Patcher**. Using Unet Patcher, methods like Self-Attention Guidance, Kohya High Res Fix, FreeU, StyleAlign, Hypertile can all be implemented in about 100 lines of codes. 

Thanks to Unet Patcher, many new things are possible now and supported in Forge, including SVD, Z123, masked Ip-adapter, masked controlnet, photomaker, etc.

**No need to monkeypatch UNet and conflict other extensions anymore!**

Forge also adds a few samplers, including but not limited to DDPM, DDPM Karras, DPM++ 2M Turbo, DPM++ 2M SDE Turbo, LCM Karras, Euler A Turbo, etc. (LCM is already in original webui since 1.7.0).

Finally, Forge promise that we will only do our jobs. Forge will never add unnecessary opinioned changes to the user interface. You are still using 100% Automatic1111 WebUI.

# Installing Forge

If you are proficient in Git and you want to install Forge as another branch of SD-WebUI, please see [here](https://github.com/continue-revolution/sd-webui-animatediff/blob/forge/master/docs/how-to-use.md#you-have-a1111-and-you-know-git). In this way, you can reuse all SD checkpoints and all extensions you installed previously in your OG SD-WebUI, but you should know what you are doing.

If you know what you are doing, you can install Forge using same method as SD-WebUI. (Install Git, Python, Git Clone the forge repo `https://github.com/lllyasviel/stable-diffusion-webui-forge.git` and then run webui-user.bat).

**Or you can just use this one-click installation package (with git and python included).**

[>>> Click Here to Download One-Click Package<<<](https://github.com/lllyasviel/stable-diffusion-webui-forge/releases/download/latest/webui_forge_cu121_torch21.7z)

After you download, you uncompress, use `update.bat` to update, and use `run.bat` to run.

Note that running `update.bat` is important, otherwise you may be using a previous version with potential bugs unfixed.

![image](https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/c49bd60d-82bd-4086-9859-88d472582b94)

# Screenshots of Comparison

I tested with several devices, and this is a typical result from 8GB VRAM (3070ti laptop) with SDXL.

**This is original WebUI:**

![image](https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/16893937-9ed9-4f8e-b960-70cd5d1e288f)

![image](https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/7bbc16fe-64ef-49e2-a595-d91bb658bd94)

![image](https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/de1747fd-47bc-482d-a5c6-0728dd475943)

![image](https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/96e5e171-2d74-41ba-9dcc-11bf68be7e16)

(average about 7.4GB/8GB, peak at about 7.9GB/8GB)

**This is WebUI Forge:**

![image](https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/ca5e05ed-bd86-4ced-8662-f41034648e8c)

![image](https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/3629ee36-4a99-4d9b-b371-12efb260a283)

![image](https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/6d13ebb7-c30d-4aa8-9242-c0b5a1af8c95)

![image](https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/c4f723c3-6ea7-4539-980b-0708ed2a69aa)

(average and peak are all 6.3GB/8GB)

You can see that Forge does not change WebUI results. Installing Forge is not a seed breaking change. 

Forge can perfectly keep WebUI unchanged even for most complicated prompts like `fantasy landscape with a [mountain:lake:0.25] and [an oak:a christmas tree:0.75][ in foreground::0.6][ in background:0.25] [shoddy:masterful:0.5]`.

All your previous works still work in Forge!

# Forge Backend

Forge backend removes all WebUI's codes related to resource management and reworked everything. All previous CMD flags like `medvram, lowvram, medvram-sdxl, precision full, no half, no half vae, attention_xxx, upcast unet`, ... are all **REMOVED**. Adding these flags will not cause error but they will not do anything now. **We highly encourage Forge users to remove all cmd flags and let Forge to decide how to load models.**

Without any cmd flag, Forge can run SDXL with 4GB vram and SD1.5 with 2GB vram.

**Some flags that you may still pay attention to:** 

1. `--always-offload-from-vram` (This flag will make things **slower** but less risky). This option will let Forge always unload models from VRAM. This can be useful if you use multiple software together and want Forge to use less VRAM and give some VRAM to other software, or when you are using some old extensions that will compete vram with Forge, or (very rarely) when you get OOM.

2. `--cuda-malloc` (This flag will make things **faster** but more risky). This will ask pytorch to use *cudaMallocAsync* for tensor malloc. On some profilers I can observe performance gain at millisecond level, but the real speed up on most my devices are often unnoticed (about or less than 0.1 second per image). This cannot be set as default because many users reported issues that the async malloc will crash the program. Users need to enable this cmd flag at their own risk.

3. `--cuda-stream` (This flag will make things **faster** but more risky). This will use pytorch CUDA streams (a special type of thread on GPU) to move models and compute tensors simultaneously. This can almost eliminate all model moving time, and speed up SDXL on 30XX/40XX devices with small VRAM (eg, RTX 4050 6GB, RTX 3060 Laptop 6GB, etc) by about 15\% to 25\%. However, this unfortunately cannot be set as default because I observe higher possibility of pure black images (Nan outputs) on 2060, and higher chance of OOM on 1080 and 2060. When the resolution is large, there is a chance that the computation time of one single attention layer is longer than the time for moving entire model to GPU. When that happens, the next attention layer will OOM since the GPU is filled with the entire model, and no remaining space is available for computing another attention layer. Most overhead detecting methods are not robust enough to be reliable on old devices (in my tests). Users need to enable this cmd flag at their own risk.

4. `--pin-shared-memory` (This flag will make things **faster** but more risky). Effective only when used together with `--cuda-stream`. This will offload modules to Shared GPU Memory instead of system RAM when offloading models. On some 30XX/40XX devices with small VRAM (eg, RTX 4050 6GB, RTX 3060 Laptop 6GB, etc), I can observe significant (at least 20\%) speed-up for SDXL. However, this unfortunately cannot be set as default because the OOM of Shared GPU Memory is a much more severe problem than common GPU memory OOM. Pytorch does not provide any robust method to unload or detect Shared GPU Memory. Once the Shared GPU Memory OOM, the entire program will crash (observed with SDXL on GTX 1060/1050/1066), and there is no dynamic method to prevent or recover from the crash. Users need to enable this cmd flag at their own risk.

If you really want to play with cmd flags, you can additionally control the GPU with:

(extreme VRAM cases)

    --always-gpu
    --always-cpu

(rare attention cases)

    --attention-split
    --attention-quad
    --attention-pytorch
    --disable-xformers
    --disable-attention-upcast

(float point type)

    --all-in-fp32
    --all-in-fp16
    --unet-in-bf16
    --unet-in-fp16
    --unet-in-fp8-e4m3fn
    --unet-in-fp8-e5m2
    --vae-in-fp16
    --vae-in-fp32
    --vae-in-bf16
    --clip-in-fp8-e4m3fn
    --clip-in-fp8-e5m2
    --clip-in-fp16
    --clip-in-fp32

(rare platforms)

    --directml
    --disable-ipex-hijack
    --pytorch-deterministic

Again, Forge do not recommend users to use any cmd flags unless you are very sure that you really need these.

# UNet Patcher

Note that [Forge does not use any other software as backend](https://github.com/lllyasviel/stable-diffusion-webui-forge/discussions/169). The full name of the backend is `Stable Diffusion WebUI with Forge backend`, or for simplicity, the `Forge backend`. The API and python symbols are made similar to previous software only for reducing the learning cost of developers.

Now developing an extension is super simple. We finally have a patchable UNet.

Below is using one single file with 80 lines of codes to support FreeU:

`extensions-builtin/sd_forge_freeu/scripts/forge_freeu.py`

```python
import torch
import gradio as gr
from modules import scripts


def Fourier_filter(x, threshold, scale):
    x_freq = torch.fft.fftn(x.float(), dim=(-2, -1))
    x_freq = torch.fft.fftshift(x_freq, dim=(-2, -1))
    B, C, H, W = x_freq.shape
    mask = torch.ones((B, C, H, W), device=x.device)
    crow, ccol = H // 2, W //2
    mask[..., crow - threshold:crow + threshold, ccol - threshold:ccol + threshold] = scale
    x_freq = x_freq * mask
    x_freq = torch.fft.ifftshift(x_freq, dim=(-2, -1))
    x_filtered = torch.fft.ifftn(x_freq, dim=(-2, -1)).real
    return x_filtered.to(x.dtype)


def set_freeu_v2_patch(model, b1, b2, s1, s2):
    model_channels = model.model.model_config.unet_config[""model_channels""]
    scale_dict = {model_channels * 4: (b1, s1), model_channels * 2: (b2, s2)}

    def output_block_patch(h, hsp, *args, **kwargs):
        scale = scale_dict.get(h.shape[1], None)
        if scale is not None:
            hidden_mean = h.mean(1).unsqueeze(1)
            B = hidden_mean.shape[0]
            hidden_max, _ = torch.max(hidden_mean.view(B, -1), dim=-1, keepdim=True)
            hidden_min, _ = torch.min(hidden_mean.view(B, -1), dim=-1, keepdim=True)
            hidden_mean = (hidden_mean - hidden_min.unsqueeze(2).unsqueeze(3)) / \
                          (hidden_max - hidden_min).unsqueeze(2).unsqueeze(3)
            h[:, :h.shape[1] // 2] = h[:, :h.shape[1] // 2] * ((scale[0] - 1) * hidden_mean + 1)
            hsp = Fourier_filter(hsp, threshold=1, scale=scale[1])
        return h, hsp

    m = model.clone()
    m.set_model_output_block_patch(output_block_patch)
    return m


class FreeUForForge(scripts.Script):
    def title(self):
        return ""FreeU Integrated""

    def show(self, is_img2img):
        # make this extension visible in both txt2img and img2img tab.
        return scripts.AlwaysVisible

    def ui(self, *args, **kwargs):
        with gr.Accordion(open=False, label=self.title()):
            freeu_enabled = gr.Checkbox(label='Enabled', value=False)
            freeu_b1 = gr.Slider(label='B1', minimum=0, maximum=2, step=0.01, value=1.01)
            freeu_b2 = gr.Slider(label='B2', minimum=0, maximum=2, step=0.01, value=1.02)
            freeu_s1 = gr.Slider(label='S1', minimum=0, maximum=4, step=0.01, value=0.99)
            freeu_s2 = gr.Slider(label='S2', minimum=0, maximum=4, step=0.01, value=0.95)

        return freeu_enabled, freeu_b1, freeu_b2, freeu_s1, freeu_s2

    def process_before_every_sampling(self, p, *script_args, **kwargs):
        # This will be called before every sampling.
        # If you use highres fix, this will be called twice.
        
        freeu_enabled, freeu_b1, freeu_b2, freeu_s1, freeu_s2 = script_args

        if not freeu_enabled:
            return

        unet = p.sd_model.forge_objects.unet

        unet = set_freeu_v2_patch(unet, freeu_b1, freeu_b2, freeu_s1, freeu_s2)

        p.sd_model.forge_objects.unet = unet

        # Below codes will add some logs to the texts below the image outputs on UI.
        # The extra_generation_params does not influence results.
        p.extra_generation_params.update(dict(
            freeu_enabled=freeu_enabled,
            freeu_b1=freeu_b1,
            freeu_b2=freeu_b2,
            freeu_s1=freeu_s1,
            freeu_s2=freeu_s2,
        ))

        return
```

It looks like this:

![image](https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/277bac6e-5ea7-4bff-b71a-e55a60cfc03c)

Similar components like HyperTile, KohyaHighResFix, SAG, can all be implemented within 100 lines of codes (see also the codes).

![image](https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/06472b03-b833-4816-ab47-70712ac024d3)

ControlNets can finally be called by different extensions.

Implementing Stable Video Diffusion and Zero123 are also super simple now (see also the codes). 

*Stable Video Diffusion:*

`extensions-builtin/sd_forge_svd/scripts/forge_svd.py`

```python
import torch
import gradio as gr
import os
import pathlib

from modules import script_callbacks
from modules.paths import models_path
from modules.ui_common import ToolButton, refresh_symbol
from modules import shared

from modules_forge.forge_util import numpy_to_pytorch, pytorch_to_numpy
from ldm_patched.modules.sd import load_checkpoint_guess_config
from ldm_patched.contrib.external_video_model import VideoLinearCFGGuidance, SVD_img2vid_Conditioning
from ldm_patched.contrib.external import KSampler, VAEDecode


opVideoLinearCFGGuidance = VideoLinearCFGGuidance()
opSVD_img2vid_Conditioning = SVD_img2vid_Conditioning()
opKSampler = KSampler()
opVAEDecode = VAEDecode()

svd_root = os.path.join(models_path, 'svd')
os.makedirs(svd_root, exist_ok=True)
svd_filenames = []


def update_svd_filenames():
    global svd_filenames
    svd_filenames = [
        pathlib.Path(x).name for x in
        shared.walk_files(svd_root, allowed_extensions=["".pt"", "".ckpt"", "".safetensors""])
    ]
    return svd_filenames


@torch.inference_mode()
@torch.no_grad()
def predict(filename, width, height, video_frames, motion_bucket_id, fps, augmentation_level,
            sampling_seed, sampling_steps, sampling_cfg, sampling_sampler_name, sampling_scheduler,
            sampling_denoise, guidance_min_cfg, input_image):
    filename = os.path.join(svd_root, filename)
    model_raw, _, vae, clip_vision = \
        load_checkpoint_guess_config(filename, output_vae=True, output_clip=False, output_clipvision=True)
    model = opVideoLinearCFGGuidance.patch(model_raw, guidance_min_cfg)[0]
    init_image = numpy_to_pytorch(input_image)
    positive, negative, latent_image = opSVD_img2vid_Conditioning.encode(
        clip_vision, init_image, vae, width, height, video_frames, motion_bucket_id, fps, augmentation_level)
    output_latent = opKSampler.sample(model, sampling_seed, sampling_steps, sampling_cfg,
                                      sampling_sampler_name, sampling_scheduler, positive,
                                      negative, latent_image, sampling_denoise)[0]
    output_pixels = opVAEDecode.decode(vae, output_latent)[0]
    outputs = pytorch_to_numpy(output_pixels)
    return outputs


def on_ui_tabs():
    with gr.Blocks() as svd_block:
        with gr.Row():
            with gr.Column():
                input_image = gr.Image(label='Input Image', source='upload', type='numpy', height=400)

                with gr.Row():
                    filename = gr.Dropdown(label=""SVD Checkpoint Filename"",
                                           choices=svd_filenames,
                                           value=svd_filenames[0] if len(svd_filenames) > 0 else None)
                    refresh_button = ToolButton(value=refresh_symbol, tooltip=""Refresh"")
                    refresh_button.click(
                        fn=lambda: gr.update(choices=update_svd_filenames),
                        inputs=[], outputs=filename)

                width = gr.Slider(label='Width', minimum=16, maximum=8192, step=8, value=1024)
                height = gr.Slider(label='Height', minimum=16, maximum=8192, step=8, value=576)
                video_frames = gr.Slider(label='Video Frames', minimum=1, maximum=4096, step=1, value=14)
                motion_bucket_id = gr.Slider(label='Motion Bucket Id', minimum=1, maximum=1023, step=1, value=127)
                fps = gr.Slider(label='Fps', minimum=1, maximum=1024, step=1, value=6)
                augmentation_level = gr.Slider(label='Augmentation Level', minimum=0.0, maximum=10.0, step=0.01,
                                               value=0.0)
                sampling_steps = gr.Slider(label='Sampling Steps', minimum=1, maximum=200, step=1, value=20)
                sampling_cfg = gr.Slider(label='CFG Scale', minimum=0.0, maximum=50.0, step=0.1, value=2.5)
                sampling_denoise = gr.Slider(label='Sampling Denoise', minimum=0.0, maximum=1.0, step=0.01, value=1.0)
                guidance_min_cfg = gr.Slider(label='Guidance Min Cfg', minimum=0.0, maximum=100.0, step=0.5, value=1.0)
                sampling_sampler_name = gr.Radio(label='Sampler Name',
                                                 choices=['euler', 'euler_ancestral', 'heun', 'heunpp2', 'dpm_2',
                                                          'dpm_2_ancestral', 'lms', 'dpm_fast', 'dpm_adaptive',
                                                          'dpmpp_2s_ancestral', 'dpmpp_sde', 'dpmpp_sde_gpu',
                                                          'dpmpp_2m', 'dpmpp_2m_sde', 'dpmpp_2m_sde_gpu',
                                                          'dpmpp_3m_sde', 'dpmpp_3m_sde_gpu', 'ddpm', 'lcm', 'ddim',
                                                          'uni_pc', 'uni_pc_bh2'], value='euler')
                sampling_scheduler = gr.Radio(label='Scheduler',
                                              choices=['normal', 'karras', 'exponential', 'sgm_uniform', 'simple',
                                                       'ddim_uniform'], value='karras')
                sampling_seed = gr.Number(label='Seed', value=12345, precision=0)

                generate_button = gr.Button(value=""Generate"")

                ctrls = [filename, width, height, video_frames, motion_bucket_id, fps, augmentation_level,
                         sampling_seed, sampling_steps, sampling_cfg, sampling_sampler_name, sampling_scheduler,
                         sampling_denoise, guidance_min_cfg, input_image]

            with gr.Column():
                output_gallery = gr.Gallery(label='Gallery', show_label=False, object_fit='contain',
                                            visible=True, height=1024, columns=4)

        generate_button.click(predict, inputs=ctrls, outputs=[output_gallery])
    return [(svd_block, ""SVD"", ""svd"")]


update_svd_filenames()
script_callbacks.on_ui_tabs(on_ui_tabs)
```

Note that although the above codes look like independent codes, they actually will automatically offload/unload any other models. For example, below is me opening webui, load SDXL, generated an image, then go to SVD, then generated image frames. You can see that the GPU memory is perfectly managed and the SDXL is moved to RAM then SVD is moved to GPU. 

Note that this management is fully automatic. This makes writing extensions super simple.

![image](https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/de1a2d05-344a-44d7-bab8-9ecc0a58a8d3)

![image](https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/14bcefcf-599f-42c3-bce9-3fd5e428dd91)

Similarly, Zero123:

![image](https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/7685019c-7239-47fb-9cb5-2b7b33943285)

### Write a simple ControlNet:

Below is a simple extension to have a completely independent pass of ControlNet that never conflicts any other extensions:

`extensions-builtin/sd_forge_controlnet_example/scripts/sd_forge_controlnet_example.py`

Note that this extension is hidden because it is only for developers. To see it in UI, use `--show-controlnet-example`.

The memory optimization in this example is fully automatic. You do not need to care about memory and inference speed, but you may want to cache objects if you wish.

```python
# Use --show-controlnet-example to see this extension.

import cv2
import gradio as gr
import torch

from modules import scripts
from modules.shared_cmd_options import cmd_opts
from modules_forge.shared import supported_preprocessors
from modules.modelloader import load_file_from_url
from ldm_patched.modules.controlnet import load_controlnet
from modules_forge.controlnet import apply_controlnet_advanced
from modules_forge.forge_util import numpy_to_pytorch
from modules_forge.shared import controlnet_dir


class ControlNetExampleForge(scripts.Script):
    model = None

    def title(self):
        return ""ControlNet Example for Developers""

    def show(self, is_img2img):
        # make this extension visible in both txt2img and img2img tab.
        return scripts.AlwaysVisible

    def ui(self, *args, **kwargs):
        with gr.Accordion(open=False, label=self.title()):
            gr.HTML('This is an example controlnet extension for developers.')
            gr.HTML('You see this extension because you used --show-controlnet-example')
            input_image = gr.Image(source='upload', type='numpy')
            funny_slider = gr.Slider(label='This slider does nothing. It just shows you how to transfer parameters.',
                                     minimum=0.0, maximum=1.0, value=0.5)

        return input_image, funny_slider

    def process(self, p, *script_args, **kwargs):
        input_image, funny_slider = script_args

        # This slider does nothing. It just shows you how to transfer parameters.
        del funny_slider

        if input_image is None:
            return

        # controlnet_canny_path = load_file_from_url(
        #     url='https://huggingface.co/lllyasviel/sd_control_collection/resolve/main/sai_xl_canny_256lora.safetensors',
        #     model_dir=model_dir,
        #     file_name='sai_xl_canny_256lora.safetensors'
        # )
        controlnet_canny_path = load_file_from_url(
            url='https://huggingface.co/lllyasviel/fav_models/resolve/main/fav/control_v11p_sd15_canny_fp16.safetensors',
            model_dir=controlnet_dir,
            file_name='control_v11p_sd15_canny_fp16.safetensors'
        )
        print('The model [control_v11p_sd15_canny_fp16.safetensors] download finished.')

        self.model = load_controlnet(controlnet_canny_path)
        print('Controlnet loaded.')

        return

    def process_before_every_sampling(self, p, *script_args, **kwargs):
        # This will be called before every sampling.
        # If you use highres fix, this will be called twice.

        input_image, funny_slider = script_args

        if input_image is None or self.model is None:
            return

        B, C, H, W = kwargs['noise'].shape  # latent_shape
        height = H * 8
        width = W * 8
        batch_size = p.batch_size

        preprocessor = supported_preprocessors['canny']

        # detect control at certain resolution
        control_image = preprocessor(
            input_image, resolution=512, slider_1=100, slider_2=200, slider_3=None)

        # here we just use nearest neighbour to align input shape.
        # You may want crop and resize, or crop and fill, or others.
        control_image = cv2.resize(
            control_image, (width, height), interpolation=cv2.INTER_NEAREST)

        # Output preprocessor result. Now called every sampling. Cache in your own way.
        p.extra_result_images.append(control_image)

        print('Preprocessor Canny finished.')

        control_image_bchw = numpy_to_pytorch(control_image).movedim(-1, 1)

        unet = p.sd_model.forge_objects.unet

        # Unet has input, middle, output blocks, and we can give different weights
        # to each layers in all blocks.
        # Below is an example for stronger control in middle block.
        # This is helpful for some high-res fix passes. (p.is_hr_pass)
        positive_advanced_weighting = {
            'input': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2],
            'middle': [1.0],
            'output': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2]
        }
        negative_advanced_weighting = {
            'input': [0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95, 1.05, 1.15, 1.25],
            'middle': [1.05],
            'output': [0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95, 1.05, 1.15, 1.25]
        }

        # The advanced_frame_weighting is a weight applied to each image in a batch.
        # The length of this list must be same with batch size
        # For example, if batch size is 5, the below list is [0.2, 0.4, 0.6, 0.8, 1.0]
        # If you view the 5 images as 5 frames in a video, this will lead to
        # progressively stronger control over time.
        advanced_frame_weighting = [float(i + 1) / float(batch_size) for i in range(batch_size)]

        # The advanced_sigma_weighting allows you to dynamically compute control
        # weights given diffusion timestep (sigma).
        # For example below code can softly make beginning steps stronger than ending steps.
        sigma_max = unet.model.model_sampling.sigma_max
        sigma_min = unet.model.model_sampling.sigma_min
        advanced_sigma_weighting = lambda s: (s - sigma_min) / (sigma_max - sigma_min)

        # You can even input a tensor to mask all control injections
        # The mask will be automatically resized during inference in UNet.
        # The size should be B 1 H W and the H and W are not important
        # because they will be resized automatically
        advanced_mask_weighting = torch.ones(size=(1, 1, 512, 512))

        # But in this simple example we do not use them
        positive_advanced_weighting = None
        negative_advanced_weighting = None
        advanced_frame_weighting = None
        advanced_sigma_weighting = None
        advanced_mask_weighting = None

        unet = apply_controlnet_advanced(unet=unet, controlnet=self.model, image_bchw=control_image_bchw,
                                         strength=0.6, start_percent=0.0, end_percent=0.8,
                                         positive_advanced_weighting=positive_advanced_weighting,
                                         negative_advanced_weighting=negative_advanced_weighting,
                                         advanced_frame_weighting=advanced_frame_weighting,
                                         advanced_sigma_weighting=advanced_sigma_weighting,
                                         advanced_mask_weighting=advanced_mask_weighting)

        p.sd_model.forge_objects.unet = unet

        # Below codes will add some logs to the texts below the image outputs on UI.
        # The extra_generation_params does not influence results.
        p.extra_generation_params.update(dict(
            controlnet_info='You should see these texts below output images!',
        ))

        return


# Use --show-controlnet-example to see this extension.
if not cmd_opts.show_controlnet_example:
    del ControlNetExampleForge

```

![image](https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/822fa2fc-c9f4-4f58-8669-4b6680b91063)


### Add a preprocessor

Below is the full codes to add a normalbae preprocessor with perfect memory managements.

You can use arbitrary independent extensions to add a preprocessor.

Your preprocessor will be read by all other extensions using `modules_forge.shared.preprocessors`

Below codes are in `extensions-builtin\forge_preprocessor_normalbae\scripts\preprocessor_normalbae.py`

```python
from modules_forge.supported_preprocessor import Preprocessor, PreprocessorParameter
from modules_forge.shared import preprocessor_dir, add_supported_preprocessor
from modules_forge.forge_util import resize_image_with_pad
from modules.modelloader import load_file_from_url

import types
import torch
import numpy as np

from einops import rearrange
from annotator.normalbae.models.NNET import NNET
from annotator.normalbae import load_checkpoint
from torchvision import transforms


class PreprocessorNormalBae(Preprocessor):
    def __init__(self):
        super().__init__()
        self.name = 'normalbae'
        self.tags = ['NormalMap']
        self.model_filename_filters = ['normal']
        self.slider_resolution = PreprocessorParameter(
            label='Resolution', minimum=128, maximum=2048, value=512, step=8, visible=True)
        self.slider_1 = PreprocessorParameter(visible=False)
        self.slider_2 = PreprocessorParameter(visible=False)
        self.slider_3 = PreprocessorParameter(visible=False)
        self.show_control_mode = True
        self.do_not_need_model = False
        self.sorting_priority = 100  # higher goes to top in the list

    def load_model(self):
        if self.model_patcher is not None:
            return

        model_path = load_file_from_url(
            ""https://huggingface.co/lllyasviel/Annotators/resolve/main/scannet.pt"",
            model_dir=preprocessor_dir)

        args = types.SimpleNamespace()
        args.mode = 'client'
        args.architecture = 'BN'
        args.pretrained = 'scannet'
        args.sampling_ratio = 0.4
        args.importance_ratio = 0.7
        model = NNET(args)
        model = load_checkpoint(model_path, model)
        self.norm = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])

        self.model_patcher = self.setup_model_patcher(model)

    def __call__(self, input_image, resolution, slider_1=None, slider_2=None, slider_3=None, **kwargs):
        input_image, remove_pad = resize_image_with_pad(input_image, resolution)

        self.load_model()

        self.move_all_model_patchers_to_gpu()

        assert input_image.ndim == 3
        image_normal = input_image

        with torch.no_grad():
            image_normal = self.send_tensor_to_model_device(torch.from_numpy(image_normal))
            image_normal = image_normal / 255.0
            image_normal = rearrange(image_normal, 'h w c -> 1 c h w')
            image_normal = self.norm(image_normal)

            normal = self.model_patcher.model(image_normal)
            normal = normal[0][-1][:, :3]
            normal = ((normal + 1) * 0.5).clip(0, 1)

            normal = rearrange(normal[0], 'c h w -> h w c').cpu().numpy()
            normal_image = (normal * 255.0).clip(0, 255).astype(np.uint8)

        return remove_pad(normal_image)


add_supported_preprocessor(PreprocessorNormalBae())

```

# New features (that are not available in original WebUI)

Thanks to Unet Patcher, many new things are possible now and supported in Forge, including SVD, Z123, masked Ip-adapter, masked controlnet, photomaker, etc.

Masked Ip-Adapter

![image](https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/d26630f9-922d-4483-8bf9-f364dca5fd50)

![image](https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/03580ef7-235c-4b03-9ca6-a27677a5a175)

![image](https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/d9ed4a01-70d4-45b4-a6a7-2f765f158fae)

Masked ControlNet

![image](https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/872d4785-60e4-4431-85c7-665c781dddaa)

![image](https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/335a3b33-1ef8-46ff-a462-9f1b4f2c49fc)

![image](https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/b3684a15-8895-414e-8188-487269dfcada)

PhotoMaker

(Note that photomaker is a special control that need you to add the trigger word ""photomaker"". Your prompt should be like ""a photo of photomaker"")

![image](https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/07b0b626-05b5-473b-9d69-3657624d59be)

Marigold Depth

![image](https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/bdf54148-892d-410d-8ed9-70b4b121b6e7)

# New Samplers (that are not in origin)

    DDPM
    DDPM Karras
    DPM++ 2M Turbo
    DPM++ 2M SDE Turbo
    LCM Karras
    Euler A Turbo

# About Extensions

ControlNet and TiledVAE are integrated, and you should uninstall these two extensions:

    sd-webui-controlnet
    multidiffusion-upscaler-for-automatic1111

Note that **AnimateDiff** is under construction by [continue-revolution](https://github.com/continue-revolution) at [sd-webui-animatediff forge/master branch](https://github.com/continue-revolution/sd-webui-animatediff/tree/forge/master) and [sd-forge-animatediff](https://github.com/continue-revolution/sd-forge-animatediff) (they are in sync). (continue-revolution original words: prompt travel, inf t2v, controlnet v2v have been proven to work well motion lora, i2i batch still under construction and may be finished in a week"")

Other extensions should work without problems, like:

    canvas-zoom
    translations/localizations
    Dynamic Prompts
    Adetailer
    Ultimate SD Upscale
    Reactor

However, if newer extensions use Forge, their codes can be much shorter. 

Usually if an old extension rework using Forge's unet patcher, 80% codes can be removed, especially when they need to call controlnet.

# Contribution

Forge uses a bot to get commits and codes from https://github.com/AUTOMATIC1111/stable-diffusion-webui/tree/dev every afternoon (if merge is automatically successful by a git bot, or by my compiler, or by my ChatGPT bot) or mid-night (if my compiler and my ChatGPT bot both failed to merge and I review it manually).

All PRs that can be implemented in https://github.com/AUTOMATIC1111/stable-diffusion-webui/tree/dev should submit PRs there.

Feel free to submit PRs related to the functionality of Forge here.
",,,,0,,,
2024-02-25,https://github.com/OS-Copilot/FRIDAY,https://raw.githubusercontent.com/OS-Copilot/FRIDAY/main/README.md,"OS-Copilot presents a novel conceptual framework for creating generalist computer agents, specifically designed for Linux and MacOS operating systems. It introduces FRIDAY, an AI assistant that exemplifies the framework's potential by demonstrating self-improvement capabilities in performing general computer tasks. Users can get started by cloning the FRIDAY GitHub repository, setting up the Python environment, installing dependencies, and configuring the OpenAI API key to run tasks. The project also encourages community participation by offering open-source toolkits (FRIDAY-Gizmos) for extending FRIDAY's functionalities. Additionally, it provides insights into deploying custom API tools with FastAPI for enhanced tool interactions. Users are advised of the ""as is"" nature of OS-Copilot and the importance of responsible usage. The community section invites collaboration and sharing among enthusiasts. The initiative is thoroughly documented, including a citation for academic referencing and contact information for further engagement.","# OS-Copilot: Towards Generalist Computer Agents with Self-Improvement

<div align=""center"">

[[Website]](https://os-copilot.github.io/)
[[Arxiv]](https://arxiv.org/abs/2402.07456)
[[PDF]](https://arxiv.org/pdf/2402.07456.pdf)
<!-- [[Tweet]](https://twitter.com/DrJimFan/status/1662115266933972993?s=20) -->

[![Static Badge](https://img.shields.io/badge/MIT-License-green)](https://github.com/OS-Copilot/FRIDAY/blob/main/LICENSE)
![Static Badge](https://img.shields.io/badge/python-3.10-blue)
[![Static Badge](https://img.shields.io/badge/FRIDAY-Frontend-yellow)](https://github.com/OS-Copilot/FRIDAY-front)



<p align=""center"">
  <img src='pic/demo.png' width=""100%"">
</p>

</div>

## üìñ Overview

- **OS-Copilot** is a pioneering conceptual framework for building generalist computer agents on Linux and MacOS, which provides a unified interface for app interactions in the heterogeneous OS ecosystem.
  
<p align=""center"">
  <img src='pic/framework.png' width=""75%"">
</p>

- Leveraging OS-Copilot, we built **FRIDAY**, a self-improving AI assistant capable of solving general computer tasks.

<p align=""center"">
  <img src='pic/FRIDAY.png' width=""75%"">
</p>

## ‚ö°Ô∏è Quickstart

1. **Clone the GitHub Repository:** 

   ```
   git clone https://github.com/OS-Copilot/FRIDAY.git
   ```

2. **Set Up Python Environment:** Ensure you have a version 3.10 or higher Python environment. You can create and
   activate this environment using the following commands, replacing `FRIDAY_env` with your preferred environment
   name:

   ```
   conda create -n FRIDAY_env python=3.10 -y
   conda activate FRIDAY_env
   ```

3. **Install Dependencies:** Move into the `FRIDAY` directory and install the necessary dependencies by running:

   ```
   cd FRIDAY
   pip install -r requirements.txt
   ```

4. **Set OpenAI API Key:** Configure your OpenAI API key in [.env](.env) and select the model you wish to use.

5. **Execute Your Task:** Run the following command to start FRIDAY. Replace `[query]` with your task as needed. By default, the task is *""Move the text files containing the word 'agent' from the folder named 'document' to the path 'working_dir/agent'""*.  If the task requires using related files, you can use `--query_file_path [file_path]`.
   ```
   python run.py --query [query]
   ```

\* FRIDAY currently only supports single-round conversation.

## üõ†Ô∏è FRIDAY-Gizmos
We maintain an open-source library of toolkits for FRIDAY, which includes tools that can be directly utilized within FRIDAY.
For a detailed list of tools, please see [FRIDAY-Gizmos](https://github.com/OS-Copilot/FRIDAY-Gizmos). The usage methods are as follows:

1. Find the tool you want to use in [FRIDAY-Gizmos](https://github.com/OS-Copilot/FRIDAY-Gizmos) and download its tool code.
2. Add the tool to FRIDAY's toolkit:
```shell
python friday/core/action_manager.py --add --tool_name [tool_name] --tool_path [tool_path]
```
3. If you wish to remove a tool, you can run:
```shell
python friday/core/action_manager.py --delete --tool_name [tool_name]
```

## üíª User Interface (UI)

**Enhance Your Experience with Our Intuitive Frontend!** This interface is crafted for effortless control of your agents. For more details, visit [FRIDAY Frontend](https://github.com/OS-Copilot/FRIDAY-front).

## ‚ú® Deploy your own API tools with FastAPI
All FastAPIs are underÔºö [friday/api](friday/api)
1. **Prepare your FastAPI file:** Create a new api folder under [friday/api](friday/api) and put your FastAPi python files under that folder.
2. **Import your FastAPI in API server:** Import your apis in [friday/core/api_server.py](friday/core/api_server.py)Ôºö
```python
import os

from fastapi import FastAPI
from friday.core.server_config import ConfigManager

app = FastAPI()


from friday.api.bing.bing_service import router as bing_router
#[TODO] Import your own api here


from starlette.middleware.base import BaseHTTPMiddleware
from starlette.requests import Request


class LoggingMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        print(f""Incoming request: {request.method} {request.url}"")
        try:
            response = await call_next(request)
        except Exception as e:
            print(f""Request error: {str(e)}"")
            raise e from None
        else:
            print(f""Outgoing response: {response.status_code}"")
        return response


app.add_middleware(LoggingMiddleware)

# Create a dictionary that maps service names to their routers
services = {
    ""bing"": bing_router,
    # [TODO] Add your api router here

}

server_list = [
    ""bing"",
    # [TODO] Add your api's service name here.
]

# Include only the routers for the services listed in server_list
for service in server_list:
    if service in services:
        app.include_router(services[service])

# proxy_manager = ConfigManager()
# proxy_manager.apply_proxies()

if __name__ == ""__main__"":
    import uvicorn
    # you can change your port anyway
    uvicorn.run(app, host=""0.0.0.0"", port=8079)
```
3. **Run API server:**
Run the server in localhost,or deploy it on your web server:
```
python api_server.py
```
4. **Update API documentation:** 

Update the API documentation located in [friday/core/openapi.json](friday/core/openapi.json). After launching the API server, you can access the current OpenAPI documentation at `http://localhost:8079/openapi.json`.

Ensure to thoroughly update each API's summary in the documentation to clearly explain its functionality and usage. This is crucial as FRIDAY relies on these descriptions to understand the purpose of each API.

For example:
```json
{
  ""openapi"": ""3.1.0"",
  ""info"": {
    ""title"": ""FastAPI"",
    ""version"": ""0.1.0""
  },
  ""paths"": {  
    ""/tools/audio2text"": {
      ""post"": {
        // [TODO] change the summary to describe the usage of your api.
        ""summary"": ""A tool that converts audio to natural language text"",
        ""operationId"": ""audio2text_tools_audio2text_post"",
        ""requestBody"": {
          ""content"": {
            ""multipart/form-data"": {
              ""schema"": {
                ""$ref"": ""#/components/schemas/Body_audio2text_tools_audio2text_post""
              }
            }
          },
          ""required"": true
        },
        ""responses"": {
          ""200"": {
            ""description"": ""Successful Response"",
            ""content"": {
              ""application/json"": {
                ""schema"": {}
              }
            }
          },
          ""422"": {
            ""description"": ""Validation Error"",
            ""content"": {
              ""application/json"": {
                ""schema"": {
                  ""$ref"": ""#/components/schemas/HTTPValidationError""
                }
              }
            }
          }
        }
      }
    },
    
  },
  ""components"": {
    ""schemas"": {
      ""Body_audio2text_tools_audio2text_post"": {
        ""properties"": {
          ""file"": {
            ""type"": ""string"",
            ""format"": ""binary"",
            ""title"": ""File""
          }
        },
        ""type"": ""object"",
        ""required"": [
          ""file""
        ],
        ""title"": ""Body_audio2text_tools_audio2text_post""
      },
      
      
    }
  }
}
```

5. **Change the base url of tool_request_util.py:** FRIDAY utilizes the script located at [friday/core/tool_request_util.py](friday/core/tool_request_util.py) to interface with your API tools. After deploying your APIs, make sure to update the base URL in this file to match your API server's URL.
```python
import requests
class ToolRequestUtil:
    def __init__(self):
        self.session = requests.session()
        self.headers = {'User-Agent': 'Mozilla/5.0 (Macintosh Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML like Gecko) Chrome/52.0.2743.116 Safari/537.36'}
        # [TODO] Change the base url
        self.base_url = ""http://localhost:8079""

    def request(self, api_path, method, params=None, files=None, content_type=""application/json""):
        """"""
        :param api_path: the path of the api
        :param method: get/post
        :param params: the params of the api, can be None
        :param files: files to be uploaded, can be None
        :param content_type: the content_type of api, e.g., application/json, multipart/form-data, can be None
        :return: the return of the api
        """"""
        url = self.base_url + api_path
        try:
            if method.lower() == ""get"":
                if content_type == ""application/json"":
                    result = self.session.get(url=url, json=params, headers=self.headers, timeout=60).json()
                else: 
                    result = self.session.get(url=url, params=params, headers=self.headers, timeout=60).json()
            elif method.lower() == ""post"":
                if content_type == ""multipart/form-data"":
                    result = self.session.post(url=url, files=files, data=params, headers=self.headers).json()
                elif content_type == ""application/json"":
                    result = self.session.post(url=url, json=params, headers=self.headers).json()
                else:
                    result = self.session.post(url=url, data=params, headers=self.headers).json()
            else:
                print(""request method error!"")
                return None
            return result
        except Exception as e:
            print(""http request error: %s"" % e)
            return None
```
<!-- ## üë®‚Äçüíª‚Äç Contributors

<a href="""">
  <img src="""" />
</a>

Made with [contrib.rocks](https://contrib.rocks). -->

## üõ° Disclaimer

OS-Copilot is provided ""as is"" without warranty of any kind. Users assume full responsibility for any risks associated with its use, including **potential data loss** or **changes to system settings**. The developers of OS-Copilot are not liable for any damages or losses resulting from its use. Users must ensure their actions comply with applicable laws and regulations.


## üè´ Community

Join our community to connect with other agent enthusiasts, share your tools and demos, and collaborate on exciting initiatives. You can find us on [Slack](https://join.slack.com/t/slack-ped8294/shared_invite/zt-2cqebow90-soac9UFKGZ2RcUy8PqjZrA).


## üîé Citation

```
@misc{wu2024oscopilot,
      title={OS-Copilot: Towards Generalist Computer Agents with Self-Improvement}, 
      author={Zhiyong Wu and Chengcheng Han and Zichen Ding and Zhenmin Weng and Zhoumianze Liu and Shunyu Yao and Tao Yu and Lingpeng Kong},
      year={2024},
      eprint={2402.07456},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
```


## üì¨ Contact

If you have any inquiries, suggestions, or wish to contact us for any reason, we warmly invite you to email us at wuzhiyong@pjlab.org.cn.",,,,0,,,
2024-02-25,https://github.com/Hillobar/Rope,https://raw.githubusercontent.com/Hillobar/Rope/main/README.md,"Rope incorporates the insightface inswapper_128 model into a user-friendly GUI, providing lightning-fast face swapping capabilities for both images and videos. It offers advanced features such as upscalers, likeness modifiers, orientation management, and various masks with customization options. The tool also supports source face merging, auto-save functionality, real-time video playback, segment recording, and detailed pre-video editing. Rope has released several updates, including new features and fixes to improve usability, performance, and expand functionalities like VRAM monitoring, better error handling, and more responsive UI design. Rope emphasizes ethical usage, responsible for creating and sharing content with consent and within legal bounds, aligning with Swap-Mukham's ethical guidelines. Users are urged to use the software ethically and legally, bearing in mind privacy, consent, and copyright laws.","![Screenshot 2024-02-10 091752](https://github.com/Hillobar/Rope/assets/63615199/dd8ab00b-d55f-4196-a50b-f2a326fba83a)

Rope implements the insightface inswapper_128 model with a helpful GUI.
### [Discord](https://discord.gg/EcdVAFJzqp)

### [Donate](https://www.paypal.com/donate/?hosted_button_id=Y5SB9LSXFGRF2)

### [Wiki with install instructions and usage](https://github.com/Hillobar/Rope/wiki)

### [Demo Video (Rope-Ruby)](https://www.youtube.com/watch?v=4Y4U0TZ8cWY)

### ${{\color{Goldenrod}{\textsf{Last Updated 2024-02-16}}}}$ ###
### ${{\color{Goldenrod}{\textsf{Welcome to Rope-Opal!}}}}$ ###
### ${{\color{Red}{\textsf{Please grab the latest yoloface model from the link in the wiki!}}}}$ ###

![Screenshot 2024-02-10 104718](https://github.com/Hillobar/Rope/assets/63615199/4b2ee574-c91e-4db2-ad66-5b775a049a6b)

### Features: ###
* Lightning speed face swapping with all the features
* Upscalers
* Likeness modifiers
* Orientation management
* Masks: borders, differentials, auto occlusion, face parsers, text-based masking - all with strength adjustments and blending settings
* Mask view to evaluate masks directly
* Source face merging and saving
* Swap images or videos
* Auto save filename generation
* Dock/Undock the video player
* Real-time player
* Segment recording
* Fine tune your video ahead of time by creating image setting markers at specific frames.
* Lightening fast!

### Updates for Rope-Opal-02: ###
### To update from Opal-01, just need to replace the .py files in /rope. You'll seem them 'Rope-Opal-02' in the comment column ###
* (feature) Auto-pads input faces to increase the chance of detection
* (feature) Update console messages for more information
* (feature) Reimplimented images
* (fixed) Adressed error with Reference detection alignment in Restorer
* (feature) Added a VRAM monitor
* (feature) Reimplemented the VRAM clear
   
### Updates for Rope-Opal-01: ###
### To update from Opal, just need to replace the .py file in /rope. You'll seem them 'Rope-Opal-01' in the comment column ###
* (Fixed) Reverted to previous model load parameters. Some people were having issues with the new settings.
* (Fixed) Markers now clear propoerly when changing videos
* (Fixed)Input Face loading errors are now handled gracefully
* (Feature) Added option to select Mean or Median when multi-selecting input faces

### Updates for Rope-Opal: ###
* This next version focuses on the UI. It's completely overhauled and finally looks more like a modern app. Lots of useability improvements and room to add new features.
* Can now set strength to zero. This effectively turns off the swapper in the render pipeline so you can apply the rest of the options to the original image (e.g., upscale the original face w/o swapping)
* Recording library can be set to FFMPEG or OPENCV
* Real-time audio is now available while previewing. Performace that renders slower than the frame rate will cause audio lag
* The Differencing fuction has been reworked into the pipeline to produce better results. Consequently it currently does not show up in the mask preview.
* Wrestled back some VRAM from the Ruby upgrades
* Faster loading of some models. Upcoming releases will do furhter optimizations
* Adjusted the use of filtering and antialiasing
* Yolov8 added as a face detection model selection. FF is having good results with it, so looking forward to hearing your thoughts on its behavior in Rope
* Scrollbars!
* Save/load paramters, and reset to defaults. Rope will auto-load your saved paramters when launched.
* Restorers (GFPGAN, etc) now have option to choose the detection alignment method. You can trade speed vs fidelity vs texture. This includes the original Rope method that, although flawed, maintain the face textures.
* Detection score. Adjust how aggressive Rope is at finding faces.
* Added detailed help text in the lower left when hovering over UI elements.
* Added reverse, forward and beginning to timeline control.

### Some Feature Still need to be re-implmented from Rope-Ruby. They'll be added back in the next updates. ###
* Stop markers
* Framerate stats while playing
* Global hotkeys for moving the timeline
* Ongoing interface maturation

### Performance:  ###
Machine: 3090Ti (24GB), i5-13600K

<img src=""https://github.com/Hillobar/Rope/assets/63615199/3e3505db-bc76-48df-b8ac-1e7e86c8d751"" width=""200"">

File: benchmark/target-1080p.mp4, 2048x1080, 269 frames, 25 fps, 10s
Rendering time in seconds:
| Option | Crystal | Sapphire | Ruby | Opal |
| --- | --- | --- | --- | --- |
| Only Swap | 7.3 | 7.5 | 4.4 | 4.3 |
| Swap+GFPGAN | 10.7 | 11.0 | 9.0 | 9.8 |
| Swap+Codeformer | 12.4 | 13.5 | 11.1 | 11.1 |
| Swap+one word CLIP | 10.4 | 11.2 | 9.1 | 9.3 |
| Swap+Occluder | 7.8 | 7.8 | 4.4 | 4.7 |
| Swap+MouthParser | 13.9 | 12.1 | 5.0 | 4.9 |

### Disclaimer: ###
Rope is a personal project that I'm making available to the community as a thank you for all of the contributors ahead of me.
I've copied the disclaimer from [Swap-Mukham](https://github.com/harisreedhar/Swap-Mukham) here since it is well-written and applies 100% to this repo.
 
I would like to emphasize that our swapping software is intended for responsible and ethical use only. I must stress that users are solely responsible for their actions when using our software.

Intended Usage: This software is designed to assist users in creating realistic and entertaining content, such as movies, visual effects, virtual reality experiences, and other creative applications. I encourage users to explore these possibilities within the boundaries of legality, ethical considerations, and respect for others' privacy.

Ethical Guidelines: Users are expected to adhere to a set of ethical guidelines when using our software. These guidelines include, but are not limited to:

Not creating or sharing content that could harm, defame, or harass individuals. Obtaining proper consent and permissions from individuals featured in the content before using their likeness. Avoiding the use of this technology for deceptive purposes, including misinformation or malicious intent. Respecting and abiding by applicable laws, regulations, and copyright restrictions.

Privacy and Consent: Users are responsible for ensuring that they have the necessary permissions and consents from individuals whose likeness they intend to use in their creations. We strongly discourage the creation of content without explicit consent, particularly if it involves non-consensual or private content. It is essential to respect the privacy and dignity of all individuals involved.

Legal Considerations: Users must understand and comply with all relevant local, regional, and international laws pertaining to this technology. This includes laws related to privacy, defamation, intellectual property rights, and other relevant legislation. Users should consult legal professionals if they have any doubts regarding the legal implications of their creations.

Liability and Responsibility: We, as the creators and providers of the deep fake software, cannot be held responsible for the actions or consequences resulting from the usage of our software. Users assume full liability and responsibility for any misuse, unintended effects, or abusive behavior associated with the content they create.

By using this software, users acknowledge that they have read, understood, and agreed to abide by the above guidelines and disclaimers. We strongly encourage users to approach this technology with caution, integrity, and respect for the well-being and rights of others.

Remember, technology should be used to empower and inspire, not to harm or deceive. Let's strive for ethical and responsible use of deep fake technology for the betterment of society.



  
",,,https://www.youtube.com/watch?v=4Y4U0TZ8cWY,0,,,
2024-02-25,https://github.com/pydantic/FastUI,https://raw.githubusercontent.com/pydantic/FastUI/main/README.md,"FastUI, in active development, offers a novel approach to building web application interfaces with Python, eliminating the need for JavaScript in creating responsive applications. It leverages Pydantic models and TypeScript interfaces for defining user interfaces, ensuring validation at both compile and runtime. FastUI's ecosystem includes a PyPI package for UI components and utilities, two npm packages for React components and Bootstrap customization, and a pre-built React app for easy setup without npm dependencies. A demonstrated FastAPI application showcases its capabilities, rendering user profiles through a declarative Python backend. FastUI embodies the RESTful principle by enabling backend-driven UI construction, simplifying feature development and deployment processes, and promoting component reuse. Its design philosophy aims to facilitate app extension from the backend without necessitating frontend changes, aspiring for compatibility beyond Python and React to potentially include other languages and frameworks.","# FastUI

[![CI](https://github.com/pydantic/FastUI/actions/workflows/ci.yml/badge.svg)](https://github.com/pydantic/FastUI/actions?query=event%3Apush+branch%3Amain+workflow%3ACI)
[![pypi](https://img.shields.io/pypi/v/fastui.svg)](https://pypi.python.org/pypi/fastui)
[![versions](https://img.shields.io/pypi/pyversions/fastui.svg)](https://github.com/pydantic/FastUI)
[![license](https://img.shields.io/github/license/pydantic/FastUI.svg)](https://github.com/pydantic/FastUI/blob/main/LICENSE)

**Please note:** FastUI is still an active work in progress, do not expect it to be complete.

## The Principle (short version)

You can see a simple demo of an application built with FastUI [here](https://fastui-demo.onrender.com).

FastUI is a new way to build web application user interfaces defined by declarative Python code.

This means:

- **If you're a Python developer** ‚Äî you can build responsive web applications using React without writing a single line of JavaScript, or touching `npm`.
- **If you're a frontend developer** ‚Äî you can concentrate on building magical components that are truly reusable, no copy-pasting components for each view.
- **For everyone** ‚Äî a true separation of concerns, the backend defines the entire application while the frontend is free to implement just the user interface

At its heart, FastUI is a set of matching [Pydantic](https://docs.pydantic.dev) models and TypeScript interfaces that allow you to define a user interface. This interface is validated at build time by TypeScript and pyright/mypy and at runtime by Pydantic.

## The Practice ‚Äî Usage

FastUI is made up of 4 things:

- [`fastui` PyPI package](https://pypi.python.org/pypi/fastui) ‚Äî Pydantic models for UI components, and some utilities. While it works well with [FastAPI](https://fastapi.tiangolo.com) it doesn't depend on FastAPI, and most of it could be used with any python web framework.
- [`@pydantic/fastui` npm package](https://www.npmjs.com/package/@pydantic/fastui) ‚Äî a React TypeScript package that lets you reuse the machinery and types of FastUI while implementing your own components
- [`@pydantic/fastui-bootstrap` npm package](https://www.npmjs.com/package/@pydantic/fastui-bootstrap) ‚Äî implementation/customisation of all FastUI components using [Bootstrap](https://getbootstrap.com)
- [`@pydantic/fastui-prebuilt` npm package](https://www.jsdelivr.com/package/npm/@pydantic/fastui-prebuilt) (available on [jsdelivr.com CDN](https://www.jsdelivr.com/package/npm/@pydantic/fastui-prebuilt)) providing a pre-built version of the FastUI React app so you can use it without installing any npm packages or building anything yourself. The Python package provides a simple HTML page to serve this app.

Here's a simple but complete FastAPI application that uses FastUI to show some user profiles:

```python
from datetime import date

from fastapi import FastAPI, HTTPException
from fastapi.responses import HTMLResponse
from fastui import FastUI, AnyComponent, prebuilt_html, components as c
from fastui.components.display import DisplayMode, DisplayLookup
from fastui.events import GoToEvent, BackEvent
from pydantic import BaseModel, Field

app = FastAPI()


class User(BaseModel):
    id: int
    name: str
    dob: date = Field(title='Date of Birth')


# define some users
users = [
    User(id=1, name='John', dob=date(1990, 1, 1)),
    User(id=2, name='Jack', dob=date(1991, 1, 1)),
    User(id=3, name='Jill', dob=date(1992, 1, 1)),
    User(id=4, name='Jane', dob=date(1993, 1, 1)),
]


@app.get(""/api/"", response_model=FastUI, response_model_exclude_none=True)
def users_table() -> list[AnyComponent]:
    """"""
    Show a table of four users, `/api` is the endpoint the frontend will connect to
    when a user visits `/` to fetch components to render.
    """"""
    return [
        c.Page(  # Page provides a basic container for components
            components=[
                c.Heading(text='Users', level=2),  # renders `<h2>Users</h2>`
                c.Table(
                    data=users,
                    # define two columns for the table
                    columns=[
                        # the first is the users, name rendered as a link to their profile
                        DisplayLookup(field='name', on_click=GoToEvent(url='/user/{id}/')),
                        # the second is the date of birth, rendered as a date
                        DisplayLookup(field='dob', mode=DisplayMode.date),
                    ],
                ),
            ]
        ),
    ]


@app.get(""/api/user/{user_id}/"", response_model=FastUI, response_model_exclude_none=True)
def user_profile(user_id: int) -> list[AnyComponent]:
    """"""
    User profile page, the frontend will fetch this when the user visits `/user/{id}/`.
    """"""
    try:
        user = next(u for u in users if u.id == user_id)
    except StopIteration:
        raise HTTPException(status_code=404, detail=""User not found"")
    return [
        c.Page(
            components=[
                c.Heading(text=user.name, level=2),
                c.Link(components=[c.Text(text='Back')], on_click=BackEvent()),
                c.Details(data=user),
            ]
        ),
    ]


@app.get('/{path:path}')
async def html_landing() -> HTMLResponse:
    """"""Simple HTML page which serves the React app, comes last as it matches all paths.""""""
    return HTMLResponse(prebuilt_html(title='FastUI Demo'))
```

Which renders like this:

![screenshot](https://raw.githubusercontent.com/pydantic/FastUI/main/screenshot.png)

Of course, that's a very simple application, the [full demo](https://fastui-demo.onrender.com) is more complete.

### Components

FastUI already defines a rich set of components.

All components are listed in the [demo app](https://fastui-demo.onrender.com).

## The Principle (long version)

FastUI is an implementation of the RESTful principle but not as it's usually understood, instead I mean the principle defined in the original [PhD dissertation](https://ics.uci.edu/~fielding/pubs/dissertation/rest_arch_style.htm) by Roy Fielding, and excellently summarised in [this essay on htmx.org](https://htmx.org/essays/how-did-rest-come-to-mean-the-opposite-of-rest/) (HTMX people, I'm sorry to use your article to promote React which I know you despise üôè).

The RESTful principle as described in the HTMX article is that the frontend doesn't need to (and shouldn't) know anything about the application you're building. Instead, it should just provide all the components you need to construct the interface, the backend can then tell the frontend what to do.

Think of your frontend as a puppet, and the backend as the hand within it ‚Äî the puppet doesn't need to know what to say, that's kind of the point.

Building an application this way has a number of significant advantages:

- You only need to write code in one place to build a new feature ‚Äî add a new view, change the behavior of an existing view or alter the URL structure
- Deploying the front and backend can be completely decoupled, provided the frontend knows how to render all the components the backend is going to ask it to use, you're good to go
- You should be able to reuse a rich set of opensource components, they should end up being better tested and more reliable than anything you could build yourself, this is possible because the components need no context about how they're going to be used (note: since FastUI is brand new, this isn't true yet, hopefully we get there)
- We can use Pydantic, TypeScript and JSON Schema to provide guarantees that the two sides are communicating with an agreed schema

In the abstract, FastUI is like the opposite of GraphQL but with the same goal ‚Äî GraphQL lets frontend developers extend an application without any new backend development FastUI lets backend developers extend an application without any new frontend development.

### Beyond Python and React

Of course, this principle shouldn't be limited to Python and React applications ‚Äî provided we use the same set of agreed schemas and encoding to communicate, we should be able to use any frontend and backend that implements the schema. Interchangeably.

This could mean:

- Implementing a web frontend using another JS framework like Vue ‚Äî lots of work, limited value IMHO
- Implementing a web frontend using an edge server, so the browser just sees HTML ‚Äî lots of work but very valuable
- Implementing frontends for other platforms like mobile or IOT ‚Äî lots of work, no idea if it's actually a good idea?
- Implementing the component models in another language like Rust or Go ‚Äî since there's actually not that much code in the backend, so this would be a relatively small and mechanical task
",,https://raw.githubusercontent.com/pydantic/FastUI/main/screenshot.png,,0,https://raw.githubusercontent.com/pydantic/FastUI/main/screenshot.png,,
2024-02-25,https://github.com/facefusion/facefusion,https://raw.githubusercontent.com/facefusion/facefusion/main/README.md,"FaceFusion is described as the next-generation software for face swapping and enhancement, boasting an open-source status under the MIT license. Its installation requires technical know-how, directing users towards a supportive Discord community for assistance rather than GitHub. The tool offers extensive command-line options for customization, including source and target selection, output specifics, execution optimization, memory management, face analysis, and mask application. It also provides frame extraction and output creation features, alongside frame processors for advanced manipulations like face enhancement, swapping, and even lip syncing. For compression and quality adjustments, it supports various video encoders and settings. The documentation available provides in-depth guidance for users.","FaceFusion
==========

> Next generation face swapper and enhancer.

[![Build Status](https://img.shields.io/github/actions/workflow/status/facefusion/facefusion/ci.yml.svg?branch=master)](https://github.com/facefusion/facefusion/actions?query=workflow:ci)
![License](https://img.shields.io/badge/license-MIT-green)


Preview
-------

![Preview](https://raw.githubusercontent.com/facefusion/facefusion/master/.github/preview.png?sanitize=true)


Installation
------------

Be aware, the installation needs technical skills and is not for beginners. Please do not open platform and installation related issues on GitHub. We have a very helpful [Discord](https://join.facefusion.io) community that will guide you to complete the installation.

Get started with the [installation](https://docs.facefusion.io/installation) guide.


Usage
-----

Run the command:

```
python run.py [options]

options:
  -h, --help                                                                                                             show this help message and exit
  -s SOURCE_PATHS, --source SOURCE_PATHS                                                                                 choose single or multiple source images or audios
  -t TARGET_PATH, --target TARGET_PATH                                                                                   choose single target image or video
  -o OUTPUT_PATH, --output OUTPUT_PATH                                                                                   specify the output file or directory
  -v, --version                                                                                                          show program's version number and exit

misc:
  --skip-download                                                                                                        omit automate downloads and remote lookups
  --headless                                                                                                             run the program without a user interface
  --log-level {error,warn,info,debug}                                                                                    adjust the message severity displayed in the terminal

execution:
  --execution-providers EXECUTION_PROVIDERS [EXECUTION_PROVIDERS ...]                                                    accelerate the model inference using different providers (choices: cpu, ...)
  --execution-thread-count [1-128]                                                                                       specify the amount of parallel threads while processing
  --execution-queue-count [1-32]                                                                                         specify the amount of frames each thread is processing

memory:
  --video-memory-strategy {strict,moderate,tolerant}                                                                     balance fast frame processing and low vram usage
  --system-memory-limit [0-128]                                                                                          limit the available ram that can be used while processing

face analyser:
  --face-analyser-order {left-right,right-left,top-bottom,bottom-top,small-large,large-small,best-worst,worst-best}      specify the order in which the face analyser detects faces.
  --face-analyser-age {child,teen,adult,senior}                                                                          filter the detected faces based on their age
  --face-analyser-gender {female,male}                                                                                   filter the detected faces based on their gender
  --face-detector-model {retinaface,yoloface,yunet}                                                                      choose the model responsible for detecting the face
  --face-detector-size FACE_DETECTOR_SIZE                                                                                specify the size of the frame provided to the face detector
  --face-detector-score [0.0-1.0]                                                                                        filter the detected faces base on the confidence score

face selector:
  --face-selector-mode {reference,one,many}                                                                              use reference based tracking with simple matching
  --reference-face-position REFERENCE_FACE_POSITION                                                                      specify the position used to create the reference face
  --reference-face-distance [0.0-1.5]                                                                                    specify the desired similarity between the reference face and target face
  --reference-frame-number REFERENCE_FRAME_NUMBER                                                                        specify the frame used to create the reference face

face mask:
  --face-mask-types FACE_MASK_TYPES [FACE_MASK_TYPES ...]                                                                mix and match different face mask types (choices: box, occlusion, region)
  --face-mask-blur [0.0-1.0]                                                                                             specify the degree of blur applied the box mask
  --face-mask-padding FACE_MASK_PADDING [FACE_MASK_PADDING ...]                                                          apply top, right, bottom and left padding to the box mask
  --face-mask-regions FACE_MASK_REGIONS [FACE_MASK_REGIONS ...]                                                          choose the facial features used for the region mask (choices: skin, left-eyebrow, right-eyebrow, left-eye, right-eye, eye-glasses, nose, mouth, upper-lip, lower-lip)

frame extraction:
  --trim-frame-start TRIM_FRAME_START                                                                                    specify the the start frame of the target video
  --trim-frame-end TRIM_FRAME_END                                                                                        specify the the end frame of the target video
  --temp-frame-format {bmp,jpg,png}                                                                                      specify the temporary resources format
  --temp-frame-quality [0-100]                                                                                           specify the temporary resources quality
  --keep-temp                                                                                                            keep the temporary resources after processing

output creation:
  --output-image-quality [0-100]                                                                                         specify the image quality which translates to the compression factor
  --output-video-encoder {libx264,libx265,libvpx-vp9,h264_nvenc,hevc_nvenc}                                              specify the encoder use for the video compression
  --output-video-preset {ultrafast,superfast,veryfast,faster,fast,medium,slow,slower,veryslow}                           balance fast video processing and video file size
  --output-video-quality [0-100]                                                                                         specify the video quality which translates to the compression factor
  --output-video-resolution OUTPUT_VIDEO_RESOLUTION                                                                      specify the video output resolution based on the target video
  --output-video-fps OUTPUT_VIDEO_FPS                                                                                    specify the video output fps based on the target video
  --skip-audio                                                                                                           omit the audio from the target video

frame processors:
  --frame-processors FRAME_PROCESSORS [FRAME_PROCESSORS ...]                                                             load a single or multiple frame processors. (choices: face_debugger, face_enhancer, face_swapper, frame_enhancer, lip_syncer, ...)
  --face-debugger-items FACE_DEBUGGER_ITEMS [FACE_DEBUGGER_ITEMS ...]                                                    load a single or multiple frame processors (choices: bounding-box, landmark-5, landmark-68, face-mask, score, age, gender)
  --face-enhancer-model {codeformer,gfpgan_1.2,gfpgan_1.3,gfpgan_1.4,gpen_bfr_256,gpen_bfr_512,restoreformer_plus_plus}  choose the model responsible for enhancing the face
  --face-enhancer-blend [0-100]                                                                                          blend the enhanced into the previous face
  --face-swapper-model {blendswap_256,inswapper_128,inswapper_128_fp16,simswap_256,simswap_512_unofficial,uniface_256}   choose the model responsible for swapping the face
  --frame-enhancer-model {real_esrgan_x2plus,real_esrgan_x4plus,real_esrnet_x4plus}                                      choose the model responsible for enhancing the frame
  --frame-enhancer-blend [0-100]                                                                                         blend the enhanced into the previous frame
  --lip-syncer-model {wav2lip_gan}                                                                                       choose the model responsible for syncing the lips

uis:
  --ui-layouts UI_LAYOUTS [UI_LAYOUTS ...]                                                                               launch a single or multiple UI layouts (choices: benchmark, default, webcam, ...)
```


Documentation
-------------

Read the [documentation](https://docs.facefusion.io) for a deep dive.
",,,,0,,,
2024-02-25,https://github.com/zhayujie/chatgpt-on-wechat,https://raw.githubusercontent.com/zhayujie/chatgpt-on-wechat/master/README.md,"The project introduces an intelligent conversational AI bot based on large language models, compatible with multiple platforms including WeChat, Enterprise WeChat, official WeChat accounts, Feishu, DingTalk, etc. It supports a range of models like GPT-3.5, GPT-4.0, Claude, and more for text, voice, and image processing, alongside plugin support for operating system and internet access. The latest version offers multi-platform deployment, base dialogues for personal and group chats, voice recognition capabilities, image recognition and generation, and rich plugin support for customization. It also boasts a knowledge-base feature for creating enterprise AI applications tailored to specific needs. Additionally, there's commercial support for an AI application platform offering knowledge bases, agent plugins, application management, and various deployment models for improving enterprise efficiency in sectors like e-commerce, education, and healthcare. The project is accompanied by a quick start guide, involving account registration, environment setup, and configurations detailed for a seamless integration process, facilitating easy adoption and utilization of AI technology for businesses and individual developers alike.","# ÁÆÄ‰ªã

> Êú¨È°πÁõÆÊòØÂü∫‰∫éÂ§ßÊ®°ÂûãÁöÑÊô∫ËÉΩÂØπËØùÊú∫Âô®‰∫∫ÔºåÊîØÊåÅÂæÆ‰ø°„ÄÅ‰ºÅ‰∏öÂæÆ‰ø°„ÄÅÂÖ¨‰ºóÂè∑„ÄÅÈ£û‰π¶„ÄÅÈíâÈíâÊé•ÂÖ•ÔºåÂèØÈÄâÊã©GPT3.5/GPT4.0/Claude/ÊñáÂøÉ‰∏ÄË®Ä/ËÆØÈ£ûÊòüÁÅ´/ÈÄö‰πâÂçÉÈóÆ/Gemini/LinkAI/ZhipuAIÔºåËÉΩÂ§ÑÁêÜÊñáÊú¨„ÄÅËØ≠Èü≥ÂíåÂõæÁâáÔºåÈÄöËøáÊèí‰ª∂ËÆøÈóÆÊìç‰ΩúÁ≥ªÁªüÂíå‰∫íËÅîÁΩëÁ≠âÂ§ñÈÉ®ËµÑÊ∫êÔºåÊîØÊåÅÂü∫‰∫éËá™ÊúâÁü•ËØÜÂ∫ìÂÆöÂà∂‰ºÅ‰∏öAIÂ∫îÁî®„ÄÇ

ÊúÄÊñ∞ÁâàÊú¨ÊîØÊåÅÁöÑÂäüËÉΩÂ¶Ç‰∏ãÔºö

- [x] **Â§öÁ´ØÈÉ®ÁΩ≤Ôºö** ÊúâÂ§öÁßçÈÉ®ÁΩ≤ÊñπÂºèÂèØÈÄâÊã©‰∏îÂäüËÉΩÂÆåÂ§áÔºåÁõÆÂâçÂ∑≤ÊîØÊåÅ‰∏™‰∫∫ÂæÆ‰ø°„ÄÅÂæÆ‰ø°ÂÖ¨‰ºóÂè∑Âíå„ÄÅ‰ºÅ‰∏öÂæÆ‰ø°„ÄÅÈ£û‰π¶„ÄÅÈíâÈíâÁ≠âÈÉ®ÁΩ≤ÊñπÂºè
- [x] **Âü∫Á°ÄÂØπËØùÔºö** ÁßÅËÅäÂèäÁæ§ËÅäÁöÑÊ∂àÊÅØÊô∫ËÉΩÂõûÂ§çÔºåÊîØÊåÅÂ§öËΩÆ‰ºöËØù‰∏ä‰∏ãÊñáËÆ∞ÂøÜÔºåÊîØÊåÅ GPT-3.5, GPT-4, claude, Gemini, ÊñáÂøÉ‰∏ÄË®Ä, ËÆØÈ£ûÊòüÁÅ´, ÈÄö‰πâÂçÉÈóÆÔºåChatGLM
- [x] **ËØ≠Èü≥ËÉΩÂäõÔºö** ÂèØËØÜÂà´ËØ≠Èü≥Ê∂àÊÅØÔºåÈÄöËøáÊñáÂ≠óÊàñËØ≠Èü≥ÂõûÂ§çÔºåÊîØÊåÅ azure, baidu, google, openai(whisper/tts) Á≠âÂ§öÁßçËØ≠Èü≥Ê®°Âûã
- [x] **ÂõæÂÉèËÉΩÂäõÔºö** ÊîØÊåÅÂõæÁâáÁîüÊàê„ÄÅÂõæÁâáËØÜÂà´„ÄÅÂõæÁîüÂõæÔºàÂ¶ÇÁÖßÁâá‰øÆÂ§çÔºâÔºåÂèØÈÄâÊã© Dall-E-3, stable diffusion, replicate, midjourney, CogView-3, visionÊ®°Âûã
- [x] **‰∏∞ÂØåÊèí‰ª∂Ôºö** ÊîØÊåÅ‰∏™ÊÄßÂåñÊèí‰ª∂Êâ©Â±ïÔºåÂ∑≤ÂÆûÁé∞Â§öËßíËâ≤ÂàáÊç¢„ÄÅÊñáÂ≠óÂÜíÈô©„ÄÅÊïèÊÑüËØçËøáÊª§„ÄÅËÅäÂ§©ËÆ∞ÂΩïÊÄªÁªì„ÄÅÊñáÊ°£ÊÄªÁªìÂíåÂØπËØù„ÄÅËÅîÁΩëÊêúÁ¥¢Á≠âÊèí‰ª∂
- [x] **Áü•ËØÜÂ∫ìÔºö** ÈÄöËøá‰∏ä‰º†Áü•ËØÜÂ∫ìÊñá‰ª∂Ëá™ÂÆö‰πâ‰∏ìÂ±ûÊú∫Âô®‰∫∫ÔºåÂèØ‰Ωú‰∏∫Êï∞Â≠óÂàÜË∫´„ÄÅÊô∫ËÉΩÂÆ¢Êúç„ÄÅÁßÅÂüüÂä©Êâã‰ΩøÁî®ÔºåÂü∫‰∫é [LinkAI](https://link-ai.tech) ÂÆûÁé∞

# ÊºîÁ§∫

https://github.com/zhayujie/chatgpt-on-wechat/assets/26161723/d5154020-36e3-41db-8706-40ce9f3f1b1e

Demo made by [Visionn](https://www.wangpc.cc/)

# ÂïÜ‰∏öÊîØÊåÅ

> Êàë‰ª¨ËøòÊèê‰æõ‰ºÅ‰∏öÁ∫ßÁöÑ **AIÂ∫îÁî®Âπ≥Âè∞**ÔºåÂåÖÂê´Áü•ËØÜÂ∫ì„ÄÅAgentÊèí‰ª∂„ÄÅÂ∫îÁî®ÁÆ°ÁêÜÁ≠âËÉΩÂäõÔºåÊîØÊåÅÂ§öÂπ≥Âè∞ËÅöÂêàÁöÑÂ∫îÁî®Êé•ÂÖ•„ÄÅÂÆ¢Êà∑Á´ØÁÆ°ÁêÜ„ÄÅÂØπËØùÁÆ°ÁêÜÔºå‰ª•ÂèäÊèê‰æõ
SaaSÊúçÂä°„ÄÅÁßÅÊúâÂåñÈÉ®ÁΩ≤„ÄÅÁ®≥ÂÆöÊâòÁÆ°Êé•ÂÖ• Á≠âÂ§öÁßçÊ®°Âºè„ÄÇ
>
> ÁõÆÂâçÂ∑≤Âú®ÁßÅÂüüËøêËê•„ÄÅÊô∫ËÉΩÂÆ¢Êúç„ÄÅ‰ºÅ‰∏öÊïàÁéáÂä©ÊâãÁ≠âÂú∫ÊôØÁßØÁ¥Ø‰∫Ü‰∏∞ÂØåÁöÑ AI Ëß£ÂÜ≥ÊñπÊ°àÔºå Âú®ÁîµÂïÜ„ÄÅÊñáÊïô„ÄÅÂÅ•Â∫∑„ÄÅÊñ∞Ê∂àË¥πÁ≠âÂêÑË°å‰∏öÊ≤âÊ∑Ä‰∫Ü AI ËêΩÂú∞ÁöÑÊúÄ‰Ω≥ÂÆûË∑µÔºåËá¥Âäõ‰∫éÊâìÈÄ†Âä©Âäõ‰∏≠Â∞è‰ºÅ‰∏öÊã•Êä± AI ÁöÑ‰∏ÄÁ´ôÂºèÂπ≥Âè∞„ÄÇ

‰ºÅ‰∏öÊúçÂä°ÂíåÂïÜÁî®Âí®ËØ¢ÂèØËÅîÁ≥ª‰∫ßÂìÅÈ°æÈóÆÔºö

<img width=""240"" src=""https://img-1317903499.cos.ap-guangzhou.myqcloud.com/docs/product-manager-qrcode.jpg"">

# ÂºÄÊ∫êÁ§æÂå∫

Ê∑ªÂä†Â∞èÂä©ÊâãÂæÆ‰ø°Âä†ÂÖ•ÂºÄÊ∫êÈ°πÁõÆ‰∫§ÊµÅÁæ§Ôºö

<img width=""240"" src=""./docs/images/contact.jpg"">

# Êõ¥Êñ∞Êó•Âøó

>**2023.11.11Ôºö** [1.5.3ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.3) Âíå [1.5.4ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.4)ÔºåÊñ∞Â¢ûGoogle Gemini„ÄÅÈÄö‰πâÂçÉÈóÆÊ®°Âûã

>**2023.11.10Ôºö** [1.5.2ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.2)ÔºåÊñ∞Â¢ûÈ£û‰π¶ÈÄöÈÅì„ÄÅÂõæÂÉèËØÜÂà´ÂØπËØù„ÄÅÈªëÂêçÂçïÈÖçÁΩÆ

>**2023.11.10Ôºö** [1.5.0ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.0)ÔºåÊñ∞Â¢û `gpt-4-turbo`, `dall-e-3`, `tts` Ê®°ÂûãÊé•ÂÖ•ÔºåÂÆåÂñÑÂõæÂÉèÁêÜËß£&ÁîüÊàê„ÄÅËØ≠Èü≥ËØÜÂà´&ÁîüÊàêÁöÑÂ§öÊ®°ÊÄÅËÉΩÂäõ

>**2023.10.16Ôºö** ÊîØÊåÅÈÄöËøáÊÑèÂõæËØÜÂà´‰ΩøÁî®LinkAIËÅîÁΩëÊêúÁ¥¢„ÄÅÊï∞Â≠¶ËÆ°ÁÆó„ÄÅÁΩëÈ°µËÆøÈóÆÁ≠âÊèí‰ª∂ÔºåÂèÇËÄÉ[Êèí‰ª∂ÊñáÊ°£](https://docs.link-ai.tech/platform/plugins)

>**2023.09.26Ôºö** Êèí‰ª∂Â¢ûÂä† Êñá‰ª∂/ÊñáÁ´†ÈìæÊé• ‰∏ÄÈîÆÊÄªÁªìÂíåÂØπËØùÁöÑÂäüËÉΩÔºå‰ΩøÁî®ÂèÇËÄÉÔºö[Êèí‰ª∂ËØ¥Êòé](https://github.com/zhayujie/chatgpt-on-wechat/tree/master/plugins/linkai#3%E6%96%87%E6%A1%A3%E6%80%BB%E7%BB%93%E5%AF%B9%E8%AF%9D%E5%8A%9F%E8%83%BD)

>**2023.08.08Ôºö** Êé•ÂÖ•ÁôæÂ∫¶ÊñáÂøÉ‰∏ÄË®ÄÊ®°ÂûãÔºåÈÄöËøá [Êèí‰ª∂](https://github.com/zhayujie/chatgpt-on-wechat/tree/master/plugins/linkai) ÊîØÊåÅ Midjourney ÁªòÂõæ

>**2023.06.12Ôºö** Êé•ÂÖ• [LinkAI](https://link-ai.tech/console) Âπ≥Âè∞ÔºåÂèØÂú®Á∫øÂàõÂª∫È¢ÜÂüüÁü•ËØÜÂ∫ìÔºåÂπ∂Êé•ÂÖ•ÂæÆ‰ø°„ÄÅÂÖ¨‰ºóÂè∑Âèä‰ºÅ‰∏öÂæÆ‰ø°‰∏≠ÔºåÊâìÈÄ†‰∏ìÂ±ûÂÆ¢ÊúçÊú∫Âô®‰∫∫„ÄÇ‰ΩøÁî®ÂèÇËÄÉ [Êé•ÂÖ•ÊñáÊ°£](https://link-ai.tech/platform/link-app/wechat)„ÄÇ

>**2023.04.26Ôºö** ÊîØÊåÅ‰ºÅ‰∏öÂæÆ‰ø°Â∫îÁî®Âè∑ÈÉ®ÁΩ≤ÔºåÂÖºÂÆπÊèí‰ª∂ÔºåÂπ∂ÊîØÊåÅËØ≠Èü≥ÂõæÁâá‰∫§‰∫íÔºåÁßÅ‰∫∫Âä©ÁêÜÁêÜÊÉ≥ÈÄâÊã©Ôºå[‰ΩøÁî®ÊñáÊ°£](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/channel/wechatcom/README.md)„ÄÇ(contributed by [@lanvent](https://github.com/lanvent) in [#944](https://github.com/zhayujie/chatgpt-on-wechat/pull/944))

>**2023.04.05Ôºö** ÊîØÊåÅÂæÆ‰ø°ÂÖ¨‰ºóÂè∑ÈÉ®ÁΩ≤ÔºåÂÖºÂÆπÊèí‰ª∂ÔºåÂπ∂ÊîØÊåÅËØ≠Èü≥ÂõæÁâá‰∫§‰∫íÔºå[‰ΩøÁî®ÊñáÊ°£](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/channel/wechatmp/README.md)„ÄÇ(contributed by [@JS00000](https://github.com/JS00000) in [#686](https://github.com/zhayujie/chatgpt-on-wechat/pull/686))

>**2023.04.05Ôºö** Â¢ûÂä†ËÉΩËÆ©ChatGPT‰ΩøÁî®Â∑•ÂÖ∑ÁöÑ`tool`Êèí‰ª∂Ôºå[‰ΩøÁî®ÊñáÊ°£](https://github.com/goldfishh/chatgpt-on-wechat/blob/master/plugins/tool/README.md)„ÄÇÂ∑•ÂÖ∑Áõ∏ÂÖ≥issueÂèØÂèçÈ¶àËá≥[chatgpt-tool-hub](https://github.com/goldfishh/chatgpt-tool-hub)„ÄÇ(contributed by [@goldfishh](https://github.com/goldfishh) in [#663](https://github.com/zhayujie/chatgpt-on-wechat/pull/663))

>**2023.03.25Ôºö** ÊîØÊåÅÊèí‰ª∂ÂåñÂºÄÂèëÔºåÁõÆÂâçÂ∑≤ÂÆûÁé∞ Â§öËßíËâ≤ÂàáÊç¢„ÄÅÊñáÂ≠óÂÜíÈô©Ê∏∏Êàè„ÄÅÁÆ°ÁêÜÂëòÊåá‰ª§„ÄÅStable DiffusionÁ≠âÊèí‰ª∂Ôºå‰ΩøÁî®ÂèÇËÄÉ [#578](https://github.com/zhayujie/chatgpt-on-wechat/issues/578)„ÄÇ(contributed by [@lanvent](https://github.com/lanvent) in [#565](https://github.com/zhayujie/chatgpt-on-wechat/pull/565))

>**2023.03.09Ôºö** Âü∫‰∫é `whisper API`(ÂêéÁª≠Â∑≤Êé•ÂÖ•Êõ¥Â§öÁöÑËØ≠Èü≥`API`ÊúçÂä°) ÂÆûÁé∞ÂØπÂæÆ‰ø°ËØ≠Èü≥Ê∂àÊÅØÁöÑËß£ÊûêÂíåÂõûÂ§çÔºåÊ∑ªÂä†ÈÖçÁΩÆÈ°π `""speech_recognition"":true` Âç≥ÂèØÂêØÁî®Ôºå‰ΩøÁî®ÂèÇËÄÉ [#415](https://github.com/zhayujie/chatgpt-on-wechat/issues/415)„ÄÇ(contributed by [wanggang1987](https://github.com/wanggang1987) in [#385](https://github.com/zhayujie/chatgpt-on-wechat/pull/385))

>**2023.02.09Ôºö** Êâ´Á†ÅÁôªÂΩïÂ≠òÂú®Ë¥¶Âè∑ÈôêÂà∂È£éÈô©ÔºåËØ∑Ë∞®ÊÖé‰ΩøÁî®ÔºåÂèÇËÄÉ[#58](https://github.com/AutumnWhj/ChatGPT-wechat-bot/issues/158)

# Âø´ÈÄüÂºÄÂßã

Âø´ÈÄüÂºÄÂßãÊñáÊ°£Ôºö[È°πÁõÆÊê≠Âª∫ÊñáÊ°£](https://docs.link-ai.tech/cow/quick-start)

## ÂáÜÂ§á

### 1. Ë¥¶Âè∑Ê≥®ÂÜå

È°πÁõÆÈªòËÆ§‰ΩøÁî®OpenAIÊé•Âè£ÔºåÈúÄÂâçÂæÄ [OpenAIÊ≥®ÂÜåÈ°µÈù¢](https://beta.openai.com/signup) ÂàõÂª∫Ë¥¶Âè∑ÔºåÂàõÂª∫ÂÆåË¥¶Âè∑ÂàôÂâçÂæÄ [APIÁÆ°ÁêÜÈ°µÈù¢](https://beta.openai.com/account/api-keys) ÂàõÂª∫‰∏Ä‰∏™ API Key Âπ∂‰øùÂ≠ò‰∏ãÊù•ÔºåÂêéÈù¢ÈúÄË¶ÅÂú®È°πÁõÆ‰∏≠ÈÖçÁΩÆËøô‰∏™key„ÄÇÊé•Âè£ÈúÄË¶ÅÊµ∑Â§ñÁΩëÁªúËÆøÈóÆÂèäÁªëÂÆö‰ø°Áî®Âç°ÊîØ‰ªò„ÄÇ

> ÈªòËÆ§ÂØπËØùÊ®°ÂûãÊòØ openai ÁöÑ gpt-3.5-turboÔºåËÆ°Ë¥πÊñπÂºèÊòØÁ∫¶ÊØè 1000tokens (Á∫¶750‰∏™Ëã±ÊñáÂçïËØç Êàñ 500Ê±âÂ≠óÔºåÂåÖÂê´ËØ∑Ê±ÇÂíåÂõûÂ§ç) Ê∂àËÄó $0.002ÔºåÂõæÁâáÁîüÊàêÊòØDell EÊ®°ÂûãÔºåÊØèÂº†Ê∂àËÄó $0.016„ÄÇ

È°πÁõÆÂêåÊó∂‰πüÊîØÊåÅ‰ΩøÁî® LinkAI Êé•Âè£ÔºåÊó†ÈúÄ‰ª£ÁêÜÔºåÂèØ‰ΩøÁî® ÊñáÂøÉ„ÄÅËÆØÈ£û„ÄÅGPT-3„ÄÅGPT-4 Á≠âÊ®°ÂûãÔºåÊîØÊåÅ ÂÆöÂà∂ÂåñÁü•ËØÜÂ∫ì„ÄÅËÅîÁΩëÊêúÁ¥¢„ÄÅMJÁªòÂõæ„ÄÅÊñáÊ°£ÊÄªÁªìÂíåÂØπËØùÁ≠âËÉΩÂäõ„ÄÇ‰øÆÊîπÈÖçÁΩÆÂç≥ÂèØ‰∏ÄÈîÆÂàáÊç¢ÔºåÂèÇËÄÉ [Êé•ÂÖ•ÊñáÊ°£](https://link-ai.tech/platform/link-app/wechat)„ÄÇ

### 2.ËøêË°åÁéØÂ¢É

ÊîØÊåÅ Linux„ÄÅMacOS„ÄÅWindows Á≥ªÁªüÔºàÂèØÂú®LinuxÊúçÂä°Âô®‰∏äÈïøÊúüËøêË°å)ÔºåÂêåÊó∂ÈúÄÂÆâË£Ö `Python`„ÄÇ
> Âª∫ËÆÆPythonÁâàÊú¨Âú® 3.7.1~3.9.X ‰πãÈó¥ÔºåÊé®Ëçê3.8ÁâàÊú¨Ôºå3.10Âèä‰ª•‰∏äÁâàÊú¨Âú® MacOS ÂèØÁî®ÔºåÂÖ∂‰ªñÁ≥ªÁªü‰∏ä‰∏çÁ°ÆÂÆöËÉΩÂê¶Ê≠£Â∏∏ËøêË°å„ÄÇ

> Ê≥®ÊÑèÔºöDocker Êàñ Railway ÈÉ®ÁΩ≤Êó†ÈúÄÂÆâË£ÖpythonÁéØÂ¢ÉÂíå‰∏ãËΩΩÊ∫êÁ†ÅÔºåÂèØÁõ¥Êé•Âø´ËøõÂà∞‰∏ã‰∏ÄËäÇ„ÄÇ

**(1) ÂÖãÈöÜÈ°πÁõÆ‰ª£Á†ÅÔºö**

```bash
git clone https://github.com/zhayujie/chatgpt-on-wechat
cd chatgpt-on-wechat/
```

Ê≥®: Â¶ÇÈÅáÂà∞ÁΩëÁªúÈóÆÈ¢òÂèØÈÄâÊã©ÂõΩÂÜÖÈïúÂÉè https://gitee.com/zhayujie/chatgpt-on-wechat

**(2) ÂÆâË£ÖÊ†∏ÂøÉ‰æùËµñ (ÂøÖÈÄâ)Ôºö**
> ËÉΩÂ§ü‰ΩøÁî®`itchat`ÂàõÂª∫Êú∫Âô®‰∫∫ÔºåÂπ∂ÂÖ∑ÊúâÊñáÂ≠ó‰∫§ÊµÅÂäüËÉΩÊâÄÈúÄÁöÑÊúÄÂ∞è‰æùËµñÈõÜÂêà„ÄÇ
```bash
pip3 install -r requirements.txt
```

**(3) ÊãìÂ±ï‰æùËµñ (ÂèØÈÄâÔºåÂª∫ËÆÆÂÆâË£Ö)Ôºö**

```bash
pip3 install -r requirements-optional.txt
```
> Â¶ÇÊûúÊüêÈ°π‰æùËµñÂÆâË£ÖÂ§±Ë¥•ÂèØÊ≥®ÈáäÊéâÂØπÂ∫îÁöÑË°åÂÜçÁªßÁª≠

## ÈÖçÁΩÆ

ÈÖçÁΩÆÊñá‰ª∂ÁöÑÊ®°ÊùøÂú®Ê†πÁõÆÂΩïÁöÑ`config-template.json`‰∏≠ÔºåÈúÄÂ§çÂà∂ËØ•Ê®°ÊùøÂàõÂª∫ÊúÄÁªàÁîüÊïàÁöÑ `config.json` Êñá‰ª∂Ôºö

```bash
  cp config-template.json config.json
```

ÁÑ∂ÂêéÂú®`config.json`‰∏≠Â°´ÂÖ•ÈÖçÁΩÆÔºå‰ª•‰∏ãÊòØÂØπÈªòËÆ§ÈÖçÁΩÆÁöÑËØ¥ÊòéÔºåÂèØÊ†πÊçÆÈúÄË¶ÅËøõË°åËá™ÂÆö‰πâ‰øÆÊîπÔºàËØ∑ÂéªÊéâÊ≥®ÈáäÔºâÔºö

```bash
# config.jsonÊñá‰ª∂ÂÜÖÂÆπÁ§∫‰æã
{
  ""open_ai_api_key"": ""YOUR API KEY"",                          # Â°´ÂÖ•‰∏äÈù¢ÂàõÂª∫ÁöÑ OpenAI API KEY
  ""model"": ""gpt-3.5-turbo"",                                   # Ê®°ÂûãÂêçÁß∞, ÊîØÊåÅ gpt-3.5-turbo, gpt-3.5-turbo-16k, gpt-4, wenxin, xunfei
  ""proxy"": """",                                                # ‰ª£ÁêÜÂÆ¢Êà∑Á´ØÁöÑipÂíåÁ´ØÂè£ÔºåÂõΩÂÜÖÁéØÂ¢ÉÂºÄÂêØ‰ª£ÁêÜÁöÑÈúÄË¶ÅÂ°´ÂÜôËØ•È°πÔºåÂ¶Ç ""127.0.0.1:7890""
  ""single_chat_prefix"": [""bot"", ""@bot""],                      # ÁßÅËÅäÊó∂ÊñáÊú¨ÈúÄË¶ÅÂåÖÂê´ËØ•ÂâçÁºÄÊâçËÉΩËß¶ÂèëÊú∫Âô®‰∫∫ÂõûÂ§ç
  ""single_chat_reply_prefix"": ""[bot] "",                       # ÁßÅËÅäÊó∂Ëá™Âä®ÂõûÂ§çÁöÑÂâçÁºÄÔºåÁî®‰∫éÂå∫ÂàÜÁúü‰∫∫
  ""group_chat_prefix"": [""@bot""],                              # Áæ§ËÅäÊó∂ÂåÖÂê´ËØ•ÂâçÁºÄÂàô‰ºöËß¶ÂèëÊú∫Âô®‰∫∫ÂõûÂ§ç
  ""group_name_white_list"": [""ChatGPTÊµãËØïÁæ§"", ""ChatGPTÊµãËØïÁæ§2""], # ÂºÄÂêØËá™Âä®ÂõûÂ§çÁöÑÁæ§ÂêçÁß∞ÂàóË°®
  ""group_chat_in_one_session"": [""ChatGPTÊµãËØïÁæ§""],              # ÊîØÊåÅ‰ºöËØù‰∏ä‰∏ãÊñáÂÖ±‰∫´ÁöÑÁæ§ÂêçÁß∞  
  ""image_create_prefix"": [""Áîª"", ""Áúã"", ""Êâæ""],                   # ÂºÄÂêØÂõæÁâáÂõûÂ§çÁöÑÂâçÁºÄ
  ""conversation_max_tokens"": 1000,                            # ÊîØÊåÅ‰∏ä‰∏ãÊñáËÆ∞ÂøÜÁöÑÊúÄÂ§öÂ≠óÁ¨¶Êï∞
  ""speech_recognition"": false,                                # ÊòØÂê¶ÂºÄÂêØËØ≠Èü≥ËØÜÂà´
  ""group_speech_recognition"": false,                          # ÊòØÂê¶ÂºÄÂêØÁæ§ÁªÑËØ≠Èü≥ËØÜÂà´
  ""use_azure_chatgpt"": false,                                 # ÊòØÂê¶‰ΩøÁî®Azure ChatGPT service‰ª£Êõøopenai ChatGPT service. ÂΩìËÆæÁΩÆ‰∏∫trueÊó∂ÈúÄË¶ÅËÆæÁΩÆ open_ai_api_baseÔºåÂ¶Ç https://xxx.openai.azure.com/
  ""azure_deployment_id"": """",                                  # ÈááÁî®Azure ChatGPTÊó∂ÔºåÊ®°ÂûãÈÉ®ÁΩ≤ÂêçÁß∞
  ""azure_api_version"": """",                                    # ÈááÁî®Azure ChatGPTÊó∂ÔºåAPIÁâàÊú¨
  ""character_desc"": ""‰Ω†ÊòØChatGPT, ‰∏Ä‰∏™Áî±OpenAIËÆ≠ÁªÉÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã, ‰Ω†Êó®Âú®ÂõûÁ≠îÂπ∂Ëß£ÂÜ≥‰∫∫‰ª¨ÁöÑ‰ªª‰ΩïÈóÆÈ¢òÔºåÂπ∂‰∏îÂèØ‰ª•‰ΩøÁî®Â§öÁßçËØ≠Ë®Ä‰∏é‰∫∫‰∫§ÊµÅ„ÄÇ"",  # ‰∫∫Ê†ºÊèèËø∞
  # ËÆ¢ÈòÖÊ∂àÊÅØÔºåÂÖ¨‰ºóÂè∑Âíå‰ºÅ‰∏öÂæÆ‰ø°channel‰∏≠ËØ∑Â°´ÂÜôÔºåÂΩìË¢´ËÆ¢ÈòÖÊó∂‰ºöËá™Âä®ÂõûÂ§çÔºåÂèØ‰ΩøÁî®ÁâπÊÆäÂç†‰ΩçÁ¨¶„ÄÇÁõÆÂâçÊîØÊåÅÁöÑÂç†‰ΩçÁ¨¶Êúâ{trigger_prefix}ÔºåÂú®Á®ãÂ∫è‰∏≠ÂÆÉ‰ºöËá™Âä®ÊõøÊç¢ÊàêbotÁöÑËß¶ÂèëËØç„ÄÇ
  ""subscribe_msg"": ""ÊÑüË∞¢ÊÇ®ÁöÑÂÖ≥Ê≥®ÔºÅ\nËøôÈáåÊòØChatGPTÔºåÂèØ‰ª•Ëá™Áî±ÂØπËØù„ÄÇ\nÊîØÊåÅËØ≠Èü≥ÂØπËØù„ÄÇ\nÊîØÊåÅÂõæÁâáËæìÂá∫ÔºåÁîªÂ≠óÂºÄÂ§¥ÁöÑÊ∂àÊÅØÂ∞ÜÊåâË¶ÅÊ±ÇÂàõ‰ΩúÂõæÁâá„ÄÇ\nÊîØÊåÅËßíËâ≤ÊâÆÊºîÂíåÊñáÂ≠óÂÜíÈô©Á≠â‰∏∞ÂØåÊèí‰ª∂„ÄÇ\nËæìÂÖ•{trigger_prefix}#help Êü•ÁúãËØ¶ÁªÜÊåá‰ª§„ÄÇ"",
  ""use_linkai"": false,                                        # ÊòØÂê¶‰ΩøÁî®LinkAIÊé•Âè£ÔºåÈªòËÆ§ÂÖ≥Èó≠ÔºåÂºÄÂêØÂêéÂèØÂõΩÂÜÖËÆøÈóÆÔºå‰ΩøÁî®Áü•ËØÜÂ∫ìÂíåMJ
  ""linkai_api_key"": """",                                       # LinkAI Api Key
  ""linkai_app_code"": """"                                       # LinkAI Â∫îÁî®code
}
```
**ÈÖçÁΩÆËØ¥ÊòéÔºö**

**1.‰∏™‰∫∫ËÅäÂ§©**

+ ‰∏™‰∫∫ËÅäÂ§©‰∏≠ÔºåÈúÄË¶Å‰ª• ""bot""Êàñ""@bot"" ‰∏∫ÂºÄÂ§¥ÁöÑÂÜÖÂÆπËß¶ÂèëÊú∫Âô®‰∫∫ÔºåÂØπÂ∫îÈÖçÁΩÆÈ°π `single_chat_prefix` (Â¶ÇÊûú‰∏çÈúÄË¶Å‰ª•ÂâçÁºÄËß¶ÂèëÂèØ‰ª•Â°´ÂÜô  `""single_chat_prefix"": [""""]`)
+ Êú∫Âô®‰∫∫ÂõûÂ§çÁöÑÂÜÖÂÆπ‰ºö‰ª• ""[bot] "" ‰Ωú‰∏∫ÂâçÁºÄÔºå ‰ª•Âå∫ÂàÜÁúü‰∫∫ÔºåÂØπÂ∫îÁöÑÈÖçÁΩÆÈ°π‰∏∫ `single_chat_reply_prefix` (Â¶ÇÊûú‰∏çÈúÄË¶ÅÂâçÁºÄÂèØ‰ª•Â°´ÂÜô `""single_chat_reply_prefix"": """"`)

**2.Áæ§ÁªÑËÅäÂ§©**

+ Áæ§ÁªÑËÅäÂ§©‰∏≠ÔºåÁæ§ÂêçÁß∞ÈúÄÈÖçÁΩÆÂú® `group_name_white_list ` ‰∏≠ÊâçËÉΩÂºÄÂêØÁæ§ËÅäËá™Âä®ÂõûÂ§ç„ÄÇÂ¶ÇÊûúÊÉ≥ÂØπÊâÄÊúâÁæ§ËÅäÁîüÊïàÔºåÂèØ‰ª•Áõ¥Êé•Â°´ÂÜô `""group_name_white_list"": [""ALL_GROUP""]`
+ ÈªòËÆ§Âè™Ë¶ÅË¢´‰∫∫ @ Â∞±‰ºöËß¶ÂèëÊú∫Âô®‰∫∫Ëá™Âä®ÂõûÂ§çÔºõÂè¶Â§ñÁæ§ËÅäÂ§©‰∏≠Âè™Ë¶ÅÊ£ÄÊµãÂà∞‰ª• ""@bot"" ÂºÄÂ§¥ÁöÑÂÜÖÂÆπÔºåÂêåÊ†∑‰ºöËá™Âä®ÂõûÂ§çÔºàÊñπ‰æøËá™Â∑±Ëß¶ÂèëÔºâÔºåËøôÂØπÂ∫îÈÖçÁΩÆÈ°π `group_chat_prefix`
+ ÂèØÈÄâÈÖçÁΩÆ: `group_name_keyword_white_list`ÈÖçÁΩÆÈ°πÊîØÊåÅÊ®°Á≥äÂåπÈÖçÁæ§ÂêçÁß∞Ôºå`group_chat_keyword`ÈÖçÁΩÆÈ°πÂàôÊîØÊåÅÊ®°Á≥äÂåπÈÖçÁæ§Ê∂àÊÅØÂÜÖÂÆπÔºåÁî®Ê≥ï‰∏é‰∏äËø∞‰∏§‰∏™ÈÖçÁΩÆÈ°πÁõ∏Âêå„ÄÇÔºàContributed by [evolay](https://github.com/evolay))
+ `group_chat_in_one_session`Ôºö‰ΩøÁæ§ËÅäÂÖ±‰∫´‰∏Ä‰∏™‰ºöËØù‰∏ä‰∏ãÊñáÔºåÈÖçÁΩÆ `[""ALL_GROUP""]` Âàô‰ΩúÁî®‰∫éÊâÄÊúâÁæ§ËÅä

**3.ËØ≠Èü≥ËØÜÂà´**

+ Ê∑ªÂä† `""speech_recognition"": true` Â∞ÜÂºÄÂêØËØ≠Èü≥ËØÜÂà´ÔºåÈªòËÆ§‰ΩøÁî®openaiÁöÑwhisperÊ®°ÂûãËØÜÂà´‰∏∫ÊñáÂ≠óÔºåÂêåÊó∂‰ª•ÊñáÂ≠óÂõûÂ§çÔºåËØ•ÂèÇÊï∞‰ªÖÊîØÊåÅÁßÅËÅä (Ê≥®ÊÑèÁî±‰∫éËØ≠Èü≥Ê∂àÊÅØÊó†Ê≥ïÂåπÈÖçÂâçÁºÄÔºå‰∏ÄÊó¶ÂºÄÂêØÂ∞ÜÂØπÊâÄÊúâËØ≠Èü≥Ëá™Âä®ÂõûÂ§çÔºåÊîØÊåÅËØ≠Èü≥Ëß¶ÂèëÁîªÂõæ)Ôºõ
+ Ê∑ªÂä† `""group_speech_recognition"": true` Â∞ÜÂºÄÂêØÁæ§ÁªÑËØ≠Èü≥ËØÜÂà´ÔºåÈªòËÆ§‰ΩøÁî®openaiÁöÑwhisperÊ®°ÂûãËØÜÂà´‰∏∫ÊñáÂ≠óÔºåÂêåÊó∂‰ª•ÊñáÂ≠óÂõûÂ§çÔºåÂèÇÊï∞‰ªÖÊîØÊåÅÁæ§ËÅä (‰ºöÂåπÈÖçgroup_chat_prefixÂíågroup_chat_keyword, ÊîØÊåÅËØ≠Èü≥Ëß¶ÂèëÁîªÂõæ)Ôºõ
+ Ê∑ªÂä† `""voice_reply_voice"": true` Â∞ÜÂºÄÂêØËØ≠Èü≥ÂõûÂ§çËØ≠Èü≥ÔºàÂêåÊó∂‰ΩúÁî®‰∫éÁßÅËÅäÂíåÁæ§ËÅäÔºâÔºå‰ΩÜÊòØÈúÄË¶ÅÈÖçÁΩÆÂØπÂ∫îËØ≠Èü≥ÂêàÊàêÂπ≥Âè∞ÁöÑkeyÔºåÁî±‰∫éitchatÂçèËÆÆÁöÑÈôêÂà∂ÔºåÂè™ËÉΩÂèëÈÄÅËØ≠Èü≥mp3Êñá‰ª∂ÔºåËã•‰ΩøÁî®wechatyÂàôÂõûÂ§çÁöÑÊòØÂæÆ‰ø°ËØ≠Èü≥„ÄÇ

**4.ÂÖ∂‰ªñÈÖçÁΩÆ**

+ `model`: Ê®°ÂûãÂêçÁß∞ÔºåÁõÆÂâçÊîØÊåÅ `gpt-3.5-turbo`, `text-davinci-003`, `gpt-4`, `gpt-4-32k`, `wenxin` , `claude` ,  `xunfei`(ÂÖ∂‰∏≠gpt-4 apiÊöÇÊú™ÂÆåÂÖ®ÂºÄÊîæÔºåÁî≥ËØ∑ÈÄöËøáÂêéÂèØ‰ΩøÁî®)
+ `temperature`,`frequency_penalty`,`presence_penalty`: Chat APIÊé•Âè£ÂèÇÊï∞ÔºåËØ¶ÊÉÖÂèÇËÄÉ[OpenAIÂÆòÊñπÊñáÊ°£„ÄÇ](https://platform.openai.com/docs/api-reference/chat)
+ `proxy`ÔºöÁî±‰∫éÁõÆÂâç `openai` Êé•Âè£ÂõΩÂÜÖÊó†Ê≥ïËÆøÈóÆÔºåÈúÄÈÖçÁΩÆ‰ª£ÁêÜÂÆ¢Êà∑Á´ØÁöÑÂú∞ÂùÄÔºåËØ¶ÊÉÖÂèÇËÄÉ  [#351](https://github.com/zhayujie/chatgpt-on-wechat/issues/351)
+ ÂØπ‰∫éÂõæÂÉèÁîüÊàêÔºåÂú®Êª°Ë∂≥‰∏™‰∫∫ÊàñÁæ§ÁªÑËß¶ÂèëÊù°‰ª∂Â§ñÔºåËøòÈúÄË¶ÅÈ¢ùÂ§ñÁöÑÂÖ≥ÈîÆËØçÂâçÁºÄÊù•Ëß¶ÂèëÔºåÂØπÂ∫îÈÖçÁΩÆ `image_create_prefix `
+ ÂÖ≥‰∫éOpenAIÂØπËØùÂèäÂõæÁâáÊé•Âè£ÁöÑÂèÇÊï∞ÈÖçÁΩÆÔºàÂÜÖÂÆπËá™Áî±Â∫¶„ÄÅÂõûÂ§çÂ≠óÊï∞ÈôêÂà∂„ÄÅÂõæÁâáÂ§ßÂ∞èÁ≠âÔºâÔºåÂèØ‰ª•ÂèÇËÄÉ [ÂØπËØùÊé•Âè£](https://beta.openai.com/docs/api-reference/completions) Âíå [ÂõæÂÉèÊé•Âè£](https://beta.openai.com/docs/api-reference/completions)  ÊñáÊ°£ÔºåÂú®[`config.py`](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/config.py)‰∏≠Ê£ÄÊü•Âì™‰∫õÂèÇÊï∞Âú®Êú¨È°πÁõÆ‰∏≠ÊòØÂèØÈÖçÁΩÆÁöÑ„ÄÇ
+ `conversation_max_tokens`ÔºöË°®Á§∫ËÉΩÂ§üËÆ∞ÂøÜÁöÑ‰∏ä‰∏ãÊñáÊúÄÂ§ßÂ≠óÊï∞Ôºà‰∏ÄÈóÆ‰∏ÄÁ≠î‰∏∫‰∏ÄÁªÑÂØπËØùÔºåÂ¶ÇÊûúÁ¥ØÁßØÁöÑÂØπËØùÂ≠óÊï∞Ë∂ÖÂá∫ÈôêÂà∂ÔºåÂ∞±‰ºö‰ºòÂÖàÁßªÈô§ÊúÄÊó©ÁöÑ‰∏ÄÁªÑÂØπËØùÔºâ
+ `rate_limit_chatgpt`Ôºå`rate_limit_dalle`ÔºöÊØèÂàÜÈíüÊúÄÈ´òÈóÆÁ≠îÈÄüÁéá„ÄÅÁîªÂõæÈÄüÁéáÔºåË∂ÖÈÄüÂêéÊéíÈòüÊåâÂ∫èÂ§ÑÁêÜ„ÄÇ
+ `clear_memory_commands`: ÂØπËØùÂÜÖÊåá‰ª§Ôºå‰∏ªÂä®Ê∏ÖÁ©∫ÂâçÊñáËÆ∞ÂøÜÔºåÂ≠óÁ¨¶‰∏≤Êï∞ÁªÑÂèØËá™ÂÆö‰πâÊåá‰ª§Âà´Âêç„ÄÇ
+ `hot_reload`: Á®ãÂ∫èÈÄÄÂá∫ÂêéÔºåÊöÇÂ≠òÂæÆ‰ø°Êâ´Á†ÅÁä∂ÊÄÅÔºåÈªòËÆ§ÂÖ≥Èó≠„ÄÇ
+ `character_desc` ÈÖçÁΩÆ‰∏≠‰øùÂ≠òÁùÄ‰Ω†ÂØπÊú∫Âô®‰∫∫ËØ¥ÁöÑ‰∏ÄÊÆµËØùÔºå‰ªñ‰ºöËÆ∞‰ΩèËøôÊÆµËØùÂπ∂‰Ωú‰∏∫‰ªñÁöÑËÆæÂÆöÔºå‰Ω†ÂèØ‰ª•‰∏∫‰ªñÂÆöÂà∂‰ªª‰Ωï‰∫∫Ê†º      (ÂÖ≥‰∫é‰ºöËØù‰∏ä‰∏ãÊñáÁöÑÊõ¥Â§öÂÜÖÂÆπÂèÇËÄÉËØ• [issue](https://github.com/zhayujie/chatgpt-on-wechat/issues/43))
+ `subscribe_msg`ÔºöËÆ¢ÈòÖÊ∂àÊÅØÔºåÂÖ¨‰ºóÂè∑Âíå‰ºÅ‰∏öÂæÆ‰ø°channel‰∏≠ËØ∑Â°´ÂÜôÔºåÂΩìË¢´ËÆ¢ÈòÖÊó∂‰ºöËá™Âä®ÂõûÂ§çÔºå ÂèØ‰ΩøÁî®ÁâπÊÆäÂç†‰ΩçÁ¨¶„ÄÇÁõÆÂâçÊîØÊåÅÁöÑÂç†‰ΩçÁ¨¶Êúâ{trigger_prefix}ÔºåÂú®Á®ãÂ∫è‰∏≠ÂÆÉ‰ºöËá™Âä®ÊõøÊç¢ÊàêbotÁöÑËß¶ÂèëËØç„ÄÇ

**5.LinkAIÈÖçÁΩÆ (ÂèØÈÄâ)**

+ `use_linkai`: ÊòØÂê¶‰ΩøÁî®LinkAIÊé•Âè£ÔºåÂºÄÂêØÂêéÂèØÂõΩÂÜÖËÆøÈóÆÔºå‰ΩøÁî®Áü•ËØÜÂ∫ìÂíå `Midjourney` ÁªòÁîª, ÂèÇËÄÉ [ÊñáÊ°£](https://link-ai.tech/platform/link-app/wechat)
+ `linkai_api_key`: LinkAI Api KeyÔºåÂèØÂú® [ÊéßÂà∂Âè∞](https://link-ai.tech/console/interface) ÂàõÂª∫
+ `linkai_app_code`: LinkAI Â∫îÁî®codeÔºåÈÄâÂ°´

**Êú¨ËØ¥ÊòéÊñáÊ°£ÂèØËÉΩ‰ºöÊú™ÂèäÊó∂Êõ¥Êñ∞ÔºåÂΩìÂâçÊâÄÊúâÂèØÈÄâÁöÑÈÖçÁΩÆÈ°πÂùáÂú®ËØ•[`config.py`](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/config.py)‰∏≠ÂàóÂá∫„ÄÇ**

## ËøêË°å

### 1.Êú¨Âú∞ËøêË°å

Â¶ÇÊûúÊòØÂºÄÂèëÊú∫ **Êú¨Âú∞ËøêË°å**ÔºåÁõ¥Êé•Âú®È°πÁõÆÊ†πÁõÆÂΩï‰∏ãÊâßË°åÔºö

```bash
python3 app.py                                    # windowsÁéØÂ¢É‰∏ãËØ•ÂëΩ‰ª§ÈÄöÂ∏∏‰∏∫ python app.py
```

ÁªàÁ´ØËæìÂá∫‰∫åÁª¥Á†ÅÂêéÔºå‰ΩøÁî®ÂæÆ‰ø°ËøõË°åÊâ´Á†ÅÔºåÂΩìËæìÂá∫ ""Start auto replying"" Êó∂Ë°®Á§∫Ëá™Âä®ÂõûÂ§çÁ®ãÂ∫èÂ∑≤ÁªèÊàêÂäüËøêË°å‰∫ÜÔºàÊ≥®ÊÑèÔºöÁî®‰∫éÁôªÂΩïÁöÑÂæÆ‰ø°ÈúÄË¶ÅÂú®ÊîØ‰ªòÂ§ÑÂ∑≤ÂÆåÊàêÂÆûÂêçËÆ§ËØÅÔºâ„ÄÇÊâ´Á†ÅÁôªÂΩïÂêé‰Ω†ÁöÑË¥¶Âè∑Â∞±Êàê‰∏∫Êú∫Âô®‰∫∫‰∫ÜÔºåÂèØ‰ª•Âú®ÂæÆ‰ø°ÊâãÊú∫Á´ØÈÄöËøáÈÖçÁΩÆÁöÑÂÖ≥ÈîÆËØçËß¶ÂèëËá™Âä®ÂõûÂ§ç (‰ªªÊÑèÂ•ΩÂèãÂèëÈÄÅÊ∂àÊÅØÁªô‰Ω†ÔºåÊàñÊòØËá™Â∑±ÂèëÊ∂àÊÅØÁªôÂ•ΩÂèã)ÔºåÂèÇËÄÉ[#142](https://github.com/zhayujie/chatgpt-on-wechat/issues/142)„ÄÇ

### 2.ÊúçÂä°Âô®ÈÉ®ÁΩ≤

‰ΩøÁî®nohupÂëΩ‰ª§Âú®ÂêéÂè∞ËøêË°åÁ®ãÂ∫èÔºö

```bash
nohup python3 app.py & tail -f nohup.out          # Âú®ÂêéÂè∞ËøêË°åÁ®ãÂ∫èÂπ∂ÈÄöËøáÊó•ÂøóËæìÂá∫‰∫åÁª¥Á†Å
```
Êâ´Á†ÅÁôªÂΩïÂêéÁ®ãÂ∫èÂç≥ÂèØËøêË°å‰∫éÊúçÂä°Âô®ÂêéÂè∞ÔºåÊ≠§Êó∂ÂèØÈÄöËøá `ctrl+c` ÂÖ≥Èó≠Êó•ÂøóÔºå‰∏ç‰ºöÂΩ±ÂìçÂêéÂè∞Á®ãÂ∫èÁöÑËøêË°å„ÄÇ‰ΩøÁî® `ps -ef | grep app.py | grep -v grep` ÂëΩ‰ª§ÂèØÊü•ÁúãËøêË°å‰∫éÂêéÂè∞ÁöÑËøõÁ®ãÔºåÂ¶ÇÊûúÊÉ≥Ë¶ÅÈáçÊñ∞ÂêØÂä®Á®ãÂ∫èÂèØ‰ª•ÂÖà `kill` ÊéâÂØπÂ∫îÁöÑËøõÁ®ã„ÄÇÊó•ÂøóÂÖ≥Èó≠ÂêéÂ¶ÇÊûúÊÉ≥Ë¶ÅÂÜçÊ¨°ÊâìÂºÄÂè™ÈúÄËæìÂÖ•¬†`tail -f nohup.out`„ÄÇÊ≠§Â§ñÔºå`scripts` ÁõÆÂΩï‰∏ãÊúâ‰∏ÄÈîÆËøêË°å„ÄÅÂÖ≥Èó≠Á®ãÂ∫èÁöÑËÑöÊú¨‰æõ‰ΩøÁî®„ÄÇ

> **Â§öË¥¶Âè∑ÊîØÊåÅÔºö** Â∞ÜÈ°πÁõÆÂ§çÂà∂Â§ö‰ªΩÔºåÂàÜÂà´ÂêØÂä®Á®ãÂ∫èÔºåÁî®‰∏çÂêåË¥¶Âè∑Êâ´Á†ÅÁôªÂΩïÂç≥ÂèØÂÆûÁé∞ÂêåÊó∂ËøêË°å„ÄÇ

> **ÁâπÊÆäÊåá‰ª§Ôºö** Áî®Êà∑ÂêëÊú∫Âô®‰∫∫ÂèëÈÄÅ **#reset** Âç≥ÂèØÊ∏ÖÁ©∫ËØ•Áî®Êà∑ÁöÑ‰∏ä‰∏ãÊñáËÆ∞ÂøÜ„ÄÇ


### 3.DockerÈÉ®ÁΩ≤

> ‰ΩøÁî®dockerÈÉ®ÁΩ≤Êó†ÈúÄ‰∏ãËΩΩÊ∫êÁ†ÅÂíåÂÆâË£Ö‰æùËµñÔºåÂè™ÈúÄË¶ÅËé∑Âèñ docker-compose.yml ÈÖçÁΩÆÊñá‰ª∂Âπ∂ÂêØÂä®ÂÆπÂô®Âç≥ÂèØ„ÄÇ

> ÂâçÊèêÊòØÈúÄË¶ÅÂÆâË£ÖÂ•Ω `docker` Âèä `docker-compose`ÔºåÂÆâË£ÖÊàêÂäüÁöÑË°®Áé∞ÊòØÊâßË°å `docker -v` Âíå `docker-compose version` (Êàñ docker compose version) ÂèØ‰ª•Êü•ÁúãÂà∞ÁâàÊú¨Âè∑ÔºåÂèØÂâçÂæÄ [dockerÂÆòÁΩë](https://docs.docker.com/engine/install/) ËøõË°å‰∏ãËΩΩ„ÄÇ

#### (1) ‰∏ãËΩΩ docker-compose.yml Êñá‰ª∂

```bash
wget https://open-1317903499.cos.ap-guangzhou.myqcloud.com/docker-compose.yml
```

‰∏ãËΩΩÂÆåÊàêÂêéÊâìÂºÄ `docker-compose.yml` ‰øÆÊîπÊâÄÈúÄÈÖçÁΩÆÔºåÂ¶Ç `OPEN_AI_API_KEY` Âíå `GROUP_NAME_WHITE_LIST` Á≠â„ÄÇ

#### (2) ÂêØÂä®ÂÆπÂô®

Âú® `docker-compose.yml` ÊâÄÂú®ÁõÆÂΩï‰∏ãÊâßË°å‰ª•‰∏ãÂëΩ‰ª§ÂêØÂä®ÂÆπÂô®Ôºö

```bash
sudo docker compose up -d
```

ËøêË°å `sudo docker ps` ËÉΩÊü•ÁúãÂà∞ NAMES ‰∏∫ chatgpt-on-wechat ÁöÑÂÆπÂô®Âç≥Ë°®Á§∫ËøêË°åÊàêÂäü„ÄÇ

Ê≥®ÊÑèÔºö

 - Â¶ÇÊûú `docker-compose` ÊòØ 1.X ÁâàÊú¨ ÂàôÈúÄË¶ÅÊâßË°å `sudo  docker-compose up -d` Êù•ÂêØÂä®ÂÆπÂô®
 - ËØ•ÂëΩ‰ª§‰ºöËá™Âä®Âéª [docker hub](https://hub.docker.com/r/zhayujie/chatgpt-on-wechat) ÊãâÂèñ latest ÁâàÊú¨ÁöÑÈïúÂÉèÔºålatest ÈïúÂÉè‰ºöÂú®ÊØèÊ¨°È°πÁõÆ release Êñ∞ÁöÑÁâàÊú¨Êó∂ÁîüÊàê

ÊúÄÂêéËøêË°å‰ª•‰∏ãÂëΩ‰ª§ÂèØÊü•ÁúãÂÆπÂô®ËøêË°åÊó•ÂøóÔºåÊâ´ÊèèÊó•Âøó‰∏≠ÁöÑ‰∫åÁª¥Á†ÅÂç≥ÂèØÂÆåÊàêÁôªÂΩïÔºö

```bash
sudo docker logs -f chatgpt-on-wechat
```

#### (3) Êèí‰ª∂‰ΩøÁî®

Â¶ÇÊûúÈúÄË¶ÅÂú®dockerÂÆπÂô®‰∏≠‰øÆÊîπÊèí‰ª∂ÈÖçÁΩÆÔºåÂèØÈÄöËøáÊåÇËΩΩÁöÑÊñπÂºèÂÆåÊàêÔºåÂ∞Ü [Êèí‰ª∂ÈÖçÁΩÆÊñá‰ª∂](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/plugins/config.json.template)
ÈáçÂëΩÂêç‰∏∫ `config.json`ÔºåÊîæÁΩÆ‰∫é `docker-compose.yml` Áõ∏ÂêåÁõÆÂΩï‰∏ãÔºåÂπ∂Âú® `docker-compose.yml` ‰∏≠ÁöÑ `chatgpt-on-wechat` ÈÉ®ÂàÜ‰∏ãÊ∑ªÂä† `volumes` Êò†Â∞Ñ:

```
volumes:
  - ./config.json:/app/plugins/config.json
```

### 4. RailwayÈÉ®ÁΩ≤

> Railway ÊØèÊúàÊèê‰æõ5ÂàÄÂíåÊúÄÂ§ö500Â∞èÊó∂ÁöÑÂÖçË¥πÈ¢ùÂ∫¶„ÄÇ (07.11Êõ¥Êñ∞: ÁõÆÂâçÂ§ßÈÉ®ÂàÜË¥¶Âè∑Â∑≤Êó†Ê≥ïÂÖçË¥πÈÉ®ÁΩ≤)

1. ËøõÂÖ• [Railway](https://railway.app/template/qApznZ?referralCode=RC3znh)
2. ÁÇπÂáª `Deploy Now` ÊåâÈíÆ„ÄÇ
3. ËÆæÁΩÆÁéØÂ¢ÉÂèòÈáèÊù•ÈáçËΩΩÁ®ãÂ∫èËøêË°åÁöÑÂèÇÊï∞Ôºå‰æãÂ¶Ç`open_ai_api_key`, `character_desc`„ÄÇ

**‰∏ÄÈîÆÈÉ®ÁΩ≤:**
  
  [![Deploy on Railway](https://railway.app/button.svg)](https://railway.app/template/qApznZ?referralCode=RC3znh)

## Â∏∏ËßÅÈóÆÈ¢ò

FAQsÔºö <https://github.com/zhayujie/chatgpt-on-wechat/wiki/FAQs>

ÊàñÁõ¥Êé•Âú®Á∫øÂí®ËØ¢ [È°πÁõÆÂ∞èÂä©Êâã](https://link-ai.tech/app/Kv2fXJcH)  (betaÁâàÊú¨ÔºåËØ≠ÊñôÂÆåÂñÑ‰∏≠ÔºåÂõûÂ§ç‰ªÖ‰æõÂèÇËÄÉ)

## ÂºÄÂèë

Ê¨¢ËøéÊé•ÂÖ•Êõ¥Â§öÂ∫îÁî®ÔºåÂèÇËÄÉ [Terminal‰ª£Á†Å](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/channel/terminal/terminal_channel.py) ÂÆûÁé∞Êé•Êî∂ÂíåÂèëÈÄÅÊ∂àÊÅØÈÄªËæëÂç≥ÂèØÊé•ÂÖ•„ÄÇ ÂêåÊó∂Ê¨¢ËøéÂ¢ûÂä†Êñ∞ÁöÑÊèí‰ª∂ÔºåÂèÇËÄÉ [Êèí‰ª∂ËØ¥ÊòéÊñáÊ°£](https://github.com/zhayujie/chatgpt-on-wechat/tree/master/plugins)„ÄÇ

## ËÅîÁ≥ª

Ê¨¢ËøéÊèê‰∫§PR„ÄÅIssuesÔºå‰ª•ÂèäStarÊîØÊåÅ‰∏Ä‰∏ã„ÄÇÁ®ãÂ∫èËøêË°åÈÅáÂà∞ÈóÆÈ¢òÂèØ‰ª•Êü•Áúã [Â∏∏ËßÅÈóÆÈ¢òÂàóË°®](https://github.com/zhayujie/chatgpt-on-wechat/wiki/FAQs) ÔºåÂÖ∂Ê¨°ÂâçÂæÄ [Issues](https://github.com/zhayujie/chatgpt-on-wechat/issues) ‰∏≠ÊêúÁ¥¢„ÄÇ‰∏™‰∫∫ÂºÄÂèëËÄÖÂèØÂä†ÂÖ•ÂºÄÊ∫ê‰∫§ÊµÅÁæ§ÂèÇ‰∏éÊõ¥Â§öËÆ®ËÆ∫Ôºå‰ºÅ‰∏öÁî®Êà∑ÂèØËÅîÁ≥ª[‰∫ßÂìÅÈ°æÈóÆ](https://img-1317903499.cos.ap-guangzhou.myqcloud.com/docs/product-manager-qrcode.jpg)Âí®ËØ¢„ÄÇ
",,,,0,,,
2024-02-25,https://github.com/danswer-ai/danswer,https://raw.githubusercontent.com/danswer-ai/danswer/main/README.md,"Danswer is an open-source platform that enables users to perform searches and generate AI-powered chat responses from their documents. It integrates with various workplace tools like Slack, Google Drive, and Confluence, allowing teams to access tailored answers from their specific knowledge bases. This functionality aims to improve efficiency in customer support, engineering, sales, and other departments by making information readily accessible. Danswer also supports document search, chat support with access to private knowledge sources, and the ability to create custom AI Assistants. It boasts key features like hybrid search capabilities, user authentication, an admin dashboard for management, and custom model training. Additionally, Danswer offers connectors to a wide range of services and platforms for pulling in the latest data, ensuring comprehensive coverage and ease of deployment across different environments, including Kubernetes. Future plans include enhancing organizational understanding and expanding its search capabilities further. The project welcomes contributions, inviting interested parties to review its guide for more information.","<!-- DANSWER_METADATA={""link"": ""https://github.com/danswer-ai/danswer/blob/main/README.md""} -->

<h2 align=""center"">
<a href=""https://www.danswer.ai/""> <img width=""50%"" src=""https://github.com/danswer-owners/danswer/blob/1fabd9372d66cd54238847197c33f091a724803b/DanswerWithName.png?raw=true)"" /></a>
</h2>

<p align=""center"">
<p align=""center"">Open Source Unified Search and Gen-AI Chat with your Docs.</p>

<p align=""center"">
<a href=""https://docs.danswer.dev/"" target=""_blank"">
    <img src=""https://img.shields.io/badge/docs-view-blue"" alt=""Documentation"">
</a>
<a href=""https://join.slack.com/t/danswer/shared_invite/zt-2afut44lv-Rw3kSWu6_OmdAXRpCv80DQ"" target=""_blank"">
    <img src=""https://img.shields.io/badge/slack-join-blue.svg?logo=slack"" alt=""Slack"">
</a>
<a href=""https://discord.gg/TDJ59cGV2X"" target=""_blank"">
    <img src=""https://img.shields.io/badge/discord-join-blue.svg?logo=discord&logoColor=white"" alt=""Discord"">
</a>
<a href=""https://github.com/danswer-ai/danswer/blob/main/README.md"" target=""_blank"">
    <img src=""https://img.shields.io/static/v1?label=license&message=MIT&color=blue"" alt=""License"">
</a>
</p>

<strong>[Danswer](https://www.danswer.ai/)</strong> lets you ask questions in natural language and get back
answers based on your team specific documents. Think ChatGPT if it had access to your team's unique
knowledge. Connects to all common workplace tools such as Slack, Google Drive, Confluence, etc.

Teams have used Danswer to:
- Speedup customer support and escalation turnaround time.
- Improve Engineering efficiency by making documentation and code changelogs easy to find.
- Let sales team get fuller context and faster in preparation for calls.
- Track customer requests and priorities for Product teams.
- Help teams self-serve IT, Onboarding, HR, etc.

<h3>Usage</h3>

Danswer Web App:

https://github.com/danswer-ai/danswer/assets/32520769/563be14c-9304-47b5-bf0a-9049c2b6f410


Or, plug Danswer into your existing Slack workflows (more integrations to come üòÅ):

https://github.com/danswer-ai/danswer/assets/25087905/3e19739b-d178-4371-9a38-011430bdec1b


For more details on the Admin UI to manage connectors and users, check out our 
<strong><a href=""https://www.youtube.com/watch?v=geNzY1nbCnU"">Full Video Demo</a></strong>!

## Deployment

Danswer can easily be run locally (even on a laptop) or deployed on a virtual machine with a single
`docker compose` command. Checkout our [docs](https://docs.danswer.dev/quickstart) to learn more.

We also have built-in support for deployment on Kubernetes. Files for that can be found [here](https://github.com/danswer-ai/danswer/tree/main/deployment/kubernetes).


## üíÉ Main Features 
* Document Search + AI Answers for natural language queries.
* Connectors to all common workplace tools like Google Drive, Confluence, Slack, etc.
* Chat support (think ChatGPT but it has access to your private knowledge sources).
* Create custom AI Assistants with different prompts and backing knowledge sets.
* Slack integration to get answers and search results directly in Slack.


## Other Noteable Benefits of Danswer
* Best in class Hybrid Search across all sources (BM-25 + prefix aware embedding models).
* User Authentication with document level access management.
* Admin Dashboard to configure connectors, document-sets, access, etc.
* Custom deep learning models + learn from user feedback.
* Connect Danswer with LLM of your choice for a fully airgapped solution.
* Easy deployment and ability to host Danswer anywhere of your choosing.


## üîå Connectors
Efficiently pulls the latest changes from:
  * Slack
  * GitHub
  * Google Drive
  * Confluence
  * Jira
  * Zendesk
  * Gmail
  * Notion
  * Gong
  * Slab
  * Linear
  * Productboard
  * Guru
  * Bookstack
  * Document360
  * Sharepoint
  * Hubspot
  * Local Files
  * Websites
  * And more ...

## üöß Roadmap
* Organizational understanding.
* Ability to locate and suggest experts from your team.
* Code Search
* Structured Query Languages (SQL, Excel formulas, etc.)

## üí° Contributing
Looking to contribute? Please check out the [Contribution Guide](CONTRIBUTING.md) for more details.
",,,https://www.youtube.com/watch?v=geNzY1nbCnU,0,,,
2024-02-25,https://github.com/xtekky/gpt4free,https://raw.githubusercontent.com/xtekky/gpt4free/main/README.md,"The text is about ""gpt4free,"" a repository demonstrating the development of an API package with multi-provider requests, emphasizing features like timeouts, load balance, and flow control. The author does not endorse or take responsibility for the use of this repository. It serves as a PoC, offering guides on smartphone usage and AI assistance in code writing, among other things. The repository undergoes continuous improvement based on user feedback, including updates to incorporate new library syntax and the addition of tutorials and documentation enhancements. Various models and providers, including GPT-4 and GPT-3.5, are supported, with the potential for local model development. It also includes comprehensive instructions on getting started, usage examples, and configurations for optimal operation, such as cookie management and proxy settings. The project welcomes contributions and has a list of related projects and a significant contributor. It is licensed under GNU GPL v3, and there's an encouragement for community involvement to further enrich the project.","![248433934-7886223b-c1d1-4260-82aa-da5741f303bb](https://github.com/xtekky/gpt4free/assets/98614666/ea012c87-76e0-496a-8ac4-e2de090cc6c9)
Written by [@xtekky](https://github.com/hlohaus) & maintained by [@hlohaus](https://github.com/hlohaus)

<div id=""top""></div>

> By using this repository or any code related to it, you agree to the [legal notice](LEGAL_NOTICE.md). The author is **not responsible for the usage of this repository nor endorses it**, nor is the author responsible for any copies, forks, re-uploads made by other users, or anything else related to GPT4Free. This is the author's only account and repository. To prevent impersonation or irresponsible actions, please comply with the GNU GPL license this Repository uses.  

> [!Warning]
*""gpt4free""* serves as a **PoC** (proof of concept), demonstrating the development of a an api package with multi-provider requests, with features like timeouts, load balance and flow control.

> [!Note]
<sup><strong>Lastet version:</strong></sup> [![PyPI version](https://img.shields.io/pypi/v/g4f?color=blue)](https://pypi.org/project/g4f) [![Docker version](https://img.shields.io/docker/v/hlohaus789/g4f?label=docker&color=blue)](https://hub.docker.com/r/hlohaus789/g4f)  
> <sup><strong>Stats:</strong></sup>  [![Downloads](https://static.pepy.tech/badge/g4f)](https://pepy.tech/project/g4f) [![Downloads](https://static.pepy.tech/badge/g4f/month)](https://pepy.tech/project/g4f)

```sh
pip install -U g4f
```
```sh
docker pull hlohaus789/g4f
```

## üÜï What's New
- Guide: How do I use my smartphoneüì±to run g4f?
  - [/docs/guides/phone](/docs/guides/phone.md)
- New: How can AI help me üíÅwith writing code?
  - [/docs/guides/help_me](/docs/guides/help_me.md)
- Join our Telegram Channel: [t.me/g4f_channel](https://telegram.me/g4f_channel)
- Join our Discord Group: [discord.gg/XfybzPXPH5](https://discord.gg/XfybzPXPH5)

## üîª Site Takedown
Is your site on this repository and you want to take it down ? email takedown@g4f.ai with proof it is yours and it will be removed as fast as possible. - to prevent reproduction please secure your api  )

## üöÄ  Feedback and Todo
You can always leave some feedback here: https://forms.gle/FeWV9RLEedfdkmFN6

As per the survey, here is a list of improvements to come
- [x] update the repository to include the new openai library syntax (ex: `Openai()` class) | completed, use `g4f.client.Client`
- [ ] golang implementation
- [ ] üöß Improve Documentation (in /docs & Guides, Howtos, & Do video tutorials
- [x] Improve the provider status list & updates
- [ ] Tutorials on how to reverse sites to write your own wrapper (PoC only ofc)
- [ ] Improve the Bing wrapper. (might write a new wrapper in golang as it is very fast)
- [ ] Write a standard provider performance test to improve the stability
- [ ] Potential support and development of local models
- [ ] üöß improve compatibility and error handling

## üìö Table of Contents

- [üÜï What's New](#-whats-new)
- [üìö Table of Contents](#-table-of-contents)
- [üõ†Ô∏è Getting Started](#-getting-started)
    + [Docker container](#docker-container)
      - [Quick start](#quick-start)
    + [Use python](#use-python)
      - [Prerequisites](#prerequisites)
      - [Install using PyPI package:](#install-using-pypi-package)
      - [Install from source:](#install-from-source)
      - [Install using Docker:](#install-using-docker)
- [üí° Usage](#-usage)
  * [Text Generation](#text-generation)
  * [Image Generation](#image-generation)
  * [Web UI](#web-ui)
  * [Interference API](#interference-api)
  * [Configuration](#configuration)
- [üöÄ Providers and Models](#-providers-and-models)
  * [GPT-4](#gpt-4)
  * [GPT-3.5](#gpt-35)
  * [Other](#other)
  * [Models](#models)
- [üîó Related GPT4Free Projects](#-related-gpt4free-projects)
- [ü§ù Contribute](#-contribute)
    + [How do i create a new Provider?](#guide-how-do-i-create-a-new-provider)
    + [How can AI help me with writing code?](#guide-how-can-ai-help-me-with-writing-code)
- [üôå Contributors](#-contributors)
- [¬©Ô∏è Copyright](#-copyright)
- [‚≠ê Star History](#-star-history)
- [üìÑ License](#-license)

## üõ†Ô∏è Getting Started

#### Docker container

##### Quick start:

1. [Download and install Docker](https://docs.docker.com/get-docker/)
2. Pull latest image and run the container:

```sh
docker pull hlohaus789/g4f
docker run -p 8080:8080 -p 1337:1337 -p 7900:7900 --shm-size=""2g"" hlohaus789/g4f:latest
```
3. Open the included client on: [http://localhost:8080/chat/](http://localhost:8080/chat/)
or set the api base in your client to: [http://localhost:1337/v1](http://localhost:1337/v1)
4. (Optional) If you need to log in to a provider, you can view the desktop from the container here: http://localhost:7900/?autoconnect=1&resize=scale&password=secret.

##### Use your smartphone:

Run the Web UI on Your Smartphone:
- [/docs/guides/phone](/docs/guides/phone.md)

#### Use python

##### Prerequisites:

1. [Download and install Python](https://www.python.org/downloads/) (Version 3.10+ is recommended).
2. [Install Google Chrome](https://www.google.com/chrome/) for providers with webdriver

##### Install using PyPI package:

```
pip install -U g4f[all]
```

How do I install only parts or do disable parts?
Use partial requirements: [/docs/requirements](/docs/requirements.md)

##### Install from source:

How do I load the project using git and installing the project requirements?
Read this tutorial and follow it step by step: [/docs/git](/docs/git.md)


##### Install using Docker:

How do I build and run composer image from source?
Use docker-compose: [/docs/docker](/docs/docker.md)


## üí° Usage

#### Text Generation

```python
from g4f.client import Client

client = Client()
response = client.chat.completions.create(
    model=""gpt-3.5-turbo"",
    messages=[{""role"": ""user"", ""content"": ""Hello""}],
    ...
)
print(response.choices[0].message.content)
```

```
Hello! How can I assist you today?
```

#### Image Generation

```python
from g4f.client import Client

client = Client()
response = client.images.generate(
  model=""gemini"",
  prompt=""a white siamese cat"",
  ...
)
image_url = response.data[0].url
```


[![Image with cat](/docs/cat.jpeg)](/docs/client.md)

**Full Documentation for Python API**

- New Client API like the OpenAI Python library: [/docs/client](/docs/client.md)
- Legacy API with python modules: [/docs/legacy](/docs/legacy.md)

#### Web UI

To start the web interface, type the following codes in python:

```python
from g4f.gui import run_gui
run_gui()
```
or execute the following command:
```bash
python -m g4f.cli gui -port 8080 -debug
```

### Interference API

You can use the Interference API to serve other OpenAI integrations with G4F.

See: [/docs/interference](/docs/interference.md)

### Configuration

##### Cookies / Access Token

For generating images with Bing and for the OpenAi Chat  you need cookies or a token from your browser session. From Bing you need the ""_U"" cookie and from OpenAI you need the ""access_token"". You can pass the cookies / the access token in the create function or you use the `set_cookies` setter before you run G4F:

```python
from g4f.cookies import set_cookies

set_cookies("".bing.com"", {
  ""_U"": ""cookie value""
})
set_cookies(""chat.openai.com"", {
  ""access_token"": ""token value""
})
set_cookies("".google.com"", {
  ""__Secure-1PSID"": ""cookie value""
})

...
```

Alternatively, G4F reads the cookies with `browser_cookie3` from your browser
or it starts a browser instance with selenium `webdriver` for logging in.

##### Using Proxy

If you want to hide or change your IP address for the providers, you can set a proxy globally via an environment variable:

- On macOS and Linux:
```bash
export G4F_PROXY=""http://host:port""
```

- On Windows:
```bash
set G4F_PROXY=http://host:port
```

## üöÄ Providers and Models

### GPT-4

| Website | Provider | GPT-3.5 | GPT-4 | Stream | Status | Auth |
| ------  | -------  | ------- | ----- | ------ | ------ | ---- |
| [bing.com](https://bing.com/chat) | `g4f.Provider.Bing` | ‚ùå | ‚úîÔ∏è | ‚úîÔ∏è | ![Active](https://img.shields.io/badge/Active-brightgreen) | ‚ùå |
| [liaobots.site](https://liaobots.site) | `g4f.Provider.Liaobots` | ‚úîÔ∏è | ‚úîÔ∏è | ‚úîÔ∏è | ![Active](https://img.shields.io/badge/Active-brightgreen) | ‚ùå |
| [chat.openai.com](https://chat.openai.com) | `g4f.Provider.OpenaiChat` | ‚úîÔ∏è | ‚úîÔ∏è | ‚úîÔ∏è | ![Unknown](https://img.shields.io/badge/Active-brightgreen) | ‚úîÔ∏è |
| [raycast.com](https://raycast.com) | `g4f.Provider.Raycast` | ‚úîÔ∏è | ‚úîÔ∏è | ‚úîÔ∏è | ![Unknown](https://img.shields.io/badge/Unknown-grey) | ‚úîÔ∏è |
| [you.com](https://you.com) | `g4f.Provider.You` | ‚úîÔ∏è | ‚úîÔ∏è | ‚úîÔ∏è | ![Active](https://img.shields.io/badge/Active-brightgreen) | ‚ùå |
| [chat.geekgpt.org](https://chat.geekgpt.org) | `g4f.Provider.GeekGpt` | ‚úîÔ∏è | ‚úîÔ∏è | ‚úîÔ∏è | ![Inactive](https://img.shields.io/badge/Inactive-red) | ‚ùå |

### GPT-3.5

| Website | Provider | GPT-3.5 | GPT-4 | Stream | Status | Auth |
| ------  | -------  | ------- | ----- | ------ | ------ | ---- |
| [chat3.aiyunos.top](https://chat3.aiyunos.top/) | `g4f.Provider.AItianhuSpace` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Unknown](https://img.shields.io/badge/Unknown-grey) | ‚ùå |
| [aichatonline.org](https://aichatonline.org) | `g4f.Provider.AiChatOnline` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Unknown](https://img.shields.io/badge/Unknown-grey) | ‚ùå |
| [openchat.team](https://openchat.team) | `g4f.Provider.Aura` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Active](https://img.shields.io/badge/Active-brightgreen) | ‚ùå |
| [chatbase.co](https://www.chatbase.co) | `g4f.Provider.ChatBase` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Unknown](https://img.shields.io/badge/Unknown-grey) | ‚ùå |
| [chatforai.store](https://chatforai.store) | `g4f.Provider.ChatForAi` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Unknown](https://img.shields.io/badge/Unknown-grey) | ‚ùå |
| [chatgpt.ai](https://chatgpt.ai) | `g4f.Provider.ChatgptAi` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Unknown](https://img.shields.io/badge/Unknown-grey) | ‚ùå |
| [chat.chatgptdemo.net](https://chat.chatgptdemo.net) | `g4f.Provider.ChatgptDemo` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Unknown](https://img.shields.io/badge/Unknown-grey) | ‚ùå |
| [chatgpt-free.cc](https://www.chatgpt-free.cc) | `g4f.Provider.ChatgptNext` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Unknown](https://img.shields.io/badge/Unknown-grey) | ‚ùå |
| [chat.3211000.xyz](https://chat.3211000.xyz) | `g4f.Provider.Chatxyz` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Unknown](https://img.shields.io/badge/Unknown-grey) | ‚ùå |
| [gptalk.net](https://gptalk.net) | `g4f.Provider.GPTalk` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Unknown](https://img.shields.io/badge/Unknown-grey) | ‚ùå |
| [gpt6.ai](https://gpt6.ai) | `g4f.Provider.Gpt6` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Unknown](https://img.shields.io/badge/Unknown-grey) | ‚ùå |
| [gptchatly.com](https://gptchatly.com) | `g4f.Provider.GptChatly` | ‚úîÔ∏è | ‚ùå | ‚ùå | ![Unknown](https://img.shields.io/badge/Unknown-grey) | ‚ùå |
| [ai18.gptforlove.com](https://ai18.gptforlove.com) | `g4f.Provider.GptForLove` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Unknown](https://img.shields.io/badge/Unknown-grey) | ‚ùå |
| [gptgo.ai](https://gptgo.ai) | `g4f.Provider.GptGo` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Unknown](https://img.shields.io/badge/Unknown-grey) | ‚ùå |
| [gpttalk.ru](https://gpttalk.ru) | `g4f.Provider.GptTalkRu` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Unknown](https://img.shields.io/badge/Unknown-grey) | ‚ùå |
| [koala.sh](https://koala.sh) | `g4f.Provider.Koala` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Active](https://img.shields.io/badge/Active-brightgreen) | ‚ùå |
| [app.myshell.ai](https://app.myshell.ai/chat) | `g4f.Provider.MyShell` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Unknown](https://img.shields.io/badge/Unknown-grey) | ‚ùå |
| [onlinegpt.org](https://onlinegpt.org) | `g4f.Provider.OnlineGpt` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Unknown](https://img.shields.io/badge/Unknown-grey) | ‚ùå |
| [perplexity.ai](https://www.perplexity.ai) | `g4f.Provider.PerplexityAi` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Unknown](https://img.shields.io/badge/Unknown-grey) | ‚ùå |
| [poe.com](https://poe.com) | `g4f.Provider.Poe` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Unknown](https://img.shields.io/badge/Unknown-grey) | ‚úîÔ∏è |
| [talkai.info](https://talkai.info) | `g4f.Provider.TalkAi` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Unknown](https://img.shields.io/badge/Unknown-grey) | ‚ùå |
| [aitianhu.com](https://www.aitianhu.com) | `g4f.Provider.AItianhu` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Inactive](https://img.shields.io/badge/Inactive-red) | ‚ùå |
| [e.aiask.me](https://e.aiask.me) | `g4f.Provider.AiAsk` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Inactive](https://img.shields.io/badge/Inactive-red) | ‚ùå |
| [chatgpt.bestim.org](https://chatgpt.bestim.org) | `g4f.Provider.Bestim` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Inactive](https://img.shields.io/badge/Inactive-red) | ‚ùå |
| [chatanywhere.cn](https://chatanywhere.cn) | `g4f.Provider.ChatAnywhere` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Inactive](https://img.shields.io/badge/Inactive-red) | ‚ùå |
| [chatgpt4online.org](https://chatgpt4online.org) | `g4f.Provider.Chatgpt4Online` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Inactive](https://img.shields.io/badge/Inactive-red) | ‚ùå |
| [chat.chatgptdemo.ai](https://chat.chatgptdemo.ai) | `g4f.Provider.ChatgptDemoAi` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Inactive](https://img.shields.io/badge/Inactive-red) | ‚ùå |
| [chatgptfree.ai](https://chatgptfree.ai) | `g4f.Provider.ChatgptFree` | ‚úîÔ∏è | ‚ùå | ‚ùå | ![Inactive](https://img.shields.io/badge/Inactive-red) | ‚ùå |
| [chatgptlogin.ai](https://chatgptlogin.ai) | `g4f.Provider.ChatgptLogin` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Inactive](https://img.shields.io/badge/Inactive-red) | ‚ùå |
| [chatgptx.de](https://chatgptx.de) | `g4f.Provider.ChatgptX` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Inactive](https://img.shields.io/badge/Inactive-red) | ‚ùå |
| [chat-shared2.zhile.io](https://chat-shared2.zhile.io) | `g4f.Provider.FakeGpt` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Inactive](https://img.shields.io/badge/Inactive-red) | ‚ùå |
| [freegpts1.aifree.site](https://freegpts1.aifree.site/) | `g4f.Provider.FreeGpt` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Inactive](https://img.shields.io/badge/Inactive-red) | ‚ùå |
| [gptgod.site](https://gptgod.site) | `g4f.Provider.GptGod` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Inactive](https://img.shields.io/badge/Inactive-red) | ‚ùå |
| [hashnode.com](https://hashnode.com) | `g4f.Provider.Hashnode` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Inactive](https://img.shields.io/badge/Inactive-red) | ‚ùå |
| [sdk.vercel.ai](https://sdk.vercel.ai) | `g4f.Provider.Vercel` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Inactive](https://img.shields.io/badge/Inactive-red) | ‚ùå |
| [chat.ylokh.xyz](https://chat.ylokh.xyz) | `g4f.Provider.Ylokh` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Inactive](https://img.shields.io/badge/Inactive-red) | ‚ùå |

### Other

| Website | Provider | GPT-3.5 | GPT-4 | Stream | Status | Auth |
| ------  | -------  | ------- | ----- | ------ | ------ | ---- |
| [bard.google.com](https://bard.google.com) | `g4f.Provider.Bard` | ‚ùå | ‚ùå | ‚ùå | ![Unknown](https://img.shields.io/badge/Unknown-grey) | ‚úîÔ∏è |
| [deepinfra.com](https://deepinfra.com) | `g4f.Provider.DeepInfra` | ‚ùå | ‚ùå | ‚úîÔ∏è | ![Active](https://img.shields.io/badge/Active-brightgreen) | ‚ùå |
| [gemini.google.com](https://gemini.google.com) | `g4f.Provider.Gemini` | ‚ùå | ‚ùå | ‚úîÔ∏è | ![Active](https://img.shields.io/badge/Active-brightgreen) | ‚úîÔ∏è |
| [ai.google.dev](https://ai.google.dev) | `g4f.Provider.GeminiPro` | ‚ùå | ‚ùå | ‚úîÔ∏è | ![Unknown](https://img.shields.io/badge/Active-brightgreen) | ‚ùå |
| [gemini-chatbot-sigma.vercel.app](https://gemini-chatbot-sigma.vercel.app) | `g4f.Provider.GeminiProChat` | ‚úîÔ∏è | ‚ùå | ‚úîÔ∏è | ![Active](https://img.shields.io/badge/Active-brightgreen) | ‚ùå |
| [huggingface.co](https://huggingface.co/chat) | `g4f.Provider.HuggingChat` | ‚ùå | ‚ùå | ‚úîÔ∏è | ![Unknown](https://img.shields.io/badge/Unknown-grey) | ‚ùå |
| [llama2.ai](https://www.llama2.ai) | `g4f.Provider.Llama2` | ‚ùå | ‚ùå | ‚úîÔ∏è | ![Active](https://img.shields.io/badge/Active-brightgreen) | ‚ùå |
| [labs.perplexity.ai](https://labs.perplexity.ai) | `g4f.Provider.PerplexityLabs` | ‚ùå | ‚ùå | ‚úîÔ∏è | ![Active](https://img.shields.io/badge/Active-brightgreen) | ‚ùå |
| [phind.com](https://www.phind.com) | `g4f.Provider.Phind` | ‚ùå | ‚ùå | ‚úîÔ∏è | ![Unknown](https://img.shields.io/badge/Active-brightgreen) | ‚ùå |
| [pi.ai](https://pi.ai/talk) | `g4f.Provider.Pi` | ‚ùå | ‚ùå | ‚úîÔ∏è | ![Unknown](https://img.shields.io/badge/Active-brightgreen) | ‚ùå |
| [beta.theb.ai](https://beta.theb.ai) | `g4f.Provider.Theb` | ‚úîÔ∏è | ‚úîÔ∏è | ‚úîÔ∏è | ![Unknown](https://img.shields.io/badge/Unknown-grey) | ‚ùå |
| [free.chatgpt.org.uk](https://free.chatgpt.org.uk) | `g4f.Provider.FreeChatgpt` | ‚úîÔ∏è | ‚úîÔ∏è | ‚úîÔ∏è | ![Unknown](https://img.shields.io/badge/Unknown-grey) | ‚ùå |
| [theb.ai](https://theb.ai) | `g4f.Provider.ThebApi` | ‚ùå | ‚ùå | ‚ùå | ![Unknown](https://img.shields.io/badge/Unknown-grey) | ‚úîÔ∏è |
| [open-assistant.io](https://open-assistant.io/chat) | `g4f.Provider.OpenAssistant` | ‚ùå | ‚ùå | ‚úîÔ∏è | ![Inactive](https://img.shields.io/badge/Inactive-red) | ‚úîÔ∏è |

### Models

| Model | Base Provider | Provider | Website |
| ----- | ------------- | -------- | ------- |
| gpt-3.5-turbo | OpenAI | 5+ Providers | [openai.com](https://openai.com/) |
| gpt-4 | OpenAI | 2+ Providers | [openai.com](https://openai.com/) |
| gpt-4-turbo | OpenAI | g4f.Provider.Bing | [openai.com](https://openai.com/) |
| Llama-2-7b-chat-hf | Meta | 2+ Providers | [llama.meta.com](https://llama.meta.com/) |
| Llama-2-13b-chat-hf | Meta | 2+ Providers | [llama.meta.com](https://llama.meta.com/) |
| Llama-2-70b-chat-hf | Meta | 4+ Providers | [llama.meta.com](https://llama.meta.com/) |
| CodeLlama-34b-Instruct-hf | Meta | 3+ Providers | [llama.meta.com](https://llama.meta.com/) |
| CodeLlama-70b-Instruct-hf | Meta | g4f.Provider.DeepInfra | [llama.meta.com](https://llama.meta.com/) |
| Mixtral-8x7B-Instruct-v0.1 | Huggingface | 3+ Providers | [huggingface.co](https://huggingface.co/) |
| Mistral-7B-Instruct-v0.1 | Huggingface | 3+ Providers | [huggingface.co](https://huggingface.co/) |
| dolphin-2.6-mixtral-8x7b | Huggingface | g4f.Provider.DeepInfra | [huggingface.co](https://huggingface.co/) |
| lzlv_70b_fp16_hf | Huggingface | g4f.Provider.DeepInfra | [huggingface.co](https://huggingface.co/) |
| airoboros-70b | Huggingface | g4f.Provider.DeepInfra | [huggingface.co](https://huggingface.co/) |
| airoboros-l2-70b-gpt4-1.4.1 | Huggingface | g4f.Provider.DeepInfra | [huggingface.co](https://huggingface.co/) |
| openchat_3.5 | Huggingface | 2+ Providers | [huggingface.co](https://huggingface.co/) |
| gemini | Google | g4f.Provider.Gemini | [gemini.google.com](https://gemini.google.com/) |
| gemini-pro | Google | 2+ Providers | [gemini.google.com](https://gemini.google.com/) |
| claude-v2 | Anthropic | 2+ Providers | [anthropic.com](https://www.anthropic.com/) |
| pi | Inflection | g4f.Provider.Pi | [inflection.ai](https://inflection.ai/) |

## üîó Related GPT4Free Projects

<table>
  <thead align=""center"">
    <tr border: none>
      <td><b>üéÅ Projects</b></td>
      <td><b>‚≠ê Stars</b></td>
      <td><b>üìö Forks</b></td>
      <td><b>üõé Issues</b></td>
      <td><b>üì¨ Pull requests</b></td>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href=""https://github.com/xtekky/gpt4free""><b>gpt4free</b></a></td>
      <td><a href=""https://github.com/xtekky/gpt4free/stargazers""><img alt=""Stars"" src=""https://img.shields.io/github/stars/xtekky/gpt4free?style=flat-square&labelColor=343b41""/></a></td>
      <td><a href=""https://github.com/xtekky/gpt4free/network/members""><img alt=""Forks"" src=""https://img.shields.io/github/forks/xtekky/gpt4free?style=flat-square&labelColor=343b41""/></a></td>
      <td><a href=""https://github.com/xtekky/gpt4free/issues""><img alt=""Issues"" src=""https://img.shields.io/github/issues/xtekky/gpt4free?style=flat-square&labelColor=343b41""/></a></td>
      <td><a href=""https://github.com/xtekky/gpt4free/pulls""><img alt=""Pull Requests"" src=""https://img.shields.io/github/issues-pr/xtekky/gpt4free?style=flat-square&labelColor=343b41""/></a></td>
    </tr>
      <td><a href=""https://github.com/xiangsx/gpt4free-ts""><b>gpt4free-ts</b></a></td>
      <td><a href=""https://github.com/xiangsx/gpt4free-ts/stargazers""><img alt=""Stars"" src=""https://img.shields.io/github/stars/xiangsx/gpt4free-ts?style=flat-square&labelColor=343b41""/></a></td>
      <td><a href=""https://github.com/xiangsx/gpt4free-ts/network/members""><img alt=""Forks"" src=""https://img.shields.io/github/forks/xiangsx/gpt4free-ts?style=flat-square&labelColor=343b41""/></a></td>
      <td><a href=""https://github.com/xiangsx/gpt4free-ts/issues""><img alt=""Issues"" src=""https://img.shields.io/github/issues/xiangsx/gpt4free-ts?style=flat-square&labelColor=343b41""/></a></td>
      <td><a href=""https://github.com/xiangsx/gpt4free-ts/pulls""><img alt=""Pull Requests"" src=""https://img.shields.io/github/issues-pr/xiangsx/gpt4free-ts?style=flat-square&labelColor=343b41""/></a></td>
    </tr>
     <tr>
      <td><a href=""https://github.com/zukixa/cool-ai-stuff/""><b>Free AI API's & Potential Providers List</b></a></td>
      <td><a href=""https://github.com/zukixa/cool-ai-stuff/stargazers""><img alt=""Stars"" src=""https://img.shields.io/github/stars/zukixa/cool-ai-stuff?style=flat-square&labelColor=343b41""/></a></td>
      <td><a href=""https://github.com/zukixa/cool-ai-stuff/network/members""><img alt=""Forks"" src=""https://img.shields.io/github/forks/zukixa/cool-ai-stuff?style=flat-square&labelColor=343b41""/></a></td>
      <td><a href=""https://github.com/zukixa/cool-ai-stuff/issues""><img alt=""Issues"" src=""https://img.shields.io/github/issues/zukixa/cool-ai-stuff?style=flat-square&labelColor=343b41""/></a></td>
      <td><a href=""https://github.com/zukixa/cool-ai-stuff/pulls""><img alt=""Pull Requests"" src=""https://img.shields.io/github/issues-pr/zukixa/cool-ai-stuff?style=flat-square&labelColor=343b41""/></a></td>
    </tr>
    <tr>
    <tr>
      <td><a href=""https://github.com/xtekky/chatgpt-clone""><b>ChatGPT-Clone</b></a></td>
      <td><a href=""https://github.com/xtekky/chatgpt-clone/stargazers""><img alt=""Stars"" src=""https://img.shields.io/github/stars/xtekky/chatgpt-clone?style=flat-square&labelColor=343b41""/></a></td>
      <td><a href=""https://github.com/xtekky/chatgpt-clone/network/members""><img alt=""Forks"" src=""https://img.shields.io/github/forks/xtekky/chatgpt-clone?style=flat-square&labelColor=343b41""/></a></td>
      <td><a href=""https://github.com/xtekky/chatgpt-clone/issues""><img alt=""Issues"" src=""https://img.shields.io/github/issues/xtekky/chatgpt-clone?style=flat-square&labelColor=343b41""/></a></td>
      <td><a href=""https://github.com/xtekky/chatgpt-clone/pulls""><img alt=""Pull Requests"" src=""https://img.shields.io/github/issues-pr/xtekky/chatgpt-clone?style=flat-square&labelColor=343b41""/></a></td>
    </tr>
    <tr>
      <td><a href=""https://github.com/mishalhossin/Discord-Chatbot-Gpt4Free""><b>ChatGpt Discord Bot</b></a></td>
      <td><a href=""https://github.com/mishalhossin/Discord-Chatbot-Gpt4Free/stargazers""><img alt=""Stars"" src=""https://img.shields.io/github/stars/mishalhossin/Discord-Chatbot-Gpt4Free?style=flat-square&labelColor=343b41""/></a></td>
      <td><a href=""https://github.com/mishalhossin/Discord-Chatbot-Gpt4Free/network/members""><img alt=""Forks"" src=""https://img.shields.io/github/forks/mishalhossin/Discord-Chatbot-Gpt4Free?style=flat-square&labelColor=343b41""/></a></td>
      <td><a href=""https://github.com/mishalhossin/Discord-Chatbot-Gpt4Free/issues""><img alt=""Issues"" src=""https://img.shields.io/github/issues/mishalhossin/Discord-Chatbot-Gpt4Free?style=flat-square&labelColor=343b41""/></a></td>
      <td><a href=""https://github.com/mishalhossin/Coding-Chatbot-Gpt4Free/pulls""><img alt=""Pull Requests"" src=""https://img.shields.io/github/issues-pr/mishalhossin/Discord-Chatbot-Gpt4Free?style=flat-square&labelColor=343b41""/></a></td>
<tr>
  <td><a href=""https://github.com/SamirXR/Nyx-Bot""><b>Nyx-Bot (Discord)</b></a></td>
  <td><a href=""https://github.com/SamirXR/Nyx-Bot/stargazers""><img alt=""Stars"" src=""https://img.shields.io/github/stars/SamirXR/Nyx-Bot?style=flat-square&labelColor=343b41""/></a></td>
  <td><a href=""https://github.com/SamirXR/Nyx-Bot/network/members""><img alt=""Forks"" src=""https://img.shields.io/github/forks/SamirXR/Nyx-Bot?style=flat-square&labelColor=343b41""/></a></td>
  <td><a href=""https://github.com/SamirXR/Nyx-Bot/issues""><img alt=""Issues"" src=""https://img.shields.io/github/issues/SamirXR/Nyx-Bot?style=flat-square&labelColor=343b41""/></a></td>
  <td><a href=""https://github.com/SamirXR/Nyx-Bot/pulls""><img alt=""Pull Requests"" src=""https://img.shields.io/github/issues-pr/SamirXR/Nyx-Bot?style=flat-square&labelColor=343b41""/></a></td>
</tr>
    </tr>
    <tr>
      <td><a href=""https://github.com/MIDORIBIN/langchain-gpt4free""><b>LangChain gpt4free</b></a></td>
      <td><a href=""https://github.com/MIDORIBIN/langchain-gpt4free/stargazers""><img alt=""Stars"" src=""https://img.shields.io/github/stars/MIDORIBIN/langchain-gpt4free?style=flat-square&labelColor=343b41""/></a></td>
      <td><a href=""https://github.com/MIDORIBIN/langchain-gpt4free/network/members""><img alt=""Forks"" src=""https://img.shields.io/github/forks/MIDORIBIN/langchain-gpt4free?style=flat-square&labelColor=343b41""/></a></td>
      <td><a href=""https://github.com/MIDORIBIN/langchain-gpt4free/issues""><img alt=""Issues"" src=""https://img.shields.io/github/issues/MIDORIBIN/langchain-gpt4free?style=flat-square&labelColor=343b41""/></a></td>
      <td><a href=""https://github.com/MIDORIBIN/langchain-gpt4free/pulls""><img alt=""Pull Requests"" src=""https://img.shields.io/github/issues-pr/MIDORIBIN/langchain-gpt4free?style=flat-square&labelColor=343b41""/></a></td>
    </tr>
    <tr>
      <td><a href=""https://github.com/HexyeDEV/Telegram-Chatbot-Gpt4Free""><b>ChatGpt Telegram Bot</b></a></td>
      <td><a href=""https://github.com/HexyeDEV/Telegram-Chatbot-Gpt4Free/stargazers""><img alt=""Stars"" src=""https://img.shields.io/github/stars/HexyeDEV/Telegram-Chatbot-Gpt4Free?style=flat-square&labelColor=343b41""/></a></td>
      <td><a href=""https://github.com/HexyeDEV/Telegram-Chatbot-Gpt4Free/network/members""><img alt=""Forks"" src=""https://img.shields.io/github/forks/HexyeDEV/Telegram-Chatbot-Gpt4Free?style=flat-square&labelColor=343b41""/></a></td>
      <td><a href=""https://github.com/HexyeDEV/Telegram-Chatbot-Gpt4Free/issues""><img alt=""Issues"" src=""https://img.shields.io/github/issues/HexyeDEV/Telegram-Chatbot-Gpt4Free?style=flat-square&labelColor=343b41""/></a></td>
      <td><a href=""https://github.com/HexyeDEV/Telegram-Chatbot-Gpt4Free/pulls""><img alt=""Pull Requests"" src=""https://img.shields.io/github/issues-pr/HexyeDEV/Telegram-Chatbot-Gpt4Free?style=flat-square&labelColor=343b41""/></a></td>
    </tr>
        <tr>
      <td><a href=""https://github.com/Lin-jun-xiang/chatgpt-line-bot""><b>ChatGpt Line Bot</b></a></td>
      <td><a href=""https://github.com/Lin-jun-xiang/chatgpt-line-bot/stargazers""><img alt=""Stars"" src=""https://img.shields.io/github/stars/Lin-jun-xiang/chatgpt-line-bot?style=flat-square&labelColor=343b41""/></a></td>
      <td><a href=""https://github.com/Lin-jun-xiang/chatgpt-line-bot/network/members""><img alt=""Forks"" src=""https://img.shields.io/github/forks/Lin-jun-xiang/chatgpt-line-bot?style=flat-square&labelColor=343b41""/></a></td>
      <td><a href=""https://github.com/Lin-jun-xiang/chatgpt-line-bot/issues""><img alt=""Issues"" src=""https://img.shields.io/github/issues/Lin-jun-xiang/chatgpt-line-bot?style=flat-square&labelColor=343b41""/></a></td>
      <td><a href=""https://github.com/Lin-jun-xiang/chatgpt-line-bot/pulls""><img alt=""Pull Requests"" src=""https://img.shields.io/github/issues-pr/Lin-jun-xiang/chatgpt-line-bot?style=flat-square&labelColor=343b41""/></a></td>
    </tr>
    <tr>
      <td><a href=""https://github.com/Lin-jun-xiang/action-translate-readme""><b>Action Translate Readme</b></a></td>
      <td><a href=""https://github.com/Lin-jun-xiang/action-translate-readme/stargazers""><img alt=""Stars"" src=""https://img.shields.io/github/stars/Lin-jun-xiang/action-translate-readme?style=flat-square&labelColor=343b41""/></a></td>
      <td><a href=""https://github.com/Lin-jun-xiang/action-translate-readme/network/members""><img alt=""Forks"" src=""https://img.shields.io/github/forks/Lin-jun-xiang/action-translate-readme?style=flat-square&labelColor=343b41""/></a></td>
      <td><a href=""https://github.com/Lin-jun-xiang/action-translate-readme/issues""><img alt=""Issues"" src=""https://img.shields.io/github/issues/Lin-jun-xiang/action-translate-readme?style=flat-square&labelColor=343b41""/></a></td>
      <td><a href=""https://github.com/Lin-jun-xiang/action-translate-readme/pulls""><img alt=""Pull Requests"" src=""https://img.shields.io/github/issues-pr/Lin-jun-xiang/action-translate-readme?style=flat-square&labelColor=343b41""/></a></td>
    </tr>
    <tr>
      <td><a href=""https://github.com/Lin-jun-xiang/docGPT-streamlit""><b>Langchain Document GPT</b></a></td>
      <td><a href=""https://github.com/Lin-jun-xiang/docGPT-streamlit/stargazers""><img alt=""Stars"" src=""https://img.shields.io/github/stars/Lin-jun-xiang/docGPT-streamlit?style=flat-square&labelColor=343b41""/></a></td>
      <td><a href=""https://github.com/Lin-jun-xiang/docGPT-streamlit/network/members""><img alt=""Forks"" src=""https://img.shields.io/github/forks/Lin-jun-xiang/docGPT-streamlit?style=flat-square&labelColor=343b41""/></a></td>
      <td><a href=""https://github.com/Lin-jun-xiang/docGPT-streamlit/issues""><img alt=""Issues"" src=""https://img.shields.io/github/issues/Lin-jun-xiang/docGPT-streamlit?style=flat-square&labelColor=343b41""/></a></td>
      <td><a href=""https://github.com/Lin-jun-xiang/docGPT-streamlit/pulls""><img alt=""Pull Requests"" src=""https://img.shields.io/github/issues-pr/Lin-jun-xiang/docGPT-streamlit?style=flat-square&labelColor=343b41""/></a></td>
    </tr>
    <tr>
      <td><a href=""https://github.com/Simatwa/python-tgpt""><b>python-tgpt</b></a></td>
      <td><a href=""https://github.com/Simatwa/python-tgpt/stargazers""><img alt=""Stars"" src=""https://img.shields.io/github/stars/Simatwa/python-tgpt?style=flat-square&labelColor=343b41""/></a></td>
      <td><a href=""https://github.com/Simatwa/python-tgpt/network/members""><img alt=""Forks"" src=""https://img.shields.io/github/forks/Simatwa/python-tgpt?style=flat-square&labelColor=343b41""/></a></td>
      <td><a href=""https://github.com/Simatwa/python-tgpt/issues""><img alt=""Issues"" src=""https://img.shields.io/github/issues/Simatwa/python-tgpt?style=flat-square&labelColor=343b41""/></a></td>
      <td><a href=""https://github.com/Simatwa/python-tgpt/pulls""><img alt=""Pull Requests"" src=""https://img.shields.io/github/issues-pr/Simatwa/python-tgpt?style=flat-square&labelColor=343b41""/></a></td>
    </tr>
  </tbody>
</table>

## ü§ù Contribute

We welcome contributions from the community. Whether you're adding new providers or features, or simply fixing typos and making small improvements, your input is valued. Creating a pull request is all it takes ‚Äì our co-pilot will handle the code review process. Once all changes have been addressed, we'll merge the pull request into the main branch and release the updates at a later time.

###### Guide: How do i create a new Provider?

 - Read: [/docs/guides/create_provider](/docs/guides/create_provider.md)

###### Guide: How can AI help me with writing code?

 - Read: [/docs/guides/help_me](/docs/guides/help_me.md)

## üôå Contributors

A list of all contributors is available [here](https://github.com/xtekky/gpt4free/graphs/contributors)   
The [`Vercel.py`](https://github.com/xtekky/gpt4free/blob/main/g4f/Provider/Vercel.py) file contains code from [vercel-llm-api](https://github.com/ading2210/vercel-llm-api) by [@ading2210](https://github.com/ading2210), which is licensed under the [GNU GPL v3](https://www.gnu.org/licenses/gpl-3.0.txt)   
Top 1 Contributor: [@hlohaus](https://github.com/hlohaus)

## ¬©Ô∏è Copyright

This program is licensed under the [GNU GPL v3](https://www.gnu.org/licenses/gpl-3.0.txt)

```
xtekky/gpt4free: Copyright (C) 2023 xtekky

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see <https://www.gnu.org/licenses/>.
```

## ‚≠ê Star History

<a href=""https://github.com/xtekky/gpt4free/stargazers"">
        <img width=""500"" alt=""Star History Chart"" src=""https://api.star-history.com/svg?repos=xtekky/gpt4free&type=Date"">
</a>

## üìÑ License

<table>
  <tr>
     <td>
       <p align=""center""> <img src=""https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/GPLv3_Logo.svg/1200px-GPLv3_Logo.svg.png"" width=""80%""></img>
    </td>
    <td> 
      <img src=""https://img.shields.io/badge/License-GNU_GPL_v3.0-red.svg""/> <br> 
This project is licensed under <a href=""./LICENSE"">GNU_GPL_v3.0</a>.
    </td>
  </tr>
</table>

<p align=""right"">(<a href=""#top"">üîº Back to top</a>)</p>
",,https://raw.githubusercontent.com/xtekky/gpt4free/main//docs/cat.jpeg,,0,https://raw.githubusercontent.com/xtekky/gpt4free/main//docs/cat.jpeg,,
2024-02-25,https://github.com/jackfrued/Python-100-Days,https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/README.md,„ÄäPython - 100Â§©‰ªéÊñ∞ÊâãÂà∞Â§ßÂ∏à„ÄãÊòØÁî±È™ÜÊòäÁºñÂÜôÁöÑ‰∏Ä‰ªΩÁºñÁ®ãÂ≠¶‰π†ÊåáÂçóÔºåÊó®Âú®Â∏ÆÂä©ÂàùÂ≠¶ËÄÖÂú®100Â§©ÂÜÖÊéåÊè°PythonËØ≠Ë®Ä„ÄÇËØ•ÊåáÂçóÂåÖÂê´‰ªéÂü∫Á°ÄËØ≠Ê≥ïÂà∞È´òÁ∫ßÁºñÁ®ãÊ¶ÇÂøµÁöÑÂπøÊ≥õÂÜÖÂÆπÔºåÂπ∂ÂàÜ‰∏∫‰∏çÂêå‰∏ìÈ¢òÔºåÂ¶ÇPythonÂü∫Á°Ä„ÄÅWebÂâçÁ´ØÂÖ•Èó®„ÄÅLinuxÊìç‰ΩúÁ≥ªÁªü„ÄÅÊï∞ÊçÆÂ∫ìÂü∫Á°Ä‰∏éËøõÈò∂„ÄÅDjangoÂÆûÊàò„ÄÅÊï∞ÊçÆÂàÜÊûê„ÄÅÁà¨Ëô´ÂºÄÂèë„ÄÅÊú∫Âô®Â≠¶‰π†‰∏éÊ∑±Â∫¶Â≠¶‰π†Á≠â„ÄÇÊØèÈÉ®ÂàÜÊèê‰æõ‰∫ÜËØ¶ÁªÜÁöÑÂ≠¶‰π†ËÆ°ÂàíÔºåÂåÖÊã¨ÁêÜËÆ∫Áü•ËØÜ„ÄÅÂÆûË∑µÊ°à‰æãÂíåÁªÉ‰π†È¢òÁõÆ„ÄÇÈ™ÜÊòäËøòÊèê‰æõ‰∫Ü‰∏Ä‰∏™ËæÖÂä©ËßÜÈ¢ëÊïôÁ®ãÈ°πÁõÆ‚ÄúPython-Core-50-Courses‚ÄùÔºå‰ª•ÂèäÂú®Áü•‰πé‰∏äÁöÑ‰∏ìÊ†è‚Äú‰ªéÈõ∂ÂºÄÂßãÂ≠¶Python‚ÄùÔºåÊñπ‰æøÂ≠¶‰π†ËÄÖÊõ¥Â•ΩÂú∞ÁêÜËß£Âü∫Á°ÄÊ¶ÇÂøµ„ÄÇÊ≠§Â§ñÔºå‰π¶‰∏≠‰πüÂº∫Ë∞É‰∫ÜÂÆûË∑µÁöÑÈáçË¶ÅÊÄßÔºåÊé®ËçêÂ≠¶‰π†ËÄÖÂä†ÂÖ•QQÂ≠¶‰π†Áæ§‰ª•Ëé∑ÂèñÈ¢ùÂ§ñÁöÑÂ≠¶‰π†ËµÑÊ∫êÂíåÈóÆÈ¢òËß£Á≠îÔºåÂêåÊó∂‰πü‰ªãÁªç‰∫ÜÂ¶Ç‰Ωï‰ΩøÁî®ÊäñÈü≥ÂíåBÁ´ôÊü•ÁúãÈÖçÂ•óËßÜÈ¢ëÊõ¥Êñ∞ÔºåÊó®Âú®‰∏∫PythonÂàùÂ≠¶ËÄÖÂíåÂºÄÂèëËÄÖÊèê‰æõ‰∏Ä‰∏™ÂÖ®Èù¢„ÄÅÈÄêÊ≠•Ê∑±ÂÖ•ÁöÑÂ≠¶‰π†Ë∑ØÂæÑ„ÄÇ,"## Python - 100Â§©‰ªéÊñ∞ÊâãÂà∞Â§ßÂ∏à

> **‰ΩúËÄÖ**ÔºöÈ™ÜÊòä
>
> **ËØ¥Êòé**Ôºö‰ªéÈ°πÁõÆ‰∏äÁ∫øÂà∞Ëé∑Âæó8w+ÊòüÊ†á‰ª•Êù•Ôºå‰∏ÄÁõ¥Êî∂Âà∞ÂèçÈ¶àËØ¥Âü∫Á°ÄÈÉ®ÂàÜÔºàÂâç15Â§©ÁöÑÂÜÖÂÆπÔºâÂØπÊñ∞ÊâãÊù•ËØ¥ÊòØÊØîËæÉÂõ∞ÈöæÁöÑÔºåÂª∫ËÆÆÊúâÈÖçÂ•óËßÜÈ¢ëËøõË°åËÆ≤Ëß£„ÄÇÊúÄËøëÊääÂü∫Á°ÄÈÉ®ÂàÜÁöÑÂÜÖÂÆπÈáçÊñ∞Âà∂‰Ωú‰∫Ü‰∏Ä‰∏™Âêç‰∏∫[‚ÄúPython-Core-50-Courses‚Äù](<https://github.com/jackfrued/Python-Core-50-Courses>)ÁöÑÈ°πÁõÆÔºå**Áî®Êõ¥‰∏∫ÁÆÄÂçïÈÄö‰øóÁöÑÊñπÂºèÈáçÂÜô‰∫ÜËøôÈÉ®ÂàÜÂÜÖÂÆπÂπ∂ÈôÑÂ∏¶‰∫ÜËßÜÈ¢ëËÆ≤Ëß£**ÔºåÂàùÂ≠¶ËÄÖÂèØ‰ª•ÂÖ≥Ê≥®‰∏ãËøô‰∏™Êñ∞È°πÁõÆ„ÄÇÂõΩÂÜÖÁî®Êà∑Â¶ÇÊûúËÆøÈóÆGitHubÊØîËæÉÊÖ¢ÁöÑËØùÔºåÂèØ‰ª•ÂÖ≥Ê≥®ÊàëÁöÑ**Áü•‰πéÂè∑[Python-Jack](https://www.zhihu.com/people/jackfrued)**Ôºå‰∏äÈù¢ÁöÑ[‚Äú‰ªéÈõ∂ÂºÄÂßãÂ≠¶Python‚Äù](<https://zhuanlan.zhihu.com/c_1216656665569013760>)‰∏ìÊ†èÊØîËæÉÈÄÇÂêàÂàùÂ≠¶ËÄÖÔºåÂÖ∂‰ªñÁöÑ‰∏ìÊ†è‰πüÂú®ÊåÅÁª≠Âàõ‰ΩúÂíåÊõ¥Êñ∞‰∏≠ÔºåÊ¨¢ËøéÂ§ßÂÆ∂ÂÖ≥Ê≥®Âπ∂ÁÇπËµûËØÑËÆ∫„ÄÇ
>
> ÈúÄË¶ÅÂä†ÂÖ•QQÂ≠¶‰π†Áæ§ÁöÑÂèØ‰ª•Êâ´Êèè‰∏ãÈù¢ÁöÑ‰∫åÁª¥Á†ÅÔºå‰∏â‰∏™Áæ§Âä†‰∏Ä‰∏™Âç≥ÂèØÔºå‰∏çË¶ÅÈáçÂ§çËøõÁæ§„ÄÇÂ≠¶‰π†Áæ§‰ºö‰∏∫Â§ßÂÆ∂Êèê‰æõ**Â≠¶‰π†ËµÑÊ∫ê**Âíå**ÈóÆÈ¢òËß£Á≠î**ÔºåÂ¶ÇÊûúÊúâ**Python‰ΩìÈ™åËØæ**Âíå**Ë°å‰∏öÂÖ¨ÂºÄËØæ**‰ºöÊèêÂâçÂú®Áæ§ÈáåÈÄöÁü•Â§ßÂÆ∂ÔºåÊ¨¢ËøéÂ§ßÂÆ∂Âä†ÂÖ•„ÄÇ
>
> <img src=""https://github.com/jackfrued/mypic/raw/master/20220616120218.JPG"" style=""zoom: 75%"">
>
> ÈÖçÂ•óÁöÑËßÜÈ¢ëÂú®ÊäñÈü≥ÂíåBÁ´ôÊåÅÁª≠Êõ¥Êñ∞‰∏≠ÔºåÊúâÂÖ¥Ë∂£ÁöÑÂ∞è‰ºô‰º¥ÂèØ‰ª•ÂÖ≥Ê≥®ÊàëÁöÑÊäñÈü≥ÊàñBÁ´ôË¥¶Âè∑ÔºåÊúÄËøëÂàöÂàöËµ∑Âè∑ÔºåËøòÂ∏åÊúõÂ§ßÂÆ∂Â§öÂ§öÊîØÊåÅÔºåÈùûÂ∏∏ÊÑüË∞¢ÊÇ®ÔºÅ
>
> <img src=""res/qrcode.JPG"" style=""zoom:30%"" />



### PythonÂ∫îÁî®È¢ÜÂüüÂíåËÅå‰∏öÂèëÂ±ïÂàÜÊûê

ÁÆÄÂçïÁöÑËØ¥ÔºåPythonÊòØ‰∏Ä‰∏™‚Äú‰ºòÈõÖ‚Äù„ÄÅ‚ÄúÊòéÁ°Æ‚Äù„ÄÅ‚ÄúÁÆÄÂçï‚ÄùÁöÑÁºñÁ®ãËØ≠Ë®Ä„ÄÇ

 - Â≠¶‰π†Êõ≤Á∫ø‰ΩéÔºåÈùû‰∏ì‰∏ö‰∫∫Â£´‰πüËÉΩ‰∏äÊâã
 - ÂºÄÊ∫êÁ≥ªÁªüÔºåÊã•ÊúâÂº∫Â§ßÁöÑÁîüÊÄÅÂúà
 - Ëß£ÈáäÂûãËØ≠Ë®ÄÔºåÂÆåÁæéÁöÑÂπ≥Âè∞ÂèØÁßªÊ§çÊÄß
 - Âä®ÊÄÅÁ±ªÂûãËØ≠Ë®ÄÔºåÊîØÊåÅÈù¢ÂêëÂØπË±°ÂíåÂáΩÊï∞ÂºèÁºñÁ®ã
 - ‰ª£Á†ÅËßÑËåÉÁ®ãÂ∫¶È´òÔºåÂèØËØªÊÄßÂº∫

PythonÂú®‰ª•‰∏ãÈ¢ÜÂüüÈÉΩÊúâÁî®Ê≠¶‰πãÂú∞„ÄÇ

 - ÂêéÁ´ØÂºÄÂèë - Python / Java / Go / PHP
 - DevOps - Python / Shell / Ruby
 - Êï∞ÊçÆÈááÈõÜ - Python / C++ / Java
 - ÈáèÂåñ‰∫§Êòì - Python / C++ / R
 - Êï∞ÊçÆÁßëÂ≠¶ - Python / R / Julia / Matlab
 - Êú∫Âô®Â≠¶‰π† - Python / R / C++ / Julia
 - Ëá™Âä®ÂåñÊµãËØï - Python / Shell

‰Ωú‰∏∫‰∏ÄÂêçPythonÂºÄÂèëËÄÖÔºåÊ†πÊçÆ‰∏™‰∫∫ÁöÑÂñúÂ•ΩÂíåËÅå‰∏öËßÑÂàíÔºåÂèØ‰ª•ÈÄâÊã©ÁöÑÂ∞±‰∏öÈ¢ÜÂüü‰πüÈùûÂ∏∏Â§ö„ÄÇ

- PythonÂêéÁ´ØÂºÄÂèëÂ∑•Á®ãÂ∏àÔºàÊúçÂä°Âô®„ÄÅ‰∫ëÂπ≥Âè∞„ÄÅÊï∞ÊçÆÊé•Âè£Ôºâ
- PythonËøêÁª¥Â∑•Á®ãÂ∏àÔºàËá™Âä®ÂåñËøêÁª¥„ÄÅSRE„ÄÅDevOpsÔºâ
- PythonÊï∞ÊçÆÂàÜÊûêÂ∏àÔºàÊï∞ÊçÆÂàÜÊûê„ÄÅÂïÜ‰∏öÊô∫ËÉΩ„ÄÅÊï∞Â≠óÂåñËøêËê•Ôºâ
- PythonÊï∞ÊçÆÊåñÊéòÂ∑•Á®ãÂ∏àÔºàÊú∫Âô®Â≠¶‰π†„ÄÅÊ∑±Â∫¶Â≠¶‰π†„ÄÅÁÆóÊ≥ï‰∏ìÂÆ∂Ôºâ
- PythonÁà¨Ëô´Â∑•Á®ãÂ∏à
- PythonÊµãËØïÂ∑•Á®ãÂ∏àÔºàËá™Âä®ÂåñÊµãËØï„ÄÅÊµãËØïÂºÄÂèëÔºâ

> **ËØ¥Êòé**ÔºöÁõÆÂâçÔºå**Êï∞ÊçÆÂàÜÊûêÂíåÊï∞ÊçÆÊåñÊéòÊòØÈùûÂ∏∏ÁÉ≠Èó®ÁöÑÊñπÂêë**ÔºåÂõ†‰∏∫‰∏çÁÆ°ÊòØ‰∫íËÅîÁΩëË°å‰∏öËøòÊòØ‰º†ÁªüË°å‰∏öÈÉΩÂ∑≤ÁªèÁßØÁ¥Ø‰∫ÜÂ§ßÈáèÁöÑÊï∞ÊçÆÔºåÂêÑË°åÂêÑ‰∏öÈÉΩÈúÄË¶ÅÊï∞ÊçÆÂàÜÊûêÂ∏à‰ªéÂ∑≤ÊúâÁöÑÊï∞ÊçÆ‰∏≠ÂèëÁé∞Êõ¥Â§öÁöÑÂïÜ‰∏ö‰ª∑ÂÄºÔºå‰ªéËÄå‰∏∫‰ºÅ‰∏öÁöÑÂÜ≥Á≠ñÊèê‰æõÊï∞ÊçÆÁöÑÊîØÊíëÔºåËøôÂ∞±ÊòØÊâÄË∞ìÁöÑÊï∞ÊçÆÈ©±Âä®ÂÜ≥Á≠ñ„ÄÇ

ÁªôÂàùÂ≠¶ËÄÖÁöÑÂá†‰∏™Âª∫ËÆÆÔºö

- Make English as your working language. ÔºàËÆ©Ëã±ËØ≠Êàê‰∏∫‰Ω†ÁöÑÂ∑•‰ΩúËØ≠Ë®ÄÔºâ
- Practice makes perfect. ÔºàÁÜüËÉΩÁîüÂ∑ßÔºâ
- All experience comes from mistakes. ÔºàÊâÄÊúâÁöÑÁªèÈ™åÈÉΩÊ∫ê‰∫é‰Ω†ÁäØËøáÁöÑÈîôËØØÔºâ
- Don't be one of the leeches. Ôºà‰∏çË¶ÅÂΩì‰º∏ÊâãÂÖöÔºâ
- Either outstanding or out. ÔºàË¶Å‰πàÂá∫‰ºóÔºåË¶Å‰πàÂá∫Â±ÄÔºâ

### Day01~15 - [PythonËØ≠Ë®ÄÂü∫Á°Ä](./Day01-15)

#### Day01 - [ÂàùËØÜPython](./Day01-15/01.ÂàùËØÜPython.md)

- PythonÁÆÄ‰ªã - PythonÁöÑÂéÜÂè≤ / PythonÁöÑ‰ºòÁº∫ÁÇπ / PythonÁöÑÂ∫îÁî®È¢ÜÂüü
- Êê≠Âª∫ÁºñÁ®ãÁéØÂ¢É - WindowsÁéØÂ¢É / LinuxÁéØÂ¢É / MacOSÁéØÂ¢É
- ‰ªéÁªàÁ´ØËøêË°åPythonÁ®ãÂ∫è - Hello, world / `print`ÂáΩÊï∞ / ËøêË°åÁ®ãÂ∫è
- ‰ΩøÁî®IDLE - ‰∫§‰∫íÂºèÁéØÂ¢É(REPL) / ÁºñÂÜôÂ§öË°å‰ª£Á†Å / ËøêË°åÁ®ãÂ∫è / ÈÄÄÂá∫IDLE
- Ê≥®Èáä - Ê≥®ÈáäÁöÑ‰ΩúÁî® / ÂçïË°åÊ≥®Èáä / Â§öË°åÊ≥®Èáä

#### Day02 - [ËØ≠Ë®ÄÂÖÉÁ¥†](./Day01-15/02.ËØ≠Ë®ÄÂÖÉÁ¥†.md)

- Á®ãÂ∫èÂíåËøõÂà∂ - Êåá‰ª§ÂíåÁ®ãÂ∫è / ÂÜØËØ∫‰æùÊõºÊú∫ / ‰∫åËøõÂà∂ÂíåÂçÅËøõÂà∂ / ÂÖ´ËøõÂà∂ÂíåÂçÅÂÖ≠ËøõÂà∂
- ÂèòÈáèÂíåÁ±ªÂûã - ÂèòÈáèÁöÑÂëΩÂêç / ÂèòÈáèÁöÑ‰ΩøÁî® / `input`ÂáΩÊï∞ / Ê£ÄÊü•ÂèòÈáèÁ±ªÂûã / Á±ªÂûãËΩ¨Êç¢
- Êï∞Â≠óÂíåÂ≠óÁ¨¶‰∏≤ - Êï¥Êï∞ / ÊµÆÁÇπÊï∞ / Â§çÊï∞ / Â≠óÁ¨¶‰∏≤ / Â≠óÁ¨¶‰∏≤Âü∫Êú¨Êìç‰Ωú / Â≠óÁ¨¶ÁºñÁ†Å
- ËøêÁÆóÁ¨¶ - Êï∞Â≠¶ËøêÁÆóÁ¨¶ / ËµãÂÄºËøêÁÆóÁ¨¶ / ÊØîËæÉËøêÁÆóÁ¨¶ / ÈÄªËæëËøêÁÆóÁ¨¶ / Ë∫´‰ªΩËøêÁÆóÁ¨¶ / ËøêÁÆóÁ¨¶ÁöÑ‰ºòÂÖàÁ∫ß
- Â∫îÁî®Ê°à‰æã - ÂçéÊ∞èÊ∏©Â∫¶ËΩ¨Êç¢ÊàêÊëÑÊ∞èÊ∏©Â∫¶ / ËæìÂÖ•ÂúÜÁöÑÂçäÂæÑËÆ°ÁÆóÂë®ÈïøÂíåÈù¢ÁßØ / ËæìÂÖ•Âπ¥‰ªΩÂà§Êñ≠ÊòØÂê¶ÊòØÈó∞Âπ¥

#### Day03 - [ÂàÜÊîØÁªìÊûÑ](./Day01-15/03.ÂàÜÊîØÁªìÊûÑ.md)

- ÂàÜÊîØÁªìÊûÑÁöÑÂ∫îÁî®Âú∫ÊôØ - Êù°‰ª∂ / Áº©Ëøõ / ‰ª£Á†ÅÂùó / ÊµÅÁ®ãÂõæ
- ifËØ≠Âè• - ÁÆÄÂçïÁöÑ`if` / `if`-`else`ÁªìÊûÑ / `if`-`elif`-`else`ÁªìÊûÑ / ÂµåÂ•óÁöÑ`if`
- Â∫îÁî®Ê°à‰æã - Áî®Êà∑Ë∫´‰ªΩÈ™åËØÅ / Ëã±Âà∂Âçï‰Ωç‰∏éÂÖ¨Âà∂Âçï‰Ωç‰∫íÊç¢ / Êé∑È™∞Â≠êÂÜ≥ÂÆöÂÅö‰ªÄ‰πà / ÁôæÂàÜÂà∂ÊàêÁª©ËΩ¨Á≠âÁ∫ßÂà∂ / ÂàÜÊÆµÂáΩÊï∞Ê±ÇÂÄº / ËæìÂÖ•‰∏âÊù°ËæπÁöÑÈïøÂ∫¶Â¶ÇÊûúËÉΩÊûÑÊàê‰∏âËßíÂΩ¢Â∞±ËÆ°ÁÆóÂë®ÈïøÂíåÈù¢ÁßØ

#### Day04 - [Âæ™ÁéØÁªìÊûÑ](./Day01-15/04.Âæ™ÁéØÁªìÊûÑ.md)

- Âæ™ÁéØÁªìÊûÑÁöÑÂ∫îÁî®Âú∫ÊôØ - Êù°‰ª∂ / Áº©Ëøõ / ‰ª£Á†ÅÂùó / ÊµÅÁ®ãÂõæ
- whileÂæ™ÁéØ - Âü∫Êú¨ÁªìÊûÑ / `break`ËØ≠Âè• / `continue`ËØ≠Âè•
- forÂæ™ÁéØ - Âü∫Êú¨ÁªìÊûÑ / `range`Á±ªÂûã / Âæ™ÁéØ‰∏≠ÁöÑÂàÜÊîØÁªìÊûÑ / ÂµåÂ•óÁöÑÂæ™ÁéØ / ÊèêÂâçÁªìÊùüÁ®ãÂ∫è 
- Â∫îÁî®Ê°à‰æã - 1~100Ê±ÇÂíå / Âà§Êñ≠Á¥†Êï∞ / ÁåúÊï∞Â≠óÊ∏∏Êàè / ÊâìÂç∞‰πù‰πùË°® / ÊâìÂç∞‰∏âËßíÂΩ¢ÂõæÊ°à / Áå¥Â≠êÂêÉÊ°É / ÁôæÈí±ÁôæÈ∏°

#### Day05 - [ÊûÑÈÄ†Á®ãÂ∫èÈÄªËæë](./Day01-15/05.ÊûÑÈÄ†Á®ãÂ∫èÈÄªËæë.md)

- ÁªèÂÖ∏Ê°à‰æãÔºöÊ∞¥‰ªôËä±Êï∞ / ÁôæÈí±ÁôæÈ∏° / CrapsËµåÂçöÊ∏∏Êàè
- ÁªÉ‰π†È¢òÁõÆÔºöÊñêÊ≥¢ÈÇ£Â•ëÊï∞Âàó / ÂÆåÁæéÊï∞ / Á¥†Êï∞

#### Day06 - [ÂáΩÊï∞ÂíåÊ®°ÂùóÁöÑ‰ΩøÁî®](./Day01-15/06.ÂáΩÊï∞ÂíåÊ®°ÂùóÁöÑ‰ΩøÁî®.md)

- ÂáΩÊï∞ÁöÑ‰ΩúÁî® - ‰ª£Á†ÅÁöÑÂùèÂë≥ÈÅì / Áî®ÂáΩÊï∞Â∞ÅË£ÖÂäüËÉΩÊ®°Âùó
- ÂÆö‰πâÂáΩÊï∞ - `def`ÂÖ≥ÈîÆÂ≠ó / ÂáΩÊï∞Âêç / ÂèÇÊï∞ÂàóË°® / `return`ËØ≠Âè• / Ë∞ÉÁî®Ëá™ÂÆö‰πâÂáΩÊï∞
- Ë∞ÉÁî®ÂáΩÊï∞ - PythonÂÜÖÁΩÆÂáΩÊï∞ /  ÂØºÂÖ•Ê®°ÂùóÂíåÂáΩÊï∞
- ÂáΩÊï∞ÁöÑÂèÇÊï∞ - ÈªòËÆ§ÂèÇÊï∞ / ÂèØÂèòÂèÇÊï∞ / ÂÖ≥ÈîÆÂ≠óÂèÇÊï∞ / ÂëΩÂêçÂÖ≥ÈîÆÂ≠óÂèÇÊï∞
- ÂáΩÊï∞ÁöÑËøîÂõûÂÄº - Ê≤°ÊúâËøîÂõûÂÄº  / ËøîÂõûÂçï‰∏™ÂÄº / ËøîÂõûÂ§ö‰∏™ÂÄº
- ‰ΩúÁî®ÂüüÈóÆÈ¢ò - Â±ÄÈÉ®‰ΩúÁî®Âüü / ÂµåÂ•ó‰ΩúÁî®Âüü / ÂÖ®Â±Ä‰ΩúÁî®Âüü / ÂÜÖÁΩÆ‰ΩúÁî®Âüü / Âíå‰ΩúÁî®ÂüüÁõ∏ÂÖ≥ÁöÑÂÖ≥ÈîÆÂ≠ó
- Áî®Ê®°ÂùóÁÆ°ÁêÜÂáΩÊï∞ - Ê®°ÂùóÁöÑÊ¶ÇÂøµ / Áî®Ëá™ÂÆö‰πâÊ®°ÂùóÁÆ°ÁêÜÂáΩÊï∞ / ÂëΩÂêçÂÜ≤Á™ÅÁöÑÊó∂ÂÄô‰ºöÊÄéÊ†∑ÔºàÂêå‰∏Ä‰∏™Ê®°ÂùóÂíå‰∏çÂêåÁöÑÊ®°ÂùóÔºâ

#### Day07 - [Â≠óÁ¨¶‰∏≤ÂíåÂ∏∏Áî®Êï∞ÊçÆÁªìÊûÑ](./Day01-15/07.Â≠óÁ¨¶‰∏≤ÂíåÂ∏∏Áî®Êï∞ÊçÆÁªìÊûÑ.md)

- Â≠óÁ¨¶‰∏≤ÁöÑ‰ΩøÁî® - ËÆ°ÁÆóÈïøÂ∫¶ / ‰∏ãÊ†áËøêÁÆó / ÂàáÁâá / Â∏∏Áî®ÊñπÊ≥ï
- ÂàóË°®Âü∫Êú¨Áî®Ê≥ï - ÂÆö‰πâÂàóË°® / Áî®‰∏ãË°®ËÆøÈóÆÂÖÉÁ¥† / ‰∏ãÊ†áË∂äÁïå / Ê∑ªÂä†ÂÖÉÁ¥† / Âà†Èô§ÂÖÉÁ¥† / ‰øÆÊîπÂÖÉÁ¥† / ÂàáÁâá / Âæ™ÁéØÈÅçÂéÜ
- ÂàóË°®Â∏∏Áî®Êìç‰Ωú - ËøûÊé• / Â§çÂà∂(Â§çÂà∂ÂÖÉÁ¥†ÂíåÂ§çÂà∂Êï∞ÁªÑ) / ÈïøÂ∫¶ / ÊéíÂ∫è / ÂÄíËΩ¨ / Êü•Êâæ
- ÁîüÊàêÂàóË°® - ‰ΩøÁî®`range`ÂàõÂª∫Êï∞Â≠óÂàóË°® / ÁîüÊàêË°®ËææÂºè / ÁîüÊàêÂô®
- ÂÖÉÁªÑÁöÑ‰ΩøÁî® - ÂÆö‰πâÂÖÉÁªÑ / ‰ΩøÁî®ÂÖÉÁªÑ‰∏≠ÁöÑÂÄº / ‰øÆÊîπÂÖÉÁªÑÂèòÈáè / ÂÖÉÁªÑÂíåÂàóË°®ËΩ¨Êç¢
- ÈõÜÂêàÂü∫Êú¨Áî®Ê≥ï - ÈõÜÂêàÂíåÂàóË°®ÁöÑÂå∫Âà´ /  ÂàõÂª∫ÈõÜÂêà / Ê∑ªÂä†ÂÖÉÁ¥† / Âà†Èô§ÂÖÉÁ¥† /  Ê∏ÖÁ©∫
- ÈõÜÂêàÂ∏∏Áî®Êìç‰Ωú - ‰∫§ÈõÜ / Âπ∂ÈõÜ / Â∑ÆÈõÜ / ÂØπÁß∞Â∑Æ / Â≠êÈõÜ / Ë∂ÖÈõÜ
- Â≠óÂÖ∏ÁöÑÂü∫Êú¨Áî®Ê≥ï - Â≠óÂÖ∏ÁöÑÁâπÁÇπ / ÂàõÂª∫Â≠óÂÖ∏ / Ê∑ªÂä†ÂÖÉÁ¥† / Âà†Èô§ÂÖÉÁ¥† / ÂèñÂÄº / Ê∏ÖÁ©∫
- Â≠óÂÖ∏Â∏∏Áî®Êìç‰Ωú - `keys`ÊñπÊ≥ï / `values`ÊñπÊ≥ï / `items`ÊñπÊ≥ï / `setdefault`ÊñπÊ≥ï
- Âü∫Á°ÄÁªÉ‰π† - Ë∑ëÈ©¨ÁÅØÊïàÊûú / ÂàóË°®ÊâæÊúÄÂ§ßÂÖÉÁ¥† / ÁªüËÆ°ËÄÉËØïÊàêÁª©ÁöÑÂπ≥ÂùáÂàÜ / FibonacciÊï∞Âàó / Êù®Ëæâ‰∏âËßí
- ÁªºÂêàÊ°à‰æã - ÂèåËâ≤ÁêÉÈÄâÂè∑ / ‰∫ïÂ≠óÊ£ã

#### Day08 - [Èù¢ÂêëÂØπË±°ÁºñÁ®ãÂü∫Á°Ä](./Day01-15/08.Èù¢ÂêëÂØπË±°ÁºñÁ®ãÂü∫Á°Ä.md)

- Á±ªÂíåÂØπË±° - ‰ªÄ‰πàÊòØÁ±ª / ‰ªÄ‰πàÊòØÂØπË±° / Èù¢ÂêëÂØπË±°ÂÖ∂‰ªñÁõ∏ÂÖ≥Ê¶ÇÂøµ
- ÂÆö‰πâÁ±ª - Âü∫Êú¨ÁªìÊûÑ / Â±ûÊÄßÂíåÊñπÊ≥ï / ÊûÑÈÄ†Âô® / ÊûêÊûÑÂô® / `__str__`ÊñπÊ≥ï
- ‰ΩøÁî®ÂØπË±° - ÂàõÂª∫ÂØπË±° / ÁªôÂØπË±°ÂèëÊ∂àÊÅØ
- Èù¢ÂêëÂØπË±°ÁöÑÂõõÂ§ßÊîØÊü± - ÊäΩË±° / Â∞ÅË£Ö / ÁªßÊâø / Â§öÊÄÅ
- Âü∫Á°ÄÁªÉ‰π† - ÂÆö‰πâÂ≠¶ÁîüÁ±ª / ÂÆö‰πâÊó∂ÈíüÁ±ª / ÂÆö‰πâÂõæÂΩ¢Á±ª / ÂÆö‰πâÊ±ΩËΩ¶Á±ª

#### Day09 - [Èù¢ÂêëÂØπË±°ËøõÈò∂](./Day01-15/09.Èù¢ÂêëÂØπË±°ËøõÈò∂.md)

- Â±ûÊÄß - Á±ªÂ±ûÊÄß / ÂÆû‰æãÂ±ûÊÄß / Â±ûÊÄßËÆøÈóÆÂô® / Â±ûÊÄß‰øÆÊîπÂô® / Â±ûÊÄßÂà†Èô§Âô® / ‰ΩøÁî®`__slots__`
- Á±ª‰∏≠ÁöÑÊñπÊ≥ï - ÂÆû‰æãÊñπÊ≥ï / Á±ªÊñπÊ≥ï / ÈùôÊÄÅÊñπÊ≥ï
- ËøêÁÆóÁ¨¶ÈáçËΩΩ - `__add__` / `__sub__` / `__or__` /`__getitem__` / `__setitem__` / `__len__` / `__repr__` / `__gt__` / `__lt__` / `__le__` / `__ge__` / `__eq__` / `__ne__` / `__contains__` 
- Á±ª(ÁöÑÂØπË±°)‰πãÈó¥ÁöÑÂÖ≥Á≥ª - ÂÖ≥ËÅî / ÁªßÊâø / ‰æùËµñ
- ÁªßÊâøÂíåÂ§öÊÄÅ - ‰ªÄ‰πàÊòØÁªßÊâø / ÁªßÊâøÁöÑËØ≠Ê≥ï / Ë∞ÉÁî®Áà∂Á±ªÊñπÊ≥ï / ÊñπÊ≥ïÈáçÂÜô / Á±ªÂûãÂà§ÂÆö / Â§öÈáçÁªßÊâø / Ëè±ÂΩ¢ÁªßÊâø(ÈíªÁü≥ÁªßÊâø)ÂíåC3ÁÆóÊ≥ï
- ÁªºÂêàÊ°à‰æã - Â∑•ËµÑÁªìÁÆóÁ≥ªÁªü / Âõæ‰π¶Ëá™Âä®ÊäòÊâ£Á≥ªÁªü / Ëá™ÂÆö‰πâÂàÜÊï∞Á±ª

#### Day10 - [ÂõæÂΩ¢Áî®Êà∑ÁïåÈù¢ÂíåÊ∏∏ÊàèÂºÄÂèë](./Day01-15/10.ÂõæÂΩ¢Áî®Êà∑ÁïåÈù¢ÂíåÊ∏∏ÊàèÂºÄÂèë.md)

- ‰ΩøÁî®`tkinter`ÂºÄÂèëGUIÁ®ãÂ∫è
- ‰ΩøÁî®`pygame`‰∏âÊñπÂ∫ìÂºÄÂèëÊ∏∏ÊàèÂ∫îÁî®
- ‚ÄúÂ§ßÁêÉÂêÉÂ∞èÁêÉ‚ÄùÊ∏∏Êàè

#### Day11 - [Êñá‰ª∂ÂíåÂºÇÂ∏∏](./Day01-15/11.Êñá‰ª∂ÂíåÂºÇÂ∏∏.md)

- ËØªÊñá‰ª∂ - ËØªÂèñÊï¥‰∏™Êñá‰ª∂ / ÈÄêË°åËØªÂèñ / Êñá‰ª∂Ë∑ØÂæÑ
- ÂÜôÊñá‰ª∂ - Ë¶ÜÁõñÂÜôÂÖ• / ËøΩÂä†ÂÜôÂÖ• / ÊñáÊú¨Êñá‰ª∂ / ‰∫åËøõÂà∂Êñá‰ª∂
- ÂºÇÂ∏∏Â§ÑÁêÜ - ÂºÇÂ∏∏Êú∫Âà∂ÁöÑÈáçË¶ÅÊÄß / `try`-`except`‰ª£Á†ÅÂùó / `else`‰ª£Á†ÅÂùó / `finally`‰ª£Á†ÅÂùó / ÂÜÖÁΩÆÂºÇÂ∏∏Á±ªÂûã / ÂºÇÂ∏∏Ê†à / `raise`ËØ≠Âè•
- Êï∞ÊçÆÊåÅ‰πÖÂåñ - CSVÊñá‰ª∂Ê¶ÇËø∞ / `csv`Ê®°ÂùóÁöÑÂ∫îÁî® / JSONÊï∞ÊçÆÊ†ºÂºè / `json`Ê®°ÂùóÁöÑÂ∫îÁî®

#### Day12 - [Â≠óÁ¨¶‰∏≤ÂíåÊ≠£ÂàôË°®ËææÂºè](./Day01-15/12.Â≠óÁ¨¶‰∏≤ÂíåÊ≠£ÂàôË°®ËææÂºè.md)

- Â≠óÁ¨¶‰∏≤È´òÁ∫ßÊìç‰Ωú - ËΩ¨‰πâÂ≠óÁ¨¶ / ÂéüÂßãÂ≠óÁ¨¶‰∏≤ / Â§öË°åÂ≠óÁ¨¶‰∏≤ / `in`Âíå`not in`ËøêÁÆóÁ¨¶ / `is_xxx`ÊñπÊ≥ï / `join`Âíå`split`ÊñπÊ≥ï / `strip`Áõ∏ÂÖ≥ÊñπÊ≥ï / `pyperclip`Ê®°Âùó / ‰∏çÂèòÂ≠óÁ¨¶‰∏≤ÂíåÂèØÂèòÂ≠óÁ¨¶‰∏≤ / `StringIO`ÁöÑ‰ΩøÁî®
- Ê≠£ÂàôË°®ËææÂºèÂÖ•Èó® - Ê≠£ÂàôË°®ËææÂºèÁöÑ‰ΩúÁî® / ÂÖÉÂ≠óÁ¨¶ / ËΩ¨‰πâ / ÈáèËØç / ÂàÜÁªÑ / Èõ∂ÂÆΩÊñ≠Ë®Ä /Ë¥™Â©™ÂåπÈÖç‰∏éÊÉ∞ÊÄßÂåπÈÖçÊáíÊÉ∞ / ‰ΩøÁî®`re`Ê®°ÂùóÂÆûÁé∞Ê≠£ÂàôË°®ËææÂºèÊìç‰ΩúÔºàÂåπÈÖç„ÄÅÊêúÁ¥¢„ÄÅÊõøÊç¢„ÄÅÊçïËé∑Ôºâ
- ‰ΩøÁî®Ê≠£ÂàôË°®ËææÂºè - `re`Ê®°Âùó / `compile`ÂáΩÊï∞ / `group`Âíå`groups`ÊñπÊ≥ï / `match`ÊñπÊ≥ï / `search`ÊñπÊ≥ï / `findall`Âíå`finditer`ÊñπÊ≥ï / `sub`Âíå`subn`ÊñπÊ≥ï / `split`ÊñπÊ≥ï
- Â∫îÁî®Ê°à‰æã - ‰ΩøÁî®Ê≠£ÂàôË°®ËææÂºèÈ™åËØÅËæìÂÖ•ÁöÑÂ≠óÁ¨¶‰∏≤

#### Day13 - [ËøõÁ®ãÂíåÁ∫øÁ®ã](./Day01-15/13.ËøõÁ®ãÂíåÁ∫øÁ®ã.md)

- ËøõÁ®ãÂíåÁ∫øÁ®ãÁöÑÊ¶ÇÂøµ - ‰ªÄ‰πàÊòØËøõÁ®ã / ‰ªÄ‰πàÊòØÁ∫øÁ®ã / Â§öÁ∫øÁ®ãÁöÑÂ∫îÁî®Âú∫ÊôØ
- ‰ΩøÁî®ËøõÁ®ã - `fork`ÂáΩÊï∞ / `multiprocessing`Ê®°Âùó / ËøõÁ®ãÊ±† / ËøõÁ®ãÈó¥ÈÄö‰ø°
- ‰ΩøÁî®Á∫øÁ®ã -  `threading`Ê®°Âùó / `Thread`Á±ª / `RLock`Á±ª / `Condition`Á±ª / Á∫øÁ®ãÊ±†

#### Day14 - [ÁΩëÁªúÁºñÁ®ãÂÖ•Èó®ÂíåÁΩëÁªúÂ∫îÁî®ÂºÄÂèë](./Day01-15/14.ÁΩëÁªúÁºñÁ®ãÂÖ•Èó®ÂíåÁΩëÁªúÂ∫îÁî®ÂºÄÂèë.md)

- ËÆ°ÁÆóÊú∫ÁΩëÁªúÂü∫Á°Ä - ËÆ°ÁÆóÊú∫ÁΩëÁªúÂèëÂ±ïÂè≤ / ‚ÄúTCP-IP‚ÄùÊ®°Âûã / IPÂú∞ÂùÄ / Á´ØÂè£ / ÂçèËÆÆ / ÂÖ∂‰ªñÁõ∏ÂÖ≥Ê¶ÇÂøµ
- ÁΩëÁªúÂ∫îÁî®Ê®°Âºè - ‚ÄúÂÆ¢Êà∑Á´Ø-ÊúçÂä°Âô®‚ÄùÊ®°Âºè / ‚ÄúÊµèËßàÂô®-ÊúçÂä°Âô®‚ÄùÊ®°Âºè
- Âü∫‰∫éHTTPÂçèËÆÆËÆøÈóÆÁΩëÁªúËµÑÊ∫ê - ÁΩëÁªúAPIÊ¶ÇËø∞ / ËÆøÈóÆURL / `requests`‰∏âÊñπÂ∫ì / Ëß£ÊûêJSONÊ†ºÂºèÊï∞ÊçÆ
- PythonÁΩëÁªúÁºñÁ®ã - Â•óÊé•Â≠óÁöÑÊ¶ÇÂøµ / `socket`Ê®°Âùó /  `socket`ÂáΩÊï∞ / ÂàõÂª∫TCPÊúçÂä°Âô® / ÂàõÂª∫TCPÂÆ¢Êà∑Á´Ø / ÂàõÂª∫UDPÊúçÂä°Âô® / ÂàõÂª∫UDPÂÆ¢Êà∑Á´Ø
- ÁîµÂ≠êÈÇÆ‰ª∂ - SMTPÂçèËÆÆ / POP3ÂçèËÆÆ / IMAPÂçèËÆÆ / `smtplib`Ê®°Âùó / `poplib`Ê®°Âùó / `imaplib`Ê®°Âùó
- Áü≠‰ø°ÊúçÂä° - Ë∞ÉÁî®Áü≠‰ø°ÊúçÂä°ÁΩëÂÖ≥

#### Day15 - [ÂõæÂÉèÂíåÊñáÊ°£Â§ÑÁêÜ](./Day01-15/15.ÂõæÂÉèÂíåÂäûÂÖ¨ÊñáÊ°£Â§ÑÁêÜ.md)

- Áî®PillowÂ§ÑÁêÜÂõæÁâá - ÂõæÁâáËØªÂÜô / ÂõæÁâáÂêàÊàê / Âá†‰ΩïÂèòÊç¢ / Ëâ≤ÂΩ©ËΩ¨Êç¢ / Êª§ÈïúÊïàÊûú
- ËØªÂÜôWordÊñáÊ°£ - ÊñáÊú¨ÂÜÖÂÆπÁöÑÂ§ÑÁêÜ / ÊÆµËêΩ / È°µÁúâÂíåÈ°µËÑö / Ê†∑ÂºèÁöÑÂ§ÑÁêÜ
- ËØªÂÜôExcelÊñá‰ª∂ - `xlrd` / `xlwt` / `openpyxl`

### Day16~Day20 - [PythonËØ≠Ë®ÄËøõÈò∂ ](./Day16-20/16-20.PythonËØ≠Ë®ÄËøõÈò∂.md)

- Â∏∏Áî®Êï∞ÊçÆÁªìÊûÑ
- ÂáΩÊï∞ÁöÑÈ´òÁ∫ßÁî®Ê≥ï - ‚Äú‰∏ÄÁ≠âÂÖ¨Ê∞ë‚Äù / È´òÈò∂ÂáΩÊï∞ / LambdaÂáΩÊï∞ / ‰ΩúÁî®ÂüüÂíåÈó≠ÂåÖ / Ë£ÖÈ•∞Âô®
- Èù¢ÂêëÂØπË±°È´òÁ∫ßÁü•ËØÜ - ‚Äú‰∏âÂ§ßÊîØÊü±‚Äù / Á±ª‰∏éÁ±ª‰πãÈó¥ÁöÑÂÖ≥Á≥ª / ÂûÉÂúæÂõûÊî∂ / È≠îÊúØÂ±ûÊÄßÂíåÊñπÊ≥ï / Ê∑∑ÂÖ• / ÂÖÉÁ±ª / Èù¢ÂêëÂØπË±°ËÆæËÆ°ÂéüÂàô / GoFËÆæËÆ°Ê®°Âºè
- Ëø≠‰ª£Âô®ÂíåÁîüÊàêÂô® - Áõ∏ÂÖ≥È≠îÊúØÊñπÊ≥ï / ÂàõÂª∫ÁîüÊàêÂô®ÁöÑ‰∏§ÁßçÊñπÂºè / 
- Âπ∂ÂèëÂíåÂºÇÊ≠•ÁºñÁ®ã - Â§öÁ∫øÁ®ã / Â§öËøõÁ®ã / ÂºÇÊ≠•IO / `async`Âíå`awai`t

### Day21~30 - [WebÂâçÁ´ØÂÖ•Èó®](./Day21-30/21-30.WebÂâçÁ´ØÊ¶ÇËø∞.md)

- Áî®HTMLÊ†áÁ≠æÊâøËΩΩÈ°µÈù¢ÂÜÖÂÆπ
- Áî®CSSÊ∏≤ÊüìÈ°µÈù¢
- Áî®JavaScriptÂ§ÑÁêÜ‰∫§‰∫íÂºèË°å‰∏∫
- jQueryÂÖ•Èó®ÂíåÊèêÈ´ò
- Vue.jsÂÖ•Èó®
- ElementÁöÑ‰ΩøÁî®
- BootstrapÁöÑ‰ΩøÁî®

### Day31~35 - [Áé©ËΩ¨LinuxÊìç‰ΩúÁ≥ªÁªü](./Day31-35/31-35.Áé©ËΩ¨LinuxÊìç‰ΩúÁ≥ªÁªü.md)

- Êìç‰ΩúÁ≥ªÁªüÂèëÂ±ïÂè≤ÂíåLinuxÊ¶ÇËø∞
- LinuxÂü∫Á°ÄÂëΩ‰ª§
- Linux‰∏≠ÁöÑÂÆûÁî®Á®ãÂ∫è
- LinuxÁöÑÊñá‰ª∂Á≥ªÁªü
- VimÁºñËæëÂô®ÁöÑÂ∫îÁî®
- ÁéØÂ¢ÉÂèòÈáèÂíåShellÁºñÁ®ã
- ËΩØ‰ª∂ÁöÑÂÆâË£ÖÂíåÊúçÂä°ÁöÑÈÖçÁΩÆ
- ÁΩëÁªúËÆøÈóÆÂíåÁÆ°ÁêÜ
- ÂÖ∂‰ªñÁõ∏ÂÖ≥ÂÜÖÂÆπ

### Day36~40 - [Êï∞ÊçÆÂ∫ìÂü∫Á°ÄÂíåËøõÈò∂](./Day36-40)

- ÂÖ≥Á≥ªÂûãÊï∞ÊçÆÂ∫ìÊ¶ÇËø∞
- MySQLÁöÑÂÆâË£ÖÂíå‰ΩøÁî®
- SQLÁöÑ‰ΩøÁî®
- DDL - Êï∞ÊçÆÂÆö‰πâËØ≠Ë®Ä - `create` / `drop` / `alter`
- DML - Êï∞ÊçÆÊìç‰ΩúËØ≠Ë®Ä - `insert` / `delete` / `update`
- DQL - Êï∞ÊçÆÊü•ËØ¢ËØ≠Ë®Ä - `select`
- DCL - Êï∞ÊçÆÊéßÂà∂ËØ≠Ë®Ä - `grant` / `revoke`
- MySQLÊñ∞ÁâπÊÄß
- Á™óÂè£ÂáΩÊï∞ÁöÑÂ∫îÁî®
- JSONÊï∞ÊçÆÁ±ªÂûã
- Áõ∏ÂÖ≥Áü•ËØÜ
- Êï∞ÊçÆÂÆåÊï¥ÊÄßÂíå‰∏ÄËá¥ÊÄß
- ËßÜÂõæ„ÄÅÂáΩÊï∞„ÄÅËøáÁ®ã„ÄÅËß¶ÂèëÂô®
- ‰∫ãÂä°ÂíåÈîÅ
- ÊâßË°åËÆ°ÂàíÂíåÁ¥¢Âºï
- ËåÉÂºèÁêÜËÆ∫ÂíåÂèçËåÉÂºèËÆæËÆ°
- Âú®Python‰∏≠Êìç‰ΩúMySQL

### Day41~55 - [ÂÆûÊàòDjango](./Day41-55)

#### Day41 - [DjangoÂø´ÈÄü‰∏äÊâã](./Day41-55/41.DjangoÂø´ÈÄü‰∏äÊâã.md)

- WebÂ∫îÁî®Â∑•‰ΩúÊú∫Âà∂
- HTTPËØ∑Ê±ÇÂíåÂìçÂ∫î
- DjangoÊ°ÜÊû∂Ê¶ÇËø∞
- 5ÂàÜÈíüÂø´ÈÄü‰∏äÊâã

#### Day42 - [Ê∑±ÂÖ•Ê®°Âûã](./Day41-55/42.Ê∑±ÂÖ•Ê®°Âûã.md)

- ÂÖ≥Á≥ªÂûãÊï∞ÊçÆÂ∫ìÈÖçÁΩÆ
- ‰ΩøÁî®ORMÂÆåÊàêÂØπÊ®°ÂûãÁöÑCRUDÊìç‰Ωú
- ÁÆ°ÁêÜÂêéÂè∞ÁöÑ‰ΩøÁî®
- DjangoÊ®°ÂûãÊúÄ‰Ω≥ÂÆûË∑µ
- Ê®°ÂûãÂÆö‰πâÂèÇËÄÉ

#### Day43 - [ÈùôÊÄÅËµÑÊ∫êÂíåAjaxËØ∑Ê±Ç](./Day41-55/43.ÈùôÊÄÅËµÑÊ∫êÂíåAjaxËØ∑Ê±Ç.md)

- Âä†ËΩΩÈùôÊÄÅËµÑÊ∫ê
- AjaxÊ¶ÇËø∞
- Áî®AjaxÂÆûÁé∞ÊäïÁ•®ÂäüËÉΩ

#### Day44 - [CookieÂíåSession](./Day41-55/44.CookieÂíåSession.md)

- ÂÆûÁé∞Áî®Êà∑Ë∑üË∏™
- cookieÂíåsessionÁöÑÂÖ≥Á≥ª
- DjangoÊ°ÜÊû∂ÂØπsessionÁöÑÊîØÊåÅ
- ËßÜÂõæÂáΩÊï∞‰∏≠ÁöÑcookieËØªÂÜôÊìç‰Ωú

#### Day45 - [Êä•Ë°®ÂíåÊó•Âøó](./Day41-55/45.Âà∂‰ΩúÊä•Ë°®.md)

- ÈÄöËøá`HttpResponse`‰øÆÊîπÂìçÂ∫îÂ§¥
- ‰ΩøÁî®`StreamingHttpResponse`Â§ÑÁêÜÂ§ßÊñá‰ª∂
- ‰ΩøÁî®`xlwt`ÁîüÊàêExcelÊä•Ë°®
- ‰ΩøÁî®`reportlab`ÁîüÊàêPDFÊä•Ë°®
- ‰ΩøÁî®EChartsÁîüÊàêÂâçÁ´ØÂõæË°®

#### Day46 - [Êó•ÂøóÂíåË∞ÉËØïÂ∑•ÂÖ∑Ê†è](./Day41-55/46.Êó•ÂøóÂíåË∞ÉËØïÂ∑•ÂÖ∑Ê†è.md)

- ÈÖçÁΩÆÊó•Âøó
- ÈÖçÁΩÆDjango-Debug-Toolbar
- ‰ºòÂåñORM‰ª£Á†Å

#### Day47 - [‰∏≠Èó¥‰ª∂ÁöÑÂ∫îÁî®](./Day41-55/47.‰∏≠Èó¥‰ª∂ÁöÑÂ∫îÁî®.md)

- ‰ªÄ‰πàÊòØ‰∏≠Èó¥‰ª∂
- DjangoÊ°ÜÊû∂ÂÜÖÁΩÆÁöÑ‰∏≠Èó¥‰ª∂
- Ëá™ÂÆö‰πâ‰∏≠Èó¥‰ª∂ÂèäÂÖ∂Â∫îÁî®Âú∫ÊôØ

#### Day48 - [ÂâçÂêéÁ´ØÂàÜÁ¶ªÂºÄÂèëÂÖ•Èó®](./Day41-55/48.ÂâçÂêéÁ´ØÂàÜÁ¶ªÂºÄÂèëÂÖ•Èó®.md)

- ËøîÂõûJSONÊ†ºÂºèÁöÑÊï∞ÊçÆ
- Áî®Vue.jsÊ∏≤ÊüìÈ°µÈù¢

#### Day49 - [RESTfulÊû∂ÊûÑÂíåDRFÂÖ•Èó®](./Day41-55/49.RESTfulÊû∂ÊûÑÂíåDRFÂÖ•Èó®.md)

#### Day50 - [RESTfulÊû∂ÊûÑÂíåDRFËøõÈò∂](./Day41-55/50.RESTfulÊû∂ÊûÑÂíåDRFËøõÈò∂.md)

#### Day51 - [‰ΩøÁî®ÁºìÂ≠ò](./Day41-55/51.‰ΩøÁî®ÁºìÂ≠ò.md)

- ÁΩëÁ´ô‰ºòÂåñÁ¨¨‰∏ÄÂÆöÂæã

- Âú®DjangoÈ°πÁõÆ‰∏≠‰ΩøÁî®RedisÊèê‰æõÁºìÂ≠òÊúçÂä°
- Âú®ËßÜÂõæÂáΩÊï∞‰∏≠ËØªÂÜôÁºìÂ≠ò
- ‰ΩøÁî®Ë£ÖÈ•∞Âô®ÂÆûÁé∞È°µÈù¢ÁºìÂ≠ò
- ‰∏∫Êï∞ÊçÆÊé•Âè£Êèê‰æõÁºìÂ≠òÊúçÂä°

#### Day52 - [Êé•ÂÖ•‰∏âÊñπÂπ≥Âè∞](./Day41-55/52.Êé•ÂÖ•‰∏âÊñπÂπ≥Âè∞.md)

- Êñá‰ª∂‰∏ä‰º†Ë°®ÂçïÊéß‰ª∂ÂíåÂõæÁâáÊñá‰ª∂È¢ÑËßà
- ÊúçÂä°Âô®Á´ØÂ¶Ç‰ΩïÂ§ÑÁêÜ‰∏ä‰º†ÁöÑÊñá‰ª∂

#### Day53 - [ÂºÇÊ≠•‰ªªÂä°ÂíåÂÆöÊó∂‰ªªÂä°](./Day41-55/53.ÂºÇÊ≠•‰ªªÂä°ÂíåÂÆöÊó∂‰ªªÂä°.md)

- ÁΩëÁ´ô‰ºòÂåñÁ¨¨‰∫åÂÆöÂæã
- ÈÖçÁΩÆÊ∂àÊÅØÈòüÂàóÊúçÂä°
- Âú®È°πÁõÆ‰∏≠‰ΩøÁî®CeleryÂÆûÁé∞‰ªªÂä°ÂºÇÊ≠•Âåñ
- Âú®È°πÁõÆ‰∏≠‰ΩøÁî®CeleryÂÆûÁé∞ÂÆöÊó∂‰ªªÂä°

#### Day54 - [ÂçïÂÖÉÊµãËØï](./Day41-55/54.ÂçïÂÖÉÊµãËØï.md)

#### Day55 - [È°πÁõÆ‰∏äÁ∫ø](./Day41-55/55.È°πÁõÆ‰∏äÁ∫ø.md)

- Python‰∏≠ÁöÑÂçïÂÖÉÊµãËØï
- DjangoÊ°ÜÊû∂ÂØπÂçïÂÖÉÊµãËØïÁöÑÊîØÊåÅ
- ‰ΩøÁî®ÁâàÊú¨ÊéßÂà∂Á≥ªÁªü
- ÈÖçÁΩÆÂíå‰ΩøÁî®uWSGI
- Âä®ÈùôÂàÜÁ¶ªÂíåNginxÈÖçÁΩÆ
- ÈÖçÁΩÆHTTPS
- ÈÖçÁΩÆÂüüÂêçËß£Êûê

### Day56~60 - [Áî®FastAPIÂºÄÂèëÊï∞ÊçÆÊé•Âè£](./Day56-60/56-60.Áî®FastAPIÂºÄÂèëÊï∞ÊçÆÊé•Âè£.md)

- FastAPI‰∫îÂàÜÈíü‰∏äÊâã
- ËØ∑Ê±ÇÂíåÂìçÂ∫î
- Êé•ÂÖ•ÂÖ≥Á≥ªÂûãÊï∞ÊçÆÂ∫ì
- ‰æùËµñÊ≥®ÂÖ•
- ‰∏≠Èó¥‰ª∂
- ÂºÇÊ≠•Âåñ
- ËôöÊãüÂåñÈÉ®ÁΩ≤ÔºàDockerÔºâ
- È°πÁõÆÂÆûÊàòÔºöËΩ¶ËæÜËøùÁ´†Êü•ËØ¢È°πÁõÆ

### Day61~65 - [Áà¨Ëô´ÂºÄÂèë](./Day61-65)

#### Day61 - [ÁΩëÁªúÊï∞ÊçÆÈááÈõÜÊ¶ÇËø∞](./Day61-65/61.ÁΩëÁªúÊï∞ÊçÆÈááÈõÜÊ¶ÇËø∞.md)

- ÁΩëÁªúÁà¨Ëô´ÁöÑÊ¶ÇÂøµÂèäÂÖ∂Â∫îÁî®È¢ÜÂüü
- ÁΩëÁªúÁà¨Ëô´ÁöÑÂêàÊ≥ïÊÄßÊé¢ËÆ®
- ÂºÄÂèëÁΩëÁªúÁà¨Ëô´ÁöÑÁõ∏ÂÖ≥Â∑•ÂÖ∑
- ‰∏Ä‰∏™Áà¨Ëô´Á®ãÂ∫èÁöÑÊûÑÊàê

#### Day62 - Êï∞ÊçÆÊäìÂèñÂíåËß£Êûê

- [‰ΩøÁî®`requests`‰∏âÊñπÂ∫ìÂÆûÁé∞Êï∞ÊçÆÊäìÂèñ](./Day61-65/62.Áî®PythonËé∑ÂèñÁΩëÁªúËµÑÊ∫ê-1.md)
- [È°µÈù¢Ëß£ÊûêÁöÑ‰∏âÁßçÊñπÂºè](./Day61-65/62.Áî®PythonËß£ÊûêHTMLÈ°µÈù¢-2.md)
    - Ê≠£ÂàôË°®ËææÂºèËß£Êûê
    - XPathËß£Êûê
    - CSSÈÄâÊã©Âô®Ëß£Êûê


#### Day63 - Python‰∏≠ÁöÑÂπ∂ÂèëÁºñÁ®ã

- [Â§öÁ∫øÁ®ã](./Day61-65/63.Python‰∏≠ÁöÑÂπ∂ÂèëÁºñÁ®ã-1.md)
- [Â§öËøõÁ®ã](./Day61-65/63.Python‰∏≠ÁöÑÂπ∂ÂèëÁºñÁ®ã-2.md)
- [ÂºÇÊ≠•I/O](./Day61-65/63.Python‰∏≠ÁöÑÂπ∂ÂèëÁºñÁ®ã-3.md)

#### Day64 - [‰ΩøÁî®SeleniumÊäìÂèñÁΩëÈ°µÂä®ÊÄÅÂÜÖÂÆπ](./Day61-65/64.‰ΩøÁî®SeleniumÊäìÂèñÁΩëÈ°µÂä®ÊÄÅÂÜÖÂÆπ.md)

#### Day65 - [Áà¨Ëô´Ê°ÜÊû∂ScrapyÁÆÄ‰ªã](./Day61-65/65.Áà¨Ëô´Ê°ÜÊû∂ScrapyÁÆÄ‰ªã.md)

### Day66~80 - [Êï∞ÊçÆÂàÜÊûê](./Day66-80)

#### Day66 - [Êï∞ÊçÆÂàÜÊûêÊ¶ÇËø∞](./Day66-80/66.Êï∞ÊçÆÂàÜÊûêÊ¶ÇËø∞.md)

#### Day67 - [ÁéØÂ¢ÉÂáÜÂ§á](./Day66-80/67.ÁéØÂ¢ÉÂáÜÂ§á.md)

#### Day68 - [NumPyÁöÑÂ∫îÁî®-1](./Day66-80/68.NumPyÁöÑÂ∫îÁî®-1.md)

#### Day69 - [NumPyÁöÑÂ∫îÁî®-2](./Day66-80/69.NumPyÁöÑÂ∫îÁî®-2.md)

#### Day70 - [NumPyÁöÑÂ∫îÁî®-3](./Day66-80/70.NumPyÁöÑÂ∫îÁî®-3.md)

#### Day71 - [NumPyÁöÑÂ∫îÁî®-4](./Day66-80/71.NumPyÁöÑÂ∫îÁî®-4.md)

#### Day72 - [Ê∑±ÂÖ•ÊµÖÂá∫pandas-1](./Day66-80/72.Ê∑±ÂÖ•ÊµÖÂá∫pandas-1.md)

#### Day73 - [Ê∑±ÂÖ•ÊµÖÂá∫pandas-2](./Day66-80/73.Ê∑±ÂÖ•ÊµÖÂá∫pandas-2.md)

#### Day74 - [Ê∑±ÂÖ•ÊµÖÂá∫pandas-3](./Day66-80/74.Ê∑±ÂÖ•ÊµÖÂá∫pandas-3.md)

#### Day75 - [Ê∑±ÂÖ•ÊµÖÂá∫pandas-4](./Day66-80/75.Ê∑±ÂÖ•ÊµÖÂá∫pandas-4.md)

#### Day76 - [Ê∑±ÂÖ•ÊµÖÂá∫pandas-5](./Day66-80/76.Ê∑±ÂÖ•ÊµÖÂá∫pandas-5.md)

#### Day77 - [Ê∑±ÂÖ•ÊµÖÂá∫pandas-6](./Day66-80/77.Ê∑±ÂÖ•ÊµÖÂá∫pandas-6.md)

#### Day78 - [Êï∞ÊçÆÂèØËßÜÂåñ-1](./Day66-80/78.Êï∞ÊçÆÂèØËßÜÂåñ-1.md)

#### Day79 - [Êï∞ÊçÆÂèØËßÜÂåñ-2](./Day66-80/79.Êï∞ÊçÆÂèØËßÜÂåñ-2.md)

#### Day80 - [Êï∞ÊçÆÂèØËßÜÂåñ-3](./Day66-80/80.Êï∞ÊçÆÂèØËßÜÂåñ-3.md)

### Day81~90 - [Êú∫Âô®Â≠¶‰π†ÂíåÊ∑±Â∫¶Â≠¶‰π†](./Day81-90)

#### Day81 - [Êú∫Âô®Â≠¶‰π†Âü∫Á°Ä](./Day81-90/81.Êú∫Âô®Â≠¶‰π†Âü∫Á°Ä.md)

#### Day82 - [kÊúÄËøëÈÇªÂàÜÁ±ª](./Day81-90/82.kÊúÄËøëÈÇªÂàÜÁ±ª.md)

#### Day83 - [ÂÜ≥Á≠ñÊ†ë](./Day81-90/83.ÂÜ≥Á≠ñÊ†ë.md)

#### Day84 - [Ë¥ùÂè∂ÊñØÂàÜÁ±ª](./Day81-90/84.Ë¥ùÂè∂ÊñØÂàÜÁ±ª.md)

#### Day85 - [ÊîØÊåÅÂêëÈáèÊú∫](./Day81-90/85.ÊîØÊåÅÂêëÈáèÊú∫.md)

#### Day86 - [K-ÂùáÂÄºËÅöÁ±ª](./Day81-90/86.K-ÂùáÂÄºËÅöÁ±ª.md)

#### Day87 - [ÂõûÂΩíÂàÜÊûê](./Day81-90/87.ÂõûÂΩíÂàÜÊûê.md)

#### Day88 - [Ê∑±Â∫¶Â≠¶‰π†ÂÖ•Èó®](./Day81-90/88.Ê∑±Â∫¶Â≠¶‰π†ÂÖ•Èó®.md)

#### Day89 - [PyTorchÊ¶ÇËø∞](./Day81-90/89.PyTorchÊ¶ÇËø∞.md)

#### Day90 - [PyTorchÂÆûÊàò](./Day81-90/90.PyTorchÂÆûÊàò.md)

### Day91~100 - [Âõ¢ÈòüÈ°πÁõÆÂºÄÂèë](./Day91-100)

#### Á¨¨91Â§©Ôºö[Âõ¢ÈòüÈ°πÁõÆÂºÄÂèëÁöÑÈóÆÈ¢òÂíåËß£ÂÜ≥ÊñπÊ°à](./Day91-100/91.Âõ¢ÈòüÈ°πÁõÆÂºÄÂèëÁöÑÈóÆÈ¢òÂíåËß£ÂÜ≥ÊñπÊ°à.md)

1. ËΩØ‰ª∂ËøáÁ®ãÊ®°Âûã
   - ÁªèÂÖ∏ËøáÁ®ãÊ®°ÂûãÔºàÁÄëÂ∏ÉÊ®°ÂûãÔºâ
     - ÂèØË°åÊÄßÂàÜÊûêÔºàÁ†îÁ©∂ÂÅöËøòÊòØ‰∏çÂÅöÔºâÔºåËæìÂá∫„ÄäÂèØË°åÊÄßÂàÜÊûêÊä•Âëä„Äã„ÄÇ
     - ÈúÄÊ±ÇÂàÜÊûêÔºàÁ†îÁ©∂ÂÅö‰ªÄ‰πàÔºâÔºåËæìÂá∫„ÄäÈúÄÊ±ÇËßÑÊ†ºËØ¥Êòé‰π¶„ÄãÂíå‰∫ßÂìÅÁïåÈù¢ÂéüÂûãÂõæ„ÄÇ
     - Ê¶ÇË¶ÅËÆæËÆ°ÂíåËØ¶ÁªÜËÆæËÆ°ÔºåËæìÂá∫Ê¶ÇÂøµÊ®°ÂûãÂõæÔºàERÂõæÔºâ„ÄÅÁâ©ÁêÜÊ®°ÂûãÂõæ„ÄÅÁ±ªÂõæ„ÄÅÊó∂Â∫èÂõæÁ≠â„ÄÇ
     - ÁºñÁ†Å / ÊµãËØï„ÄÇ
     - ‰∏äÁ∫ø / Áª¥Êä§„ÄÇ

     ÁÄëÂ∏ÉÊ®°ÂûãÊúÄÂ§ßÁöÑÁº∫ÁÇπÊòØÊó†Ê≥ïÊã•Êä±ÈúÄÊ±ÇÂèòÂåñÔºåÊï¥Â•óÊµÅÁ®ãÁªìÊùüÂêéÊâçËÉΩÁúãÂà∞‰∫ßÂìÅÔºåÂõ¢ÈòüÂ£´Ê∞î‰ΩéËêΩ„ÄÇ
   - ÊïèÊç∑ÂºÄÂèëÔºàScrumÔºâ- ‰∫ßÂìÅÊâÄÊúâËÄÖ„ÄÅScrum Master„ÄÅÁ†îÂèë‰∫∫Âëò - Sprint
     - ‰∫ßÂìÅÁöÑBacklogÔºàÁî®Êà∑ÊïÖ‰∫ã„ÄÅ‰∫ßÂìÅÂéüÂûãÔºâ„ÄÇ
     - ËÆ°Âàí‰ºöËÆÆÔºàËØÑ‰º∞ÂíåÈ¢ÑÁÆóÔºâ„ÄÇ
     - Êó•Â∏∏ÂºÄÂèëÔºàÁ´ôÁ´ã‰ºöËÆÆ„ÄÅÁï™ËåÑÂ∑•‰ΩúÊ≥ï„ÄÅÁªìÂØπÁºñÁ®ã„ÄÅÊµãËØïÂÖàË°å„ÄÅ‰ª£Á†ÅÈáçÊûÑ‚Ä¶‚Ä¶Ôºâ„ÄÇ
     - ‰øÆÂ§çbugÔºàÈóÆÈ¢òÊèèËø∞„ÄÅÈáçÁé∞Ê≠•È™§„ÄÅÊµãËØï‰∫∫Âëò„ÄÅË¢´ÊåáÊ¥æ‰∫∫Ôºâ„ÄÇ
     - ÂèëÂ∏ÉÁâàÊú¨„ÄÇ
     - ËØÑÂÆ°‰ºöËÆÆÔºàShowcaseÔºåÁî®Êà∑ÈúÄË¶ÅÂèÇ‰∏éÔºâ„ÄÇ
     - ÂõûÈ°æ‰ºöËÆÆÔºàÂØπÂΩìÂâçËø≠‰ª£Âë®ÊúüÂÅö‰∏Ä‰∏™ÊÄªÁªìÔºâ„ÄÇ

     > Ë°•ÂÖÖÔºöÊïèÊç∑ËΩØ‰ª∂ÂºÄÂèëÂÆ£Ë®Ä
     >
     > - **‰∏™‰ΩìÂíå‰∫íÂä®** È´ò‰∫é ÊµÅÁ®ãÂíåÂ∑•ÂÖ∑
     > - **Â∑•‰ΩúÁöÑËΩØ‰ª∂** È´ò‰∫é ËØ¶Â∞ΩÁöÑÊñáÊ°£
     > - **ÂÆ¢Êà∑Âêà‰Ωú** È´ò‰∫é ÂêàÂêåË∞àÂà§
     > - **ÂìçÂ∫îÂèòÂåñ** È´ò‰∫é ÈÅµÂæ™ËÆ°Âàí

     ![](./res/agile-scrum-sprint-cycle.png)

     > ËßíËâ≤Ôºö‰∫ßÂìÅÊâÄÊúâËÄÖÔºàÂÜ≥ÂÆöÂÅö‰ªÄ‰πàÔºåËÉΩÂØπÈúÄÊ±ÇÊãçÊùøÁöÑ‰∫∫Ôºâ„ÄÅÂõ¢ÈòüË¥üË¥£‰∫∫ÔºàËß£ÂÜ≥ÂêÑÁßçÈóÆÈ¢òÔºå‰∏ìÊ≥®Â¶Ç‰ΩïÊõ¥Â•ΩÁöÑÂ∑•‰ΩúÔºåÂ±èËîΩÂ§ñÈÉ®ÂØπÂºÄÂèëÂõ¢ÈòüÁöÑÂΩ±ÂìçÔºâ„ÄÅÂºÄÂèëÂõ¢ÈòüÔºàÈ°πÁõÆÊâßË°å‰∫∫ÂëòÔºåÂÖ∑‰ΩìÊåáÂºÄÂèë‰∫∫ÂëòÂíåÊµãËØï‰∫∫ÂëòÔºâ„ÄÇ

     > ÂáÜÂ§áÂ∑•‰ΩúÔºöÂïÜ‰∏öÊ°à‰æãÂíåËµÑÈáë„ÄÅÂêàÂêå„ÄÅÊÜßÊÜ¨„ÄÅÂàùÂßã‰∫ßÂìÅÈúÄÊ±Ç„ÄÅÂàùÂßãÂèëÂ∏ÉËÆ°Âàí„ÄÅÂÖ•ËÇ°„ÄÅÁªÑÂª∫Âõ¢Èòü„ÄÇ

     >  ÊïèÊç∑Âõ¢ÈòüÈÄöÂ∏∏‰∫∫Êï∞‰∏∫8-10‰∫∫„ÄÇ

     >  Â∑•‰ΩúÈáè‰º∞ÁÆóÔºöÂ∞ÜÂºÄÂèë‰ªªÂä°ÈáèÂåñÔºåÂåÖÊã¨ÂéüÂûã„ÄÅLogoËÆæËÆ°„ÄÅUIËÆæËÆ°„ÄÅÂâçÁ´ØÂºÄÂèëÁ≠âÔºåÂ∞ΩÈáèÊääÊØè‰∏™Â∑•‰ΩúÂàÜËß£Âà∞ÊúÄÂ∞è‰ªªÂä°ÈáèÔºåÊúÄÂ∞è‰ªªÂä°ÈáèÊ†áÂáÜ‰∏∫Â∑•‰ΩúÊó∂Èó¥‰∏çËÉΩË∂ÖËøá‰∏§Â§©ÔºåÁÑ∂Âêé‰º∞ÁÆóÊÄª‰ΩìÈ°πÁõÆÊó∂Èó¥„ÄÇÊääÊØè‰∏™‰ªªÂä°ÈÉΩË¥¥Âú®ÁúãÊùø‰∏äÈù¢ÔºåÁúãÊùø‰∏äÂàÜ‰∏âÈÉ®ÂàÜÔºöto doÔºàÂæÖÂÆåÊàêÔºâ„ÄÅin progressÔºàËøõË°å‰∏≠ÔºâÂíådoneÔºàÂ∑≤ÂÆåÊàêÔºâ„ÄÇ

2. È°πÁõÆÂõ¢ÈòüÁªÑÂª∫

   - Âõ¢ÈòüÁöÑÊûÑÊàêÂíåËßíËâ≤

     > ËØ¥ÊòéÔºöË∞¢Ë∞¢**‰ªòÁ••Ëã±**Â•≥Â£´Â∏ÆÂä©ÊàëÁªòÂà∂‰∫Ü‰∏ãÈù¢ËøôÂº†Á≤æÁæéÁöÑÂÖ¨Âè∏ÁªÑÁªáÊû∂ÊûÑÂõæ„ÄÇ

     ![company_architecture](./res/company_architecture.png)

   - ÁºñÁ®ãËßÑËåÉÂíå‰ª£Á†ÅÂÆ°Êü•Ôºà`flake8`„ÄÅ`pylint`Ôºâ

     ![](./res/pylint.png)

   - Python‰∏≠ÁöÑ‰∏Ä‰∫õ‚ÄúÊÉØ‰æã‚ÄùÔºàËØ∑ÂèÇËÄÉ[„ÄäPythonÊÉØ‰æã-Â¶Ç‰ΩïÁºñÂÜôPythonicÁöÑ‰ª£Á†Å„Äã](PythonÊÉØ‰æã.md)Ôºâ

   - ÂΩ±Âìç‰ª£Á†ÅÂèØËØªÊÄßÁöÑÂéüÂõ†Ôºö

     - ‰ª£Á†ÅÊ≥®ÈáäÂ§™Â∞ëÊàñËÄÖÊ≤°ÊúâÊ≥®Èáä
     - ‰ª£Á†ÅÁ†¥Âùè‰∫ÜËØ≠Ë®ÄÁöÑÊúÄ‰Ω≥ÂÆûË∑µ
     - ÂèçÊ®°ÂºèÁºñÁ®ãÔºàÊÑèÂ§ßÂà©Èù¢‰ª£Á†Å„ÄÅÂ§çÂà∂-ÈªèË¥¥ÁºñÁ®ã„ÄÅËá™Ë¥üÁºñÁ®ã„ÄÅ‚Ä¶‚Ä¶Ôºâ

3. Âõ¢ÈòüÂºÄÂèëÂ∑•ÂÖ∑‰ªãÁªç
   - ÁâàÊú¨ÊéßÂà∂ÔºöGit„ÄÅMercury
   - Áº∫Èô∑ÁÆ°ÁêÜÔºö[Gitlab](https://about.gitlab.com/)„ÄÅ[Redmine](http://www.redmine.org.cn/)
   - ÊïèÊç∑Èó≠ÁéØÂ∑•ÂÖ∑Ôºö[Á¶ÖÈÅì](https://www.zentao.net/)„ÄÅ[JIRA](https://www.atlassian.com/software/jira/features)
   - ÊåÅÁª≠ÈõÜÊàêÔºö[Jenkins](https://jenkins.io/)„ÄÅ[Travis-CI](https://travis-ci.org/)

   ËØ∑ÂèÇËÄÉ[„ÄäÂõ¢ÈòüÈ°πÁõÆÂºÄÂèëÁöÑÈóÆÈ¢òÂíåËß£ÂÜ≥ÊñπÊ°à„Äã](Day91-100/91.Âõ¢ÈòüÈ°πÁõÆÂºÄÂèëÁöÑÈóÆÈ¢òÂíåËß£ÂÜ≥ÊñπÊ°à.md)„ÄÇ

##### È°πÁõÆÈÄâÈ¢òÂíåÁêÜËß£‰∏öÂä°

1. ÈÄâÈ¢òËåÉÂõ¥ËÆæÂÆö

   - CMSÔºàÁî®Êà∑Á´ØÔºâÔºöÊñ∞ÈóªËÅöÂêàÁΩëÁ´ô„ÄÅÈóÆÁ≠î/ÂàÜ‰∫´Á§æÂå∫„ÄÅÂΩ±ËØÑ/‰π¶ËØÑÁΩëÁ´ôÁ≠â„ÄÇ
   - MISÔºàÁî®Êà∑Á´Ø+ÁÆ°ÁêÜÁ´ØÔºâÔºöKMS„ÄÅKPIËÄÉÊ†∏Á≥ªÁªü„ÄÅHRS„ÄÅCRMÁ≥ªÁªü„ÄÅ‰æõÂ∫îÈìæÁ≥ªÁªü„ÄÅ‰ªìÂÇ®ÁÆ°ÁêÜÁ≥ªÁªüÁ≠â„ÄÇ

   - AppÂêéÂè∞ÔºàÁÆ°ÁêÜÁ´Ø+Êï∞ÊçÆÊé•Âè£ÔºâÔºö‰∫åÊâã‰∫§ÊòìÁ±ª„ÄÅÊä•ÂàäÊùÇÂøóÁ±ª„ÄÅÂ∞è‰ºóÁîµÂïÜÁ±ª„ÄÅÊñ∞ÈóªËµÑËÆØÁ±ª„ÄÅÊóÖÊ∏∏Á±ª„ÄÅÁ§æ‰∫§Á±ª„ÄÅÈòÖËØªÁ±ªÁ≠â„ÄÇ
   - ÂÖ∂‰ªñÁ±ªÂûãÔºöËá™Ë∫´Ë°å‰∏öËÉåÊôØÂíåÂ∑•‰ΩúÁªèÈ™å„ÄÅ‰∏öÂä°ÂÆπÊòìÁêÜËß£ÂíåÊääÊéß„ÄÇ

2. ÈúÄÊ±ÇÁêÜËß£„ÄÅÊ®°ÂùóÂàíÂàÜÂíå‰ªªÂä°ÂàÜÈÖç

   - ÈúÄÊ±ÇÁêÜËß£ÔºöÂ§¥ËÑëÈ£éÊö¥ÂíåÁ´ûÂìÅÂàÜÊûê„ÄÇ
   - Ê®°ÂùóÂàíÂàÜÔºöÁîªÊÄùÁª¥ÂØºÂõæÔºàXMindÔºâÔºåÊØè‰∏™Ê®°ÂùóÊòØ‰∏Ä‰∏™ÊûùËäÇÁÇπÔºåÊØè‰∏™ÂÖ∑‰ΩìÁöÑÂäüËÉΩÊòØ‰∏Ä‰∏™Âè∂ËäÇÁÇπÔºàÁî®Âä®ËØçË°®Ëø∞ÔºâÔºåÈúÄË¶ÅÁ°Æ‰øùÊØè‰∏™Âè∂ËäÇÁÇπÊó†Ê≥ïÂÜçÁîüÂá∫Êñ∞ËäÇÁÇπÔºåÁ°ÆÂÆöÊØè‰∏™Âè∂Â≠êËäÇÁÇπÁöÑÈáçË¶ÅÊÄß„ÄÅ‰ºòÂÖàÁ∫ßÂíåÂ∑•‰ΩúÈáè„ÄÇ
   - ‰ªªÂä°ÂàÜÈÖçÔºöÁî±È°πÁõÆË¥üË¥£‰∫∫Ê†πÊçÆ‰∏äÈù¢ÁöÑÊåáÊ†á‰∏∫ÊØè‰∏™Âõ¢ÈòüÊàêÂëòÂàÜÈÖç‰ªªÂä°„ÄÇ

   ![](./res/requirements_by_xmind.png)

3. Âà∂ÂÆöÈ°πÁõÆËøõÂ∫¶Ë°®ÔºàÊØèÊó•Êõ¥Êñ∞Ôºâ

   | Ê®°Âùó | ÂäüËÉΩ     | ‰∫∫Âëò   | Áä∂ÊÄÅ     | ÂÆåÊàê | Â∑•Êó∂ | ËÆ°ÂàíÂºÄÂßã | ÂÆûÈôÖÂºÄÂßã | ËÆ°ÂàíÁªìÊùü | ÂÆûÈôÖÁªìÊùü | Â§áÊ≥®             |
   | ---- | -------- | ------ | -------- | ---- | ---- | -------- | -------- | -------- | -------- | ---------------- |
   | ËØÑËÆ∫ | Ê∑ªÂä†ËØÑËÆ∫ | ÁéãÂ§ßÈî§ | Ê≠£Âú®ËøõË°å | 50%  | 4    | 2018/8/7 |          | 2018/8/7 |          |                  |
   |      | Âà†Èô§ËØÑËÆ∫ | ÁéãÂ§ßÈî§ | Á≠âÂæÖ     | 0%   | 2    | 2018/8/7 |          | 2018/8/7 |          |                  |
   |      | Êü•ÁúãËØÑËÆ∫ | ÁôΩÂÖÉËä≥ | Ê≠£Âú®ËøõË°å | 20%  | 4    | 2018/8/7 |          | 2018/8/7 |          | ÈúÄË¶ÅËøõË°å‰ª£Á†ÅÂÆ°Êü• |
   |      | ËØÑËÆ∫ÊäïÁ•® | ÁôΩÂÖÉËä≥ | Á≠âÂæÖ     | 0%   | 4    | 2018/8/8 |          | 2018/8/8 |          |                  |

4. OOADÂíåÊï∞ÊçÆÂ∫ìËÆæËÆ°

  - UMLÔºàÁªü‰∏ÄÂª∫Ê®°ËØ≠Ë®ÄÔºâÁöÑÁ±ªÂõæ

    ![uml](./res/uml-class-diagram.png)

  - ÈÄöËøáÊ®°ÂûãÂàõÂª∫Ë°®ÔºàÊ≠£ÂêëÂ∑•Á®ãÔºâÔºå‰æãÂ¶ÇÂú®DjangoÈ°πÁõÆ‰∏≠ÂèØ‰ª•ÈÄöËøá‰∏ãÈù¢ÁöÑÂëΩ‰ª§ÂàõÂª∫‰∫åÁª¥Ë°®„ÄÇ

    ```Shell
    python manage.py makemigrations app
    python manage.py migrate
    ```

  - ‰ΩøÁî®PowerDesignerÁªòÂà∂Áâ©ÁêÜÊ®°ÂûãÂõæ„ÄÇ

    ![](./res/power-designer-pdm.png)

  - ÈÄöËøáÊï∞ÊçÆË°®ÂàõÂª∫Ê®°ÂûãÔºàÂèçÂêëÂ∑•Á®ãÔºâÔºå‰æãÂ¶ÇÂú®DjangoÈ°πÁõÆ‰∏≠ÂèØ‰ª•ÈÄöËøá‰∏ãÈù¢ÁöÑÂëΩ‰ª§ÁîüÊàêÊ®°Âûã„ÄÇ

    ```Shell
    python manage.py inspectdb > app/models.py
    ```

#### Á¨¨92Â§©Ôºö[DockerÂÆπÂô®ËØ¶Ëß£](./Day91-100/92.DockerÂÆπÂô®ËØ¶Ëß£.md)

1. DockerÁÆÄ‰ªã
2. ÂÆâË£ÖDocker
3. ‰ΩøÁî®DockerÂàõÂª∫ÂÆπÂô®ÔºàNginx„ÄÅMySQL„ÄÅRedis„ÄÅGitlab„ÄÅJenkinsÔºâ
4. ÊûÑÂª∫DockerÈïúÂÉèÔºàDockerfileÁöÑÁºñÂÜôÂíåÁõ∏ÂÖ≥Êåá‰ª§Ôºâ
5. ÂÆπÂô®ÁºñÊéíÔºàDocker-composeÔºâ
6. ÈõÜÁæ§ÁÆ°ÁêÜÔºàKubernetesÔºâ

#### Á¨¨93Â§©Ôºö[MySQLÊÄßËÉΩ‰ºòÂåñ](./Day91-100/93.MySQLÊÄßËÉΩ‰ºòÂåñ.md)

#### Á¨¨94Â§©Ôºö[ÁΩëÁªúAPIÊé•Âè£ËÆæËÆ°](./Day91-100/94.ÁΩëÁªúAPIÊé•Âè£ËÆæËÆ°.md)

#### Á¨¨95Â§©Ôºö[‰ΩøÁî®DjangoÂºÄÂèëÂïÜ‰∏öÈ°πÁõÆ](./Day91-100/95.‰ΩøÁî®DjangoÂºÄÂèëÂïÜ‰∏öÈ°π	ÁõÆ.md)

##### È°πÁõÆÂºÄÂèë‰∏≠ÁöÑÂÖ¨ÂÖ±ÈóÆÈ¢ò

1. Êï∞ÊçÆÂ∫ìÁöÑÈÖçÁΩÆÔºàÂ§öÊï∞ÊçÆÂ∫ì„ÄÅ‰∏ª‰ªéÂ§çÂà∂„ÄÅÊï∞ÊçÆÂ∫ìË∑ØÁî±Ôºâ
2. ÁºìÂ≠òÁöÑÈÖçÁΩÆÔºàÂàÜÂå∫ÁºìÂ≠ò„ÄÅÈîÆËÆæÁΩÆ„ÄÅË∂ÖÊó∂ËÆæÁΩÆ„ÄÅ‰∏ª‰ªéÂ§çÂà∂„ÄÅÊïÖÈöúÊÅ¢Â§çÔºàÂì®ÂÖµÔºâÔºâ
3. Êó•ÂøóÁöÑÈÖçÁΩÆ
4. ÂàÜÊûêÂíåË∞ÉËØïÔºàDjango-Debug-ToolBarÔºâ
5. Â•ΩÁî®ÁöÑPythonÊ®°ÂùóÔºàÊó•ÊúüËÆ°ÁÆó„ÄÅÂõæÂÉèÂ§ÑÁêÜ„ÄÅÊï∞ÊçÆÂä†ÂØÜ„ÄÅ‰∏âÊñπAPIÔºâ

##### REST APIËÆæËÆ°

1. RESTfulÊû∂ÊûÑ
   - [ÁêÜËß£RESTfulÊû∂ÊûÑ](http://www.ruanyifeng.com/blog/2011/09/restful.html)
   - [RESTful APIËÆæËÆ°ÊåáÂçó](http://www.ruanyifeng.com/blog/2014/05/restful_api.html)
   - [RESTful APIÊúÄ‰Ω≥ÂÆûË∑µ](http://www.ruanyifeng.com/blog/2018/10/restful-api-best-practices.html)
2. APIÊé•Âè£ÊñáÊ°£ÁöÑÊí∞ÂÜô
   - [RAP2](http://rap2.taobao.org/)
   - [YAPI](http://yapi.demo.qunar.com/)
3. [django-REST-framework](https://www.django-rest-framework.org/)ÁöÑÂ∫îÁî®

##### È°πÁõÆ‰∏≠ÁöÑÈáçÁÇπÈöæÁÇπÂâñÊûê

1. ‰ΩøÁî®ÁºìÂ≠òÁºìËß£Êï∞ÊçÆÂ∫ìÂéãÂäõ - Redis
2. ‰ΩøÁî®Ê∂àÊÅØÈòüÂàóÂÅöËß£ËÄ¶ÂêàÂíåÂâäÂ≥∞ - Celery + RabbitMQ

#### Á¨¨96Â§©Ôºö[ËΩØ‰ª∂ÊµãËØïÂíåËá™Âä®ÂåñÊµãËØï](Day91-100/96.ËΩØ‰ª∂ÊµãËØïÂíåËá™Âä®ÂåñÊµãËØï.md)

##### ÂçïÂÖÉÊµãËØï

1. ÊµãËØïÁöÑÁßçÁ±ª
2. ÁºñÂÜôÂçïÂÖÉÊµãËØïÔºà`unittest`„ÄÅ`pytest`„ÄÅ`nose2`„ÄÅ`tox`„ÄÅ`ddt`„ÄÅ‚Ä¶‚Ä¶Ôºâ
3. ÊµãËØïË¶ÜÁõñÁéáÔºà`coverage`Ôºâ

##### DjangoÈ°πÁõÆÈÉ®ÁΩ≤

1. ÈÉ®ÁΩ≤ÂâçÁöÑÂáÜÂ§áÂ∑•‰Ωú
   - ÂÖ≥ÈîÆËÆæÁΩÆÔºàSECRET_KEY / DEBUG / ALLOWED_HOSTS / ÁºìÂ≠ò / Êï∞ÊçÆÂ∫ìÔºâ
   - HTTPS / CSRF_COOKIE_SECUR  / SESSION_COOKIE_SECURE  
   - Êó•ÂøóÁõ∏ÂÖ≥ÈÖçÁΩÆ
2. LinuxÂ∏∏Áî®ÂëΩ‰ª§ÂõûÈ°æ
3. LinuxÂ∏∏Áî®ÊúçÂä°ÁöÑÂÆâË£ÖÂíåÈÖçÁΩÆ
4. uWSGI/GunicornÂíåNginxÁöÑ‰ΩøÁî®
   - GunicornÂíåuWSGIÁöÑÊØîËæÉ
     - ÂØπ‰∫é‰∏çÈúÄË¶ÅÂ§ßÈáèÂÆöÂà∂ÂåñÁöÑÁÆÄÂçïÂ∫îÁî®Á®ãÂ∫èÔºåGunicornÊòØ‰∏Ä‰∏™‰∏çÈîôÁöÑÈÄâÊã©ÔºåuWSGIÁöÑÂ≠¶‰π†Êõ≤Á∫øÊØîGunicornË¶ÅÈô°Â≥≠ÂæóÂ§öÔºåGunicornÁöÑÈªòËÆ§ÂèÇÊï∞Â∞±Â∑≤ÁªèËÉΩÂ§üÈÄÇÂ∫îÂ§ßÂ§öÊï∞Â∫îÁî®Á®ãÂ∫è„ÄÇ
     - uWSGIÊîØÊåÅÂºÇÊûÑÈÉ®ÁΩ≤„ÄÇ
     - Áî±‰∫éNginxÊú¨Ë∫´ÊîØÊåÅuWSGIÔºåÂú®Á∫ø‰∏ä‰∏ÄËà¨ÈÉΩÂ∞ÜNginxÂíåuWSGIÊçÜÁªëÂú®‰∏ÄËµ∑ÈÉ®ÁΩ≤ÔºåËÄå‰∏îuWSGIÂ±û‰∫éÂäüËÉΩÈΩêÂÖ®‰∏îÈ´òÂ∫¶ÂÆöÂà∂ÁöÑWSGI‰∏≠Èó¥‰ª∂„ÄÇ
     - Âú®ÊÄßËÉΩ‰∏äÔºåGunicornÂíåuWSGIÂÖ∂ÂÆûË°®Áé∞Áõ∏ÂΩì„ÄÇ
5. ‰ΩøÁî®ËôöÊãüÂåñÊäÄÊúØÔºàDockerÔºâÈÉ®ÁΩ≤ÊµãËØïÁéØÂ¢ÉÂíåÁîü‰∫ßÁéØÂ¢É

##### ÊÄßËÉΩÊµãËØï

1. ABÁöÑ‰ΩøÁî®
2. SQLslapÁöÑ‰ΩøÁî®
3. sysbenchÁöÑ‰ΩøÁî®

##### Ëá™Âä®ÂåñÊµãËØï

1. ‰ΩøÁî®ShellÂíåPythonËøõË°åËá™Âä®ÂåñÊµãËØï
2. ‰ΩøÁî®SeleniumÂÆûÁé∞Ëá™Âä®ÂåñÊµãËØï
   - Selenium IDE
   - Selenium WebDriver
   - Selenium Remote Control
3. ÊµãËØïÂ∑•ÂÖ∑Robot Framework‰ªãÁªç

#### Á¨¨97Â§©Ôºö[ÁîµÂïÜÁΩëÁ´ôÊäÄÊúØË¶ÅÁÇπÂâñÊûê](./Day91-100/97.ÁîµÂïÜÁΩëÁ´ôÊäÄÊúØË¶ÅÁÇπÂâñÊûê.md)

#### Á¨¨98Â§©Ôºö[È°πÁõÆÈÉ®ÁΩ≤‰∏äÁ∫øÂíåÊÄßËÉΩË∞É‰ºò](./Day91-100/98.È°πÁõÆÈÉ®ÁΩ≤‰∏äÁ∫øÂíåÊÄßËÉΩË∞É‰ºò.md)

1. MySQLÊï∞ÊçÆÂ∫ìË∞É‰ºò
2. WebÊúçÂä°Âô®ÊÄßËÉΩ‰ºòÂåñ
   - NginxË¥üËΩΩÂùáË°°ÈÖçÁΩÆ
   - KeepalivedÂÆûÁé∞È´òÂèØÁî®
3. ‰ª£Á†ÅÊÄßËÉΩË∞É‰ºò
   - Â§öÁ∫øÁ®ã
   - ÂºÇÊ≠•Âåñ
4. ÈùôÊÄÅËµÑÊ∫êËÆøÈóÆ‰ºòÂåñ
   - ‰∫ëÂ≠òÂÇ®
   - CDN

#### Á¨¨99Â§©Ôºö[Èù¢ËØï‰∏≠ÁöÑÂÖ¨ÂÖ±ÈóÆÈ¢ò](./Day91-100/99.Èù¢ËØï‰∏≠ÁöÑÂÖ¨ÂÖ±ÈóÆÈ¢ò.md)

#### Á¨¨100Â§©Ôºö[PythonÈù¢ËØïÈ¢òÂÆûÂΩï](./Day91-100/100.PythonÈù¢ËØïÈ¢òÂÆûÂΩï.md)

",,https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/./res/agile-scrum-sprint-cycle.png; https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/./res/company_architecture.png; https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/./res/pylint.png; https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/./res/requirements_by_xmind.png; https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/./res/uml-class-diagram.png; https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/./res/power-designer-pdm.png,,0,https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/./res/agile-scrum-sprint-cycle.png; https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/./res/company_architecture.png; https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/./res/pylint.png; https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/./res/requirements_by_xmind.png; https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/./res/uml-class-diagram.png; https://raw.githubusercontent.com/jackfrued/Python-100-Days/master/./res/power-designer-pdm.png,,
2024-02-25,https://github.com/3DTopia/LGM,https://raw.githubusercontent.com/reflex-dev/reflex/main/README.md,"The text announces the renaming of Pynecone to Reflex and introduces Reflex as a Python library for creating performance-oriented, customizable web applications. It emphasizes ease of deployment and offers a user-friendly approach to web app development in pure Python. Installation instructions are provided, requiring Python 3.8 or higher, alongside a guide for creating a first app using the Reflex command line tool. An example app demonstrates generating a UI for image creation with DALL¬∑E, highlighting Reflex's capabilities in integrating Python code with UI components for dynamic web applications. Reflex is described as being in Public Beta as of July 2023, inviting contributions from the community. Resources such as documentation, blogs, and community forums are mentioned for support and further learning. The text concludes with information about Reflex's open-source nature under the Apache License 2.0.","```diff
+ Searching for Pynecone? You are in the right repo. Pynecone has been renamed to Reflex. +
```

<div align=""center"">
<img src=""https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/reflex_dark.svg#gh-light-mode-only"" alt=""Reflex Logo"" width=""300px"">
<img src=""https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/reflex_light.svg#gh-dark-mode-only"" alt=""Reflex Logo"" width=""300px"">

<hr>

### **‚ú® Performant, customizable web apps in pure Python. Deploy in seconds. ‚ú®**
[![PyPI version](https://badge.fury.io/py/reflex.svg)](https://badge.fury.io/py/reflex)
![tests](https://github.com/pynecone-io/pynecone/actions/workflows/integration.yml/badge.svg)
![versions](https://img.shields.io/pypi/pyversions/reflex.svg)
[![Documentation](https://img.shields.io/badge/Documentation%20-Introduction%20-%20%23007ec6)](https://reflex.dev/docs/getting-started/introduction)
[![Discord](https://img.shields.io/discord/1029853095527727165?color=%237289da&label=Discord)](https://discord.gg/T5WSbC2YtQ)
</div>

---

[English](https://github.com/reflex-dev/reflex/blob/main/README.md) | [ÁÆÄ‰Ωì‰∏≠Êñá](https://github.com/reflex-dev/reflex/blob/main/docs/zh/zh_cn/README.md) | [ÁπÅÈ´î‰∏≠Êñá](https://github.com/reflex-dev/reflex/blob/main/docs/zh/zh_tw/README.md) | [T√ºrk√ße](https://github.com/reflex-dev/reflex/blob/main/docs/tr/README.md) | [‡§π‡§ø‡§Ç‡§¶‡•Ä](https://github.com/reflex-dev/reflex/blob/main/docs/in/README.md) | [Portugu√™s (Brasil)](https://github.com/reflex-dev/reflex/blob/main/docs/pt/pt_br/README.md) | [Italiano](https://github.com/reflex-dev/reflex/blob/main/docs/it/README.md) | [Espa√±ol](https://github.com/reflex-dev/reflex/blob/main/docs/es/README.md) | [ÌïúÍµ≠Ïñ¥](https://github.com/reflex-dev/reflex/blob/main/docs/kr/README.md)

---
## ‚öôÔ∏è Installation

Open a terminal and run (Requires Python 3.8+):

```bash
pip install reflex
```

## ü•≥ Create your first app

Installing `reflex` also installs the `reflex` command line tool.

Test that the install was successful by creating a new project. (Replace `my_app_name` with your project name):

```bash
mkdir my_app_name
cd my_app_name
reflex init
```

This command initializes a template app in your new directory. 

You can run this app in development mode:

```bash
reflex run
```

You should see your app running at http://localhost:3000.

Now you can modify the source code in `my_app_name/my_app_name.py`. Reflex has fast refreshes so you can see your changes instantly when you save your code.


## ü´ß Example App

Let's go over an example: creating an image generation UI around [DALL¬∑E](https://platform.openai.com/docs/guides/images/image-generation?context=node). For simplicity, we just call the [OpenAI API](https://platform.openai.com/docs/api-reference/authentication), but you could replace this with an ML model run locally.

&nbsp

<div align=""center"">
<img src=""https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/dalle.gif"" alt=""A frontend wrapper for DALL¬∑E, shown in the process of generating an image."" width=""550"" />
</div>

&nbsp

Here is the complete code to create this. This is all done in one Python file!

```python
import reflex as rx
import openai

openai_client = openai.OpenAI()


class State(rx.State):
    """"""The app state.""""""

    prompt = """"
    image_url = """"
    processing = False
    complete = False

    def get_image(self):
        """"""Get the image from the prompt.""""""
        if self.prompt == """":
            return rx.window_alert(""Prompt Empty"")

        self.processing, self.complete = True, False
        yield
        response = openai_client.images.generate(
            prompt=self.prompt, n=1, size=""1024x1024""
        )
        self.image_url = response.data[0].url
        self.processing, self.complete = False, True

def index():
    return rx.center(
        rx.vstack(
            rx.heading(""DALL-E"", font_size=""1.5em""),
            rx.input(
                placeholder=""Enter a prompt.."",
                on_blur=State.set_prompt,
                width=""25em"",
            ),
            rx.button(""Generate Image"", on_click=State.get_image, width=""25em""),
            rx.cond(
                State.processing,
                rx.chakra.circular_progress(is_indeterminate=True),
                rx.cond(
                    State.complete,
                    rx.image(src=State.image_url, width=""20em""),
                ),
            ),
            align=""center"",
        ),
        width=""100%"",
        height=""100vh"",
    )


# Add state and page to the app.
app = rx.App()
app.add_page(index, title=""Reflex:DALL-E"")
```

## Let's break this down.

### **Reflex UI**

Let's start with the UI.

```python
def index():
    return rx.center(
        ...
    )
```

This `index` function defines the frontend of the app.

We use different components such as `center`, `vstack`, `input`, and `button` to build the frontend. Components can be nested within each other
to create complex layouts. And you can use keyword args to style them with the full power of CSS.

Reflex comes with [60+ built-in components](https://reflex.dev/docs/library) to help you get started. We are actively adding more components, and it's easy to [create your own components](https://reflex.dev/docs/wrapping-react/overview/).

### **State**

Reflex represents your UI as a function of your state.

```python
class State(rx.State):
    """"""The app state.""""""
    prompt = """"
    image_url = """"
    processing = False
    complete = False

```

The state defines all the variables (called vars) in an app that can change and the functions that change them.

Here the state is comprised of a `prompt` and `image_url`. There are also the booleans `processing` and `complete` to indicate when to show the circular progress and image.

### **Event Handlers**

```python
def get_image(self):
    """"""Get the image from the prompt.""""""
    if self.prompt == """":
        return rx.window_alert(""Prompt Empty"")

    self.processing, self.complete = True, False
    yield
    response = openai_client.images.generate(
        prompt=self.prompt, n=1, size=""1024x1024""
    )
    self.image_url = response.data[0].url
    self.processing, self.complete = False, True
```

Within the state, we define functions called event handlers that change the state vars. Event handlers are the way that we can modify the state in Reflex. They can be called in response to user actions, such as clicking a button or typing in a text box. These actions are called events.

Our DALL¬∑E. app has an event handler, `get_image` to which get this image from the OpenAI API. Using `yield` in the middle of an event handler will cause the UI to update. Otherwise the UI will update at the end of the event handler.

### **Routing**

Finally, we define our app.

```python
app = rx.App()
```

We add a page from the root of the app to the index component. We also add a title that will show up in the page preview/browser tab.

```python
app.add_page(index, title=""DALL-E"")
```

You can create a multi-page app by adding more pages.

## üìë Resources

<div align=""center"">

üìë [Docs](https://reflex.dev/docs/getting-started/introduction) &nbsp |  &nbsp üóûÔ∏è [Blog](https://reflex.dev/blog) &nbsp |  &nbsp üì± [Component Library](https://reflex.dev/docs/library) &nbsp |  &nbsp üñºÔ∏è [Gallery](https://reflex.dev/docs/gallery) &nbsp |  &nbsp üõ∏ [Deployment](https://reflex.dev/docs/hosting/deploy)  &nbsp   

</div>





## ‚úÖ Status

Reflex launched in December 2022 with the name Pynecone.

As of July 2023, we are in the **Public Beta** stage.

-   :white_check_mark: **Public Alpha**: Anyone can install and use Reflex. There may be issues, but we are working to resolve them actively.
-   :large_orange_diamond: **Public Beta**: Stable enough for non-enterprise use-cases.
-   **Public Hosting Beta**: _Optionally_, deploy and host your apps on Reflex!
-   **Public**: Reflex is production ready.

Reflex has new releases and features coming every week! Make sure to :star: star and :eyes: watch this repository to stay up to date.

## Contributing

We welcome contributions of any size! Below are some good ways to get started in the Reflex community.

-   **Join Our Discord**: Our [Discord](https://discord.gg/T5WSbC2YtQ) is the best place to get help on your Reflex project and to discuss how you can contribute.
-   **GitHub Discussions**: A great way to talk about features you want added or things that are confusing/need clarification.
-   **GitHub Issues**: [Issues](https://github.com/reflex-dev/reflex/issues) are an excellent way to report bugs. Additionally, you can try and solve an existing issue and submit a PR.

We are actively looking for contributors, no matter your skill level or experience. To contribute check out [CONTIBUTING.md](https://github.com/reflex-dev/reflex/blob/main/CONTRIBUTING.md)


## All Thanks To Our Contributors:
<a href=""https://github.com/reflex-dev/reflex/graphs/contributors"">
  <img src=""https://contrib.rocks/image?repo=reflex-dev/reflex"" />
</a>

## License

Reflex is open-source and licensed under the [Apache License 2.0](LICENSE).
",,,,0,,,
2024-02-25,https://github.com/reflex-dev/reflex,https://raw.githubusercontent.com/facebookresearch/DiT/main/README.md,"The official PyTorch implementation of the Scalable Diffusion Models with Transformers (DiT) introduces a novel approach to training latent diffusion models by replacing the traditional U-Net architecture with transformers that operate on latent patches, aiming for improved scalability and performance. Analyzing through the forward pass complexity measured by Gflops, the findings suggest that DiTs with higher Gflops consistently achieve lower FID scores, demonstrating good scalability properties. Notably, the DiT-XL/2 models surpass prior diffusion models in class-conditional ImageNet benchmarks, achieving a state-of-the-art FID of 2.27 on the 256x256 benchmark. This repository offers a simple PyTorch implementation, pre-trained class-conditional DiT models, a quick setup for running models locally or through Hugging Face Spaces and Colab, and a training script using PyTorch DDP. Furthermore, the repository provides links for directly downloading pre-trained models, instructions for sampling from these models or custom checkpoints, and methods for training and evaluating DiT models with various configurations and resolutions. Additionally, potential enhancements and differences from JAX are discussed, alongside an acknowledgment section and a note about the license under which the code and models are released.","## Scalable Diffusion Models with Transformers (DiT)<br><sub>Official PyTorch Implementation</sub>

### [Paper](http://arxiv.org/abs/2212.09748) | [Project Page](https://www.wpeebles.com/DiT) | Run DiT-XL/2 [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/wpeebles/DiT) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/facebookresearch/DiT/blob/main/run_DiT.ipynb) <a href=""https://replicate.com/arielreplicate/scalable_diffusion_with_transformers""><img src=""https://replicate.com/arielreplicate/scalable_diffusion_with_transformers/badge""></a>

![DiT samples](visuals/sample_grid_0.png)

This repo contains PyTorch model definitions, pre-trained weights and training/sampling code for our paper exploring 
diffusion models with transformers (DiTs). You can find more visualizations on our [project page](https://www.wpeebles.com/DiT).

> [**Scalable Diffusion Models with Transformers**](https://www.wpeebles.com/DiT)<br>
> [William Peebles](https://www.wpeebles.com), [Saining Xie](https://www.sainingxie.com)
> <br>UC Berkeley, New York University<br>

We train latent diffusion models, replacing the commonly-used U-Net backbone with a transformer that operates on 
latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass 
complexity as measured by Gflops. We find that DiTs with higher Gflops---through increased transformer depth/width or
increased number of input tokens---consistently have lower FID. In addition to good scalability properties, our 
DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512√ó512 and 256√ó256 benchmarks, 
achieving a state-of-the-art FID of 2.27 on the latter.

This repository contains:

* ü™ê A simple PyTorch [implementation](models.py) of DiT
* ‚ö°Ô∏è Pre-trained class-conditional DiT models trained on ImageNet (512x512 and 256x256)
* üí• A self-contained [Hugging Face Space](https://huggingface.co/spaces/wpeebles/DiT) and [Colab notebook](http://colab.research.google.com/github/facebookresearch/DiT/blob/main/run_DiT.ipynb) for running pre-trained DiT-XL/2 models
* üõ∏ A DiT [training script](train.py) using PyTorch DDP

An implementation of DiT directly in Hugging Face `diffusers` can also be found [here](https://github.com/huggingface/diffusers/blob/main/docs/source/en/api/pipelines/dit.mdx).


## Setup

First, download and set up the repo:

```bash
git clone https://github.com/facebookresearch/DiT.git
cd DiT
```

We provide an [`environment.yml`](environment.yml) file that can be used to create a Conda environment. If you only want 
to run pre-trained models locally on CPU, you can remove the `cudatoolkit` and `pytorch-cuda` requirements from the file.

```bash
conda env create -f environment.yml
conda activate DiT
```


## Sampling [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/wpeebles/DiT) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/facebookresearch/DiT/blob/main/run_DiT.ipynb)
![More DiT samples](visuals/sample_grid_1.png)

**Pre-trained DiT checkpoints.** You can sample from our pre-trained DiT models with [`sample.py`](sample.py). Weights for our pre-trained DiT model will be 
automatically downloaded depending on the model you use. The script has various arguments to switch between the 256x256
and 512x512 models, adjust sampling steps, change the classifier-free guidance scale, etc. For example, to sample from
our 512x512 DiT-XL/2 model, you can use:

```bash
python sample.py --image-size 512 --seed 1
```

For convenience, our pre-trained DiT models can be downloaded directly here as well:

| DiT Model     | Image Resolution | FID-50K | Inception Score | Gflops | 
|---------------|------------------|---------|-----------------|--------|
| [XL/2](https://dl.fbaipublicfiles.com/DiT/models/DiT-XL-2-256x256.pt) | 256x256          | 2.27    | 278.24          | 119    |
| [XL/2](https://dl.fbaipublicfiles.com/DiT/models/DiT-XL-2-512x512.pt) | 512x512          | 3.04    | 240.82          | 525    |


**Custom DiT checkpoints.** If you've trained a new DiT model with [`train.py`](train.py) (see [below](#training-dit)), you can add the `--ckpt`
argument to use your own checkpoint instead. For example, to sample from the EMA weights of a custom 
256x256 DiT-L/4 model, run:

```bash
python sample.py --model DiT-L/4 --image-size 256 --ckpt /path/to/model.pt
```


## Training DiT

We provide a training script for DiT in [`train.py`](train.py). This script can be used to train class-conditional 
DiT models, but it can be easily modified to support other types of conditioning. To launch DiT-XL/2 (256x256) training with `N` GPUs on 
one node:

```bash
torchrun --nnodes=1 --nproc_per_node=N train.py --model DiT-XL/2 --data-path /path/to/imagenet/train
```

### PyTorch Training Results

We've trained DiT-XL/2 and DiT-B/4 models from scratch with the PyTorch training script
to verify that it reproduces the original JAX results up to several hundred thousand training iterations. Across our experiments, the PyTorch-trained models give 
similar (and sometimes slightly better) results compared to the JAX-trained models up to reasonable random variation. Some data points:

| DiT Model  | Train Steps | FID-50K<br> (JAX Training) | FID-50K<br> (PyTorch Training) | PyTorch Global Training Seed |
|------------|-------------|----------------------------|--------------------------------|------------------------------|
| XL/2       | 400K        | 19.5                       | **18.1**                       | 42                           |
| B/4        | 400K        | **68.4**                   | 68.9                           | 42                           |
| B/4        | 400K        | 68.4                       | **68.3**                       | 100                          |

These models were trained at 256x256 resolution we used 8x A100s to train XL/2 and 4x A100s to train B/4. Note that FID 
here is computed with 250 DDPM sampling steps, with the `mse` VAE decoder and without guidance (`cfg-scale=1`). 

**TF32 Note (important for A100 users).** When we ran the above tests, TF32 matmuls were disabled per PyTorch's defaults. 
We've enabled them at the top of `train.py` and `sample.py` because it makes training and sampling way way way faster on 
A100s (and should for other Ampere GPUs too), but note that the use of TF32 may lead to some differences compared to 
the above results.

### Enhancements
Training (and sampling) could likely be sped-up significantly by:
- [ ] using [Flash Attention](https://github.com/HazyResearch/flash-attention) in the DiT model
- [ ] using `torch.compile` in PyTorch 2.0

Basic features that would be nice to add:
- [ ] Monitor FID and other metrics
- [ ] Generate and save samples from the EMA model periodically
- [ ] Resume training from a checkpoint
- [ ] AMP/bfloat16 support

**üî• Feature Update** Check out this repository at https://github.com/chuanyangjin/fast-DiT to preview a selection of training speed acceleration and memory saving features including gradient checkpointing, mixed precision training and pre-extrated VAE features. With these advancements, we have achieved a training speed of 0.84 steps/sec for DiT-XL/2 using just a single A100 GPU.

## Evaluation (FID, Inception Score, etc.)

We include a [`sample_ddp.py`](sample_ddp.py) script which samples a large number of images from a DiT model in parallel. This script 
generates a folder of samples as well as a `.npz` file which can be directly used with [ADM's TensorFlow
evaluation suite](https://github.com/openai/guided-diffusion/tree/main/evaluations) to compute FID, Inception Score and
other metrics. For example, to sample 50K images from our pre-trained DiT-XL/2 model over `N` GPUs, run:

```bash
torchrun --nnodes=1 --nproc_per_node=N sample_ddp.py --model DiT-XL/2 --num-fid-samples 50000
```

There are several additional options see [`sample_ddp.py`](sample_ddp.py) for details. 


## Differences from JAX

Our models were originally trained in JAX on TPUs. The weights in this repo are ported directly from the JAX models. 
There may be minor differences in results stemming from sampling with different floating point precisions. We re-evaluated 
our ported PyTorch weights at FP32, and they actually perform marginally better than sampling in JAX (2.21 FID 
versus 2.27 in the paper).


## BibTeX

```bibtex
@article{Peebles2022DiT,
  title={Scalable Diffusion Models with Transformers},
  author={William Peebles and Saining Xie},
  year={2022},
  journal={arXiv preprint arXiv:2212.09748},
}
```


## Acknowledgments
We thank Kaiming He, Ronghang Hu, Alexander Berg, Shoubhik Debnath, Tim Brooks, Ilija Radosavovic and Tete Xiao for helpful discussions. 
William Peebles is supported by the NSF Graduate Research Fellowship.

This codebase borrows from OpenAI's diffusion repos, most notably [ADM](https://github.com/openai/guided-diffusion).


## License
The code and model weights are licensed under CC-BY-NC. See [`LICENSE.txt`](LICENSE.txt) for details.
",,https://raw.githubusercontent.com/facebookresearch/DiT/main/visuals/sample_grid_0.png; https://raw.githubusercontent.com/facebookresearch/DiT/main/visuals/sample_grid_1.png,,0,https://raw.githubusercontent.com/facebookresearch/DiT/main/visuals/sample_grid_0.png; https://raw.githubusercontent.com/facebookresearch/DiT/main/visuals/sample_grid_1.png,,
2024-02-25,https://github.com/facebookresearch/DiT,https://raw.githubusercontent.com/microsoft/UFO/main/README.md,"UFO is a groundbreaking UI-Focused dual-agent framework for Windows OS, facilitating seamless interaction across one or more applications to fulfill user requests. It consists of two main components: the AppAgent, which selects and switches between applications as needed to complete tasks, and the ActAgent, which executes actions within these applications. These agents use GPT-Vision for understanding and interacting with applications, specifically targeting controls compatible with the Windows UI Automation API. UFO is notable for being the first agent to translate natural language requests into actions on the Windows OS, enabling complex tasks to be performed more efficiently. It ensures security through action safeguards and offers easy extensibility for additional functionalities. UFO is available on GitHub and requires Python 3.10 or higher. It emphasizes user privacy and data handling practices in its disclaimer.","<h1 align=""center"">
    <b>UFO</b> <img src=""./assets/ufo_blue.png"" alt=""UFO Image"" width=""40"">: A <b>U</b>I-<b>F</b>ocused Agent for Windows <b>O</b>S Interaction
</h1>


<div align=""center"">

[![arxiv](https://img.shields.io/badge/Paper-arXiv:202402.07939-b31b1b.svg)](https://arxiv.org/abs/2402.07939)&ensp
![Python Version](https://img.shields.io/badge/Python-3776AB?&logo=python&logoColor=white-blue&label=3.10%20%7C%203.11)&ensp
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)&ensp
![Welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)&ensp
[![X (formerly Twitter) Follow](https://img.shields.io/twitter/follow/UFO_Agent)](https://twitter.com/intent/follow?screen_name=UFO_Agent)

</div>

**UFO** is a **UI-Focused** dual-agent framework to fulfill user requests on **Windows OS** by seamlessly navigating and operating within individual or spanning multiple applications.

<h1 align=""center"">
    <img src=""./assets/overview_n.png""/> 
</h1>


## üïå Framework
<b>UFO</b> <img src=""./assets/ufo_blue.png"" alt=""UFO Image"" width=""24""> operates as a dual-agent framework, encompassing:
- <b>AppAgent ü§ñ</b>, tasked with choosing an application for fulfilling user requests. This agent may also switch to a different application when a request spans multiple applications, and the task is partially completed in the preceding application. 
- <b>ActAgent üëæ</b>, responsible for iteratively executing actions on the selected applications until the task is successfully concluded within a specific application. 
- <b>Control Interaction üéÆ</b>, is tasked with translating actions from AppAgent and ActAgent into interactions with the application and its UI controls. It's essential that the targeted controls are compatible with the Windows **UI Automation** API.

Both agents leverage the multi-modal capabilities of GPT-Vision to comprehend the application UI and fulfill the user's request. For more details, please consult our [technical report](https://arxiv.org/abs/2402.07939).
<h1 align=""center"">
    <img src=""./assets/framework.png""/> 
</h1>


## üì¢ News
- üìÖ 2024-02-14: Our [technical report](https://arxiv.org/abs/2402.07939) is online!
- üìÖ 2024-02-10: UFO is released on GitHubüéà. Happy Chinese New yearüêâ!



## üí• Highlights

- [x] **First Windows Agent** - UFO is the pioneering agent framework capable of translating user requests in natural language into actionable operations on Windows OS.
- [x] **Interactive Mode** - UFO facilitates multiple sub-requests from users within the same session, enabling the completion of complex tasks seamlessly.
- [x] **Action Safeguard** - UFO incorporates safeguards to prompt user confirmation for sensitive actions, enhancing security and preventing inadvertent operations.
- [x] **Easy Extension** - UFO offers extensibility, allowing for the integration of additional functionalities and control types to tackle diverse and intricate tasks with ease.



## ‚ú® Getting Started


### üõ†Ô∏è Step 1: Installation
UFO requires **Python >= 3.10** running on **Windows OS >= 10**. It can be installed by running the following command:
```bash
# [optional to create conda environment]
# conda create -n ufo python=3.10
# conda activate ufo

# clone the repository
git clone https://github.com/microsoft/UFO.git
cd UFO
# install the requirements
pip install -r requirements.txt
```

### ‚öôÔ∏è Step 2: Configure the LLMs
Before running UFO, you need to provide your LLM configurations. You can configure `ufo/config/config.yaml` file as follows. 

#### OpenAI
```
API_TYPE: ""openai"" 
OPENAI_API_BASE: ""https://api.openai.com/v1/chat/completions"" # The base URL for the OpenAI API
OPENAI_API_KEY: ""YOUR_API_KEY""  # Set the value to the openai key for the llm model
OPENAI_API_MODEL: ""GPTV_MODEL_NAME""  # The only OpenAI model by now that accepts visual input
```

#### Azure OpenAI (AOAI)
```
API_TYPE: ""aoai"" 
OPENAI_API_BASE: ""YOUR_ENDPOINT"" # The AOAI API address. Format: https://{your-resource-name}.openai.azure.com/openai/deployments/{deployment-id}/chat/completions?api-version={api-version}
OPENAI_API_KEY: ""YOUR_API_KEY""  # Set the value to the openai key for the llm model
OPENAI_API_MODEL: ""GPTV_MODEL_NAME""  # The only OpenAI model by now that accepts visual input
```


### üéâ Step 3: Start UFO

#### ‚å®Ô∏è You can execute the following on your Windows command Line (CLI):

```bash
# assume you are in the cloned UFO folder
python -m ufo --task <your_task_name>
```

This will start the UFO process and you can interact with it through the command line interface. 
If everything goes well, you will see the following message:

```bash
Welcome to use UFOüõ∏, A UI-focused Agent for Windows OS Interaction. 
 _   _  _____   ___
| | | ||  ___| / _ \
| | | || |_   | | | |
| |_| ||  _|  | |_| |
 \___/ |_|     \___/
Please enter your request to be completedüõ∏:
```
#### ‚ö†Ô∏èReminder:  ####
- Before UFO executing your request, please make sure the targeted applications are active on the system.
- The GPT-V accepts screenshots of your desktop and application GUI as input. Please ensure that no sensitive or confidential information is visible or captured during the execution process. For further information, refer to [DISCLAIMER.md](./DISCLAIMER.md).


###  Step 4 üé•: Execution Logs 

You can find the screenshots taken and request & response logs in the following folder:
```
./ufo/logs/<your_task_name>/
```
You may use them to debug, replay, or analyze the agent output.


## ‚ùìGet help 
* ‚ùîGitHub Issues (prefered)
* For other communications, please contact ufo-agent@microsoft.com
---

## üé¨ Demo Examples

We present two demo videos that complete user request on Windows OS using UFO. For more case study, please consult our [technical report](https://arxiv.org/abs/2402.07939).

#### 1Ô∏è‚É£üóëÔ∏è Example 1: Deleting all notes on a PowerPoint presentation.
In this example, we will demonstrate how to efficiently use UFO to delete all notes on a PowerPoint presentation with just a few simple steps. Explore this functionality to enhance your productivity and work smarter, not harder!


https://github.com/microsoft/UFO/assets/11352048/cf60c643-04f7-4180-9a55-5fb240627834



#### 2Ô∏è‚É£üìß Example 2: Composing an email using text from multiple sources.
In this example, we will demonstrate how to utilize UFO to extract text from Word documents, describe an image, compose an email, and send it seamlessly. Enjoy the versatility and efficiency of cross-application experiences with UFO!


https://github.com/microsoft/UFO/assets/11352048/aa41ad47-fae7-4334-8e0b-ba71c4fc32e0




## üìä Evaluation

Please consult the [WindowsBench](https://arxiv.org/pdf/2402.07939.pdf) provided in Section A of the Appendix within our technical report. Here are some tips (and requirements) to aid in completing your request:

- Prior to UFO execution of your request, ensure that the targeted application is active (though it may be minimized).
- Occasionally, requests to GPT-V may trigger content safety measures. UFO will attempt to retry regardless, but adjusting the size or scale of the application window may prove helpful. We are actively solving this issue.
- Currently, UFO supports a limited set of applications and UI controls that are compatible with the Windows **UI Automation** API. Our future plans include extending support to the Win32 API to enhance its capabilities.
- Please note that the output of GPT-V may not consistently align with the same request. If unsuccessful with your initial attempt, consider trying again.



## üìö Citation
Our technical report paper can be found [here](https://arxiv.org/abs/2402.07939). 
If you use UFO in your research, please cite our paper:
```
@article{ufo,
  title={{UFO: A UI-Focused Agent for Windows OS Interaction}},
  author={Zhang, Chaoyun and Li, Liqun and He, Shilin and  Zhang, Xu and Qiao, Bo and  Qin, Si and Ma, Minghua and Kang, Yu and Lin, Qingwei and Rajmohan, Saravan and Zhang, Dongmei and  Zhang, Qi},
  journal={arXiv preprint arXiv:2402.07939},
  year={2024}
}
```

## üìù Todo List
- ‚è© Documentation.
- ‚è© Support local host GUI interaction model.
- ‚è© Support more control using Win32 API.
- ‚è© RAG enhanced UFO.
- ‚è© Chatbox GUI for UFO.



## üé® Related Project
You may also find [TaskWeaver](https://github.com/microsoft/TaskWeaver?tab=readme-ov-file) useful, a code-first LLM agent framework for seamlessly planning and executing data analytics tasks.


## ‚ö†Ô∏è Disclaimer
By choosing to run the provided code, you acknowledge and agree to the following terms and conditions regarding the functionality and data handling practices in [DISCLAIMER.md](./DISCLAIMER.md)


## <img src=""./assets/ufo_blue.png"" alt=""logo"" width=""30""> Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft 
trademarks or logos is subject to and must follow 
[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.
",,,,0,,,
2024-02-25,https://github.com/microsoft/UFO,https://raw.githubusercontent.com/facebookresearch/jepa/main/README.md,"The V-JEPA (Video Joint Embedding Predictive Architecture) is a self-supervised learning method developed for creating visual representations from video data, presented by the Meta AI Research team. It leverages unsupervised feature prediction, requiring no human annotations, pretrained image encoders, or pixel-level reconstruction, solely relying on video pixels from the VideoMix2M dataset. This method trains models to generate versatile visual representations effective in various video and image tasks using a frozen backbone architecture and a lightweight, task-specific attentive probe, without necessitating model parameter adjustments. V-JEPA differentiates itself by predicting in latent space, avoiding the need for pixel-level generation. The architecture includes pretrained models and attentive probes designed for accuracy across different resolutions on datasets like K400, SSv2, ImageNet1K, Places205, and iNat21. The official PyTorch codebase provides comprehensive instructions for data preparation, model training (locally and distributed), and evaluations, aiming to facilitate broad usage and further experimentation in the field.","# V-JEPA: Video Joint Embedding Predictive Architecture

Official PyTorch codebase for the _video joint-embedding predictive architecture_, V-JEPA, a method for self-supervised learning of visual representations from video.

**[Meta AI Research, FAIR](https://ai.facebook.com/research/)**

Adrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mahmoud Assran*, Nicolas Ballas*

[\[Blog\]](https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/)
[\[Paper\]](https://ai.meta.com/research/publications/revisiting-feature-prediction-for-learning-visual-representations-from-video/)
[\[Yannic Kilcher's Video\]](https://www.youtube.com/watch?v=7UkJPwz_N_0)

V-JEPA models are trained by passively watching video pixels from the VideoMix2M dataset, and produce versatile visual representations that perform well on downstream video and image tasks, without adaption of the model‚Äôs parameters e.g., using a frozen backbone and only a light-weight task-specific attentive probe.

## Method
V-JEPA pretraining is based solely on an unsupervised feature prediction objective, and does not utilize pretrained image encoders, text, negative examples, human annotations, or pixel-level reconstruction.


<img src=""https://github.com/facebookresearch/jepa/assets/7530871/72df7ef0-2ef5-48bb-be46-27963db91f3d"" width=40%>
&emsp&emsp&emsp&emsp&emsp
<img src=""https://github.com/facebookresearch/jepa/assets/7530871/f26b2e96-0227-44e2-b058-37e7bf1e10db"" width=40%>



## Visualizations
As opposed to generative methods that have a pixel decoder, V-JEPA has a predictor that makes predictions in latent space.
We train a conditional diffusion model to decode the V-JEPA feature-space predictions to interpretable pixels the pretrained V-JEPA encoder and predictor networks are kept frozen in this process.
The decoder is only fed the representations predicted for the missing regions of the video, and does not have access to the unmasked regions of the video.

The V-JEPA feature predictions are indeed grounded, and exhibit spatio-temporal consistency with the unmasked regions of the video.

<img src=""https://github.com/facebookresearch/jepa/assets/7530871/8bb5e338-0db8-4532-ba6f-fc62729acc26"" width=90%>
<br/>
<img src=""https://github.com/facebookresearch/jepa/assets/7530871/93e15a3b-9119-4149-ac88-4e6288f2043d"" width=22%>
<img src=""https://github.com/facebookresearch/jepa/assets/7530871/7efd2ee2-2aa0-4065-a4a6-12f1d9d0499c"" width=22%>
<img src=""https://github.com/facebookresearch/jepa/assets/7530871/06626018-cd5a-4536-9d0e-de58506ce5ed"" width=22%>
<img src=""https://github.com/facebookresearch/jepa/assets/7530871/766da53a-e6b8-4f94-82c8-9a53b4764358"" width=22%>
<br/>

## MODEL ZOO

#### Pretrained models

<table>
  <tr>
    <th colspan=""1"">model</th>
    <th colspan=""1"">patch size</th>
    <th colspan=""1"">resolution</th>
    <th colspan=""1"">iterations</th>
    <th colspan=""1"">batch size</th>
    <th colspan=""1"">data</th>
    <th colspan=""2"">download</th>
  </tr>
  <tr>
    <td>ViT-L</td>
    <td>2x16x16</td>
    <td>224x224</td>
    <td>90K</td>
    <td>3072</td>
    <td>VideoMix2M</td>
    <td><a href=""https://dl.fbaipublicfiles.com/jepa/vitl16/vitl16.pth.tar"">checkpoint</a></td>
    <td><a href=""https://github.com/facebookresearch/jepa/blob/master/configs/pretrain/vitl16.yaml"">configs</a></td>
  </tr>
  <tr>
    <td>ViT-H</td>
    <td>2x16x16</td>
    <td>224x224</td>
    <td>90K</td>
    <td>3072</td>
    <td>VideoMix2M</td>
    <td><a href=""https://dl.fbaipublicfiles.com/jepa/vith16/vith16.pth.tar"">checkpoint</a></td>
    <td><a href=""https://github.com/facebookresearch/jepa/blob/master/configs/pretrain/vith16.yaml"">configs</a></td>
  </tr>
  <tr>
    <td>ViT-H</td>
    <td>2x16x16</td>
    <td>384x384</td>
    <td>90K</td>
    <td>2400</td>
    <td>VideoMix2M</td>
    <td><a href=""https://dl.fbaipublicfiles.com/jepa/vith16-384/vith16-384.pth.tar"">checkpoint</a></td>
    <td><a href=""https://github.com/facebookresearch/jepa/blob/master/configs/pretrain/vith16_384.yaml"">configs</a></td>
  </tr>
</table>

#### K400 Attentive probes

<table>
  <tr>
    <th colspan=""1"">model</th>
    <th colspan=""1"">resolution</th>
    <th colspan=""1"">accuracy (16x8x3)</th>
    <th colspan=""2"">download</th>
  </tr>
  <tr>
    <td>ViT-L/16</td>
    <td>224x224</td>
    <td>80.8</td>
    <td><a href=""https://dl.fbaipublicfiles.com/jepa/vitl16/k400-probe.pth.tar"">attentive probe checkpoint</a></td>
    <td><a href=""https://github.com/facebookresearch/jepa/blob/master/configs/evals/vitl16_k400_16x8x3.yaml"">configs</a></td>
  </tr>
  <tr>
    <td>ViT-H/16</td>
    <td>224x224</td>
    <td>82.0</td>
    <td><a href=""https://dl.fbaipublicfiles.com/jepa/vith16/k400-probe.pth.tar"">attentive probe checkpoint</a></td>
    <td><a href=""https://github.com/facebookresearch/jepa/blob/master/configs/evals/vith16_k400_16x8x3.yaml"">configs</a></td>
  </tr>
  <tr>
    <td>ViT-H/16</td>
    <td>384x384</td>
    <td>81.9</td>
    <td><a href=""https://dl.fbaipublicfiles.com/jepa/vith16-384/k400-probe.pth.tar"">attentive probe checkpoint</a></td>
    <td><a href=""https://github.com/facebookresearch/jepa/blob/master/configs/evals/vith16_384_k400_16x8x3.yaml"">configs</a></td>
  </tr>
</table>

#### SSv2 Attentive probes

<table>
  <tr>
    <th colspan=""1"">model</th>
    <th colspan=""1"">resolution</th>
    <th colspan=""1"">accuracy (16x2x3)</th>
    <th colspan=""2"">download</th>
  </tr>
  <tr>
    <td>ViT-L/16</td>
    <td>224x224</td>
    <td>69.5</td>
    <td><a href=""https://dl.fbaipublicfiles.com/jepa/vitl16/ssv2-probe.pth.tar"">attentive probe checkpoint</a></td>
    <td><a href=""https://github.com/facebookresearch/jepa/blob/master/configs/evals/vitl16_ssv2_16x2x3.yaml"">configs</a></td>
  </tr>
  <tr>
    <td>ViT-H/16</td>
    <td>224x224</td>
    <td>71.4</td>
    <td><a href=""https://dl.fbaipublicfiles.com/jepa/vith16/ssv2-probe.pth.tar"">attentive probe checkpoint</a></td>
    <td><a href=""https://github.com/facebookresearch/jepa/blob/master/configs/evals/vith16_ssv2_16x2x3.yaml"">configs</a></td>
  </tr>
  <tr>
    <td>ViT-H/16</td>
    <td>384x384</td>
    <td>72.2</td>
    <td><a href=""https://dl.fbaipublicfiles.com/jepa/vith16-384/ssv2-probe.pth.tar"">attentive probe checkpoint</a></td>
    <td><a href=""https://github.com/facebookresearch/jepa/blob/master/configs/evals/vith16_384_ssv2_16x2x3.yaml"">configs</a></td>
  </tr>
</table>

#### ImageNet1K Attentive probes

<table>
  <tr>
    <th colspan=""1"">model</th>
    <th colspan=""1"">resolution</th>
    <th colspan=""1"">accuracy</th>
    <th colspan=""2"">download</th>
  </tr>
  <tr>
    <td>ViT-L/16</td>
    <td>224x224</td>
    <td>74.8</td>
    <td><a href=""https://dl.fbaipublicfiles.com/jepa/vitl16/in1k-probe.pth.tar"">attentive probe checkpoint</a></td>
    <td><a href=""https://github.com/facebookresearch/jepa/blob/master/configs/evals/vitl16_in1k.yaml"">configs</a></td>
  </tr>
  <tr>
    <td>ViT-H/16</td>
    <td>224x224</td>
    <td>75.9</td>
    <td><a href=""https://dl.fbaipublicfiles.com/jepa/vith16/in1k-probe.pth.tar"">attentive probe checkpoint</a></td>
    <td><a href=""https://github.com/facebookresearch/jepa/blob/master/configs/evals/vith16_in1k.yaml"">configs</a></td>
  </tr>
  <tr>
    <td>ViT-H/16</td>
    <td>384x384</td>
    <td>77.4</td>
    <td><a href=""https://dl.fbaipublicfiles.com/jepa/vith16-384/in1k-probe.pth.tar"">attentive probe checkpoint</a></td>
    <td><a href=""https://github.com/facebookresearch/jepa/blob/master/configs/evals/vith16_384_in1k.yaml"">configs</a></td>
  </tr>
</table>

#### Places205 Attentive probes

<table>
  <tr>
    <th colspan=""1"">model</th>
    <th colspan=""1"">resolution</th>
    <th colspan=""1"">accuracy</th>
    <th colspan=""2"">download</th>
  </tr>
  <tr>
    <td>ViT-L/16</td>
    <td>224x224</td>
    <td>60.3</td>
    <td><a href=""https://dl.fbaipublicfiles.com/jepa/vitl16/places-probe.pth.tar"">attentive probe checkpoint</a></td>
    <td><a href=""https://github.com/facebookresearch/jepa/blob/master/configs/evals/vitl16_places.yaml"">configs</a></td>
  </tr>
  <tr>
    <td>ViT-H/16</td>
    <td>224x224</td>
    <td>61.7</td>
    <td><a href=""https://dl.fbaipublicfiles.com/jepa/vith16/places-probe.pth.tar"">attentive probe checkpoint</a></td>
    <td><a href=""https://github.com/facebookresearch/jepa/blob/master/configs/evals/vith16_places.yaml"">configs</a></td>
  </tr>
  <tr>
    <td>ViT-H/16</td>
    <td>384x384</td>
    <td>62.8</td>
    <td><a href=""https://dl.fbaipublicfiles.com/jepa/vith16-384/places-probe.pth.tar"">attentive probe checkpoint</a></td>
    <td><a href=""https://github.com/facebookresearch/jepa/blob/master/configs/evals/vith16_384_places.yaml"">configs</a></td>
  </tr>
</table>

#### iNat21 Attentive probes

<table>
  <tr>
    <th colspan=""1"">model</th>
    <th colspan=""1"">resolution</th>
    <th colspan=""1"">accuracy</th>
    <th colspan=""2"">download</th>
  </tr>
  <tr>
    <td>ViT-L/16</td>
    <td>224x224</td>
    <td>67.8</td>
    <td><a href=""https://dl.fbaipublicfiles.com/jepa/vitl16/inat-probe.pth.tar"">attentive probe checkpoint</a></td>
    <td><a href=""https://github.com/facebookresearch/jepa/blob/master/configs/evals/vitl16_inat.yaml"">configs</a></td>
  </tr>
  <tr>
    <td>ViT-H/16</td>
    <td>224x224</td>
    <td>67.9</td>
    <td><a href=""https://dl.fbaipublicfiles.com/jepa/vith16/inat-probe.pth.tar"">attentive probe checkpoint</a></td>
    <td><a href=""https://github.com/facebookresearch/jepa/blob/master/configs/evals/vith16_inat.yaml"">configs</a></td>
  </tr>
  <tr>
    <td>ViT-H/16</td>
    <td>384x384</td>
    <td>72.6</td>
    <td><a href=""https://dl.fbaipublicfiles.com/jepa/vith16-384/inat-probe.pth.tar"">attentive probe checkpoint</a></td>
    <td><a href=""https://github.com/facebookresearch/jepa/blob/master/configs/evals/vith16_384_inat.yaml"">configs</a></td>
  </tr>
</table>

## Code Structure

**Config files:**
All experiment parameters are specified in config files (as opposed to command-line arguments). See the [configs/](configs/) directory for example config files. Note, before launching an experiment, you must update the paths in the config file to point to your own directories, indicating where to save the logs and checkpoints and where to find the training data.


```
.
‚îú‚îÄ‚îÄ app                       # the only place where training loops are allowed
‚îÇ   ‚îú‚îÄ‚îÄ vjepa                 #   Video JEPA pre-training
‚îÇ   ‚îú‚îÄ‚îÄ main_distributed.py   #   entrypoint for launching app on slurm cluster
‚îÇ   ‚îî‚îÄ‚îÄ main.py               #   entrypoint for launching app locally on your machine for debugging
‚îú‚îÄ‚îÄ evals                     # the only place where evaluation of 'apps' are allowed
‚îÇ   ‚îú‚îÄ‚îÄ image_classification  #   training an attentive probe for image classification with frozen backbone
‚îÇ   ‚îú‚îÄ‚îÄ video_classification  #   training an attentive probe for video classification with frozen backbone
‚îÇ   ‚îú‚îÄ‚îÄ main_distributed.py   #   entrypoint for launching distributed evaluations on slurm cluster
‚îÇ   ‚îî‚îÄ‚îÄ main.py               #   entrypoint for launching evaluations locally on your machine for debugging
‚îú‚îÄ‚îÄ src                       # the package
‚îÇ   ‚îú‚îÄ‚îÄ datasets              #   datasets, data loaders, ...
‚îÇ   ‚îú‚îÄ‚îÄ models                #   model definitions
‚îÇ   ‚îú‚îÄ‚îÄ masks                 #   mask collators, masking utilities, ...
‚îÇ   ‚îî‚îÄ‚îÄ utils                 #   shared utilities
‚îî‚îÄ‚îÄ configs                   # the only place where config files are allowed (specify experiment params for app/eval runs)
    ‚îú‚îÄ‚îÄ evals                 #   configs for launching vjepa frozen evaluations
    ‚îî‚îÄ‚îÄ pretrain              #   configs for launching vjepa pretraining

```

## Data preparation

### Video Datasets
V-JEPA pretraining and evaluations work with many standard video formats.
To make a video dataset compatible with the V-JEPA codebase, you simply need to create a `.csv` file with the following format and then specify the path to this CSV file in your config.
```
/absolute_file_path.[mp4, webvid, etc.] $integer_class_label
/absolute_file_path.[mp4, webvid, etc.] $integer_class_label
/absolute_file_path.[mp4, webvid, etc.] $integer_class_label
...
```
Since V-JEPA is entirely unsupervised, the pretraining code will disregard the `$integer_class_label` in the CSV file.
Thus, feel free to put a random value in this column.
However, if you wish to run a supervised video classification evaluation on your video dataset, you must replace ```$integer_class_label``` with the ground truth label for each video.

### Image Datasets
We use the standard PyTorch ```ImageFolder``` class in our image classification evals.
Thus, to set up an image dataset for the image classification evaluation, first create a directory to store your image datasets ```$your_directory_containing_image_datasets```.
Next, download your image datasets into this directory in a format compatible with [PyTorch ImageFolder](https://pytorch.org/vision/main/generated/torchvision.datasets.ImageFolder.html).

For example, suppose we have a directory called ``my_image_datasets``. We would then download our image datasets into this directory so that we end up with the following file tree
```
.
‚îî‚îÄ‚îÄ /my_image_datasets/                # where we store image datasets
    ‚îú‚îÄ‚îÄ places205/121517/pytorch/      #   Places205
    ‚îÇ   ‚îî‚îÄ‚îÄ [...]
    ‚îú‚îÄ‚îÄ iNaturalist-2021/110421/       #   iNaturalist21
    ‚îÇ   ‚îî‚îÄ‚îÄ [...]
    ‚îú‚îÄ‚îÄ [...]                          #   Other Image Datasets
    ‚îÇ   ‚îî‚îÄ‚îÄ [...]
    ‚îî‚îÄ‚îÄ imagenet_full_size/061417/     #   ImageNet1k
        ‚îî‚îÄ‚îÄ train
        ‚îÇ   ‚îú‚îÄ‚îÄ $class_1
        ‚îÇ   ‚îÇ    ‚îú‚îÄ‚îÄ xxx.[png, jpeg, etc.]
        ‚îÇ   ‚îÇ    ‚îú‚îÄ‚îÄ [...]
        ‚îÇ   ‚îÇ    ‚îî‚îÄ‚îÄ xxz.[png, jpeg, etc.]
        ‚îÇ   ‚îú‚îÄ‚îÄ [...]
        ‚îÇ   ‚îî‚îÄ‚îÄ $class_n
        ‚îÇ       ‚îú‚îÄ‚îÄ abc.[png, jpeg, etc.]
        ‚îÇ       ‚îú‚îÄ‚îÄ [...]
        ‚îÇ       ‚îî‚îÄ‚îÄ abz.[png, jpeg, etc.]
        ‚îî‚îÄ‚îÄ val
            ‚îú‚îÄ‚îÄ $class_1
            ‚îÇ    ‚îú‚îÄ‚îÄ xxx.[png, jpeg, etc.]
            ‚îÇ    ‚îú‚îÄ‚îÄ [...]
            ‚îÇ    ‚îî‚îÄ‚îÄ xxz.[png, jpeg, etc.]
            ‚îú‚îÄ‚îÄ [...]
            ‚îî‚îÄ‚îÄ $class_n
                ‚îú‚îÄ‚îÄ abc.[png, jpeg, etc.]
                ‚îú‚îÄ‚îÄ [...]
                ‚îî‚îÄ‚îÄ abz.[png, jpeg, etc.]
```


## Launching V-JEPA pretraining

### Local training
If you wish to debug your code or setup before launching a distributed training run, we provide the functionality to do so by running the pretraining script locally on a multi-GPU (or single-GPU) machine, however, reproducing our results requires launching distributed training.

The single-machine implementation starts from the [app/main.py](appmain.py), which parses the experiment config file and runs the pretraining locally on a multi-GPU (or single-GPU) machine.
For example, to run V-JEPA pretraining on GPUs ""0"", ""1"", and ""2"" on a local machine using the config [configs/pretrain/vitl16.yaml](configs/pretrain/vitl16.yaml), type the command:
```bash
python -m app.main \
  --fname configs/pretrain/vitl16.yaml \
  --devices cuda:0 cuda:1 cuda:2
```

### Distributed training
To launch a distributed training run, the implementation starts from [app/main_distributed.py](app/main_distributed.py), which, in addition to parsing the config file, also allows for specifying details about distributed training. For distributed training, we use the popular open-source [submitit](https://github.com/facebookincubator/submitit) tool and provide examples for a SLURM cluster.

For example, to launch a distributed pre-training experiment using the config [configs/pretrain/vitl16.yaml](configs/pretrain/vitl16.yaml), type the command:
```bash
python -m app.main_distributed \
  --fname configs/pretrain/vitl16.yaml \
  --folder $path_to_save_stderr_and_stdout \
  --partition $slurm_partition
```

## Launching Evaluations

### Local training
If you wish to debug your eval code or setup before launching a distributed training run, we provide the functionality to do so by running the pretraining script locally on a multi-GPU (or single-GPU) machine, however, reproducing the full eval would require launching distributed training.
The single-machine implementation starts from the [eval/main.py](eval/main.py), which parses the experiment config file and runs the eval locally on a multi-GPU (or single-GPU) machine.

For example, to run ImageNet image classification on GPUs ""0"", ""1"", and ""2"" on a local machine using the config [configs/eval/vitl16_in1k.yaml](configs/eval/vitl16_in1k.yaml), type the command:
```bash
python -m evals.main \
  --fname configs/eval/vitl16_in1k.yaml \
  --devices cuda:0 cuda:1 cuda:2
```


### Distributed training
To launch a distributed evaluation run, the implementation starts from [eval/main_distributed.py](eval/main_distributed.py), which, in addition to parsing the config file, also allows for specifying details about distributed training. For distributed training, we use the popular open-source [submitit](https://github.com/facebookincubator/submitit) tool and provide examples for a SLURM cluster.

For example, to launch a distributed ImageNet image classification experiment using the config [configs/eval/vitl16_in1k.yaml](configs/eval/vitl16_in1k.yaml), type the command:
```bash
python -m evals.main_distributed \
  --fname configs/eval/vitl16_in1k.yaml \
  --folder $path_to_save_stderr_and_stdout \
  --partition $slurm_partition
```

Similarly, to launch a distributed K400 video classification experiment using the config [configs/eval/vitl16_k400.yaml](configs/eval/vitl16_k400.yaml), type the command:
```bash
python -m evals.main_distributed \
  --fname configs/eval/vitl16_k400.yaml \
  --folder $path_to_save_stderr_and_stdout \
  --partition $slurm_partition
```

---

### Setup

Run:
```bash
conda create -n jepa python=3.9 pip
conda activate jepa
python setup.py install
```

## License
See the [LICENSE](./LICENSE) file for details about the license under which this code is made available.

## Citation
If you find this repository useful in your research, please consider giving a star :star: and a citation
```bibtex
@article{bardes2024revisiting,
  title={Revisiting Feature Prediction for Learning Visual Representations from Video},
  author={Bardes, Adrien and Garrido, Quentin and Ponce, Jean and Rabbat, Michael, and LeCun, Yann and Assran, Mahmoud and Ballas, Nicolas},
  journal={arXiv preprint},
  year={2024}
}
",,,https://www.youtube.com/watch?v=7UkJPwz_N_0,0,,,
2024-02-25,https://github.com/facebookresearch/jepa,https://raw.githubusercontent.com/vvbbnn00/WARP-Clash-API/master/README.md,"The WARP Clash API is a non-commercial project designed for educational and communication purposes, enabling users to utilize WARP+ through subscriptions on clients like Clash and Shadowrocket. It features an automatic WARP+ traffic acquisition every 18 seconds, promising unlimited WARP+ bandwidth and includes an IP optimization tool. The project allows for easy one-click deployment via Docker compose, supports the setting of a personal LicenseKey, and supports various clients. Additionally, the API offers features like IP optimization, automatic WARP+ traffic acquisition through proxies to prevent IP blocking, and random node updates for a unique subscription experience. For deployment, users are advised to install Docker, clone the project, optionally configure a `SECRET_KEY` for public deployments, and then compile and run using docker-compose to enjoy their own private high-speed WARP+ node. The project also includes advanced features like resetting account keys and setting one's own LicenseKey, alongside referencing several open-source projects that contributed to its development.","# WARP Clash API

‰∏≠Êñá | [English](./README_en.md)

> **Warning**
>
> Êú¨È°πÁõÆÊòØÂÆåÂÖ®ÈùûÂïÜ‰∏öÈ°πÁõÆÔºå‰ªÖ‰æõÂ≠¶‰π†‰∫§ÊµÅ‰ΩøÁî®ÔºåËØ∑ÂãøÁî®‰∫éÈùûÊ≥ïÁî®ÈÄîÔºåÂê¶ÂàôÂêéÊûúËá™Ë¥ü„ÄÇ

## ü§î ËøôÊòØ‰ªÄ‰πàÔºü

ËØ•È°πÁõÆÂèØ‰ª•ËÆ©‰Ω†ÈÄöËøáËÆ¢ÈòÖÁöÑÊñπÂºè‰ΩøÁî®`WARP+`ÔºåÊîØÊåÅ`Clash`„ÄÅ`Shadowrocket`Á≠âÂÆ¢Êà∑Á´Ø„ÄÇÈ°πÁõÆÂÜÖÁΩÆ‰∫ÜÂà∑Âèñ`WARP+`
ÊµÅÈáèÁöÑÂäüËÉΩÔºåÂèØ‰ª•ËÆ©‰Ω†ÁöÑ`WARP+`ÊµÅÈáè‰∏çÂÜçÂèóÈôêÂà∂ÔºàÊØè`18`ÁßíÂèØËé∑Âæó`1GB`ÊµÅÈáèÔºâÔºåÂêåÊó∂ÔºåÈÖçÂ§á‰∫Ü`IP`ÈÄâ‰ºòÂäüËÉΩ„ÄÇÊîØÊåÅ`Docker compose`
‰∏ÄÈîÆÈÉ®ÁΩ≤ÔºåÊó†ÈúÄÈ¢ùÂ§ñÊìç‰ΩúÔºåÂç≥ÂèØ‰∫´Âèó‰Ω†Ëá™Â∑±ÁöÑ`WARP+`ÁßÅÊúâÈ´òÈÄüËäÇÁÇπÔºÅ

## üí° ÁâπËâ≤ÂäüËÉΩ

- üíª ÊîØÊåÅ`Clash`„ÄÅ`Surge`„ÄÅ`Shadowrocket`Á≠âÂÆ¢Êà∑Á´Ø
- üîë ÊîØÊåÅËÆæÁΩÆÊÇ®Ëá™Â∑±ÁöÑ`LicenseKey`
- üåè ÊîØÊåÅ`IP`ÈÄâ‰ºò
- üêã ÊîØÊåÅ`Docker compose`‰∏ÄÈîÆÈÉ®ÁΩ≤
- üìï ÂÖ®Ëá™Âä®Âà∑Âèñ`WARP+`ÊµÅÈáèÔºåËØ∑Ê±ÇÁªèËøá‰ª£ÁêÜÔºåÈò≤Â∞Å`IP`
- ‚ùì ÊØèÊ¨°Êõ¥Êñ∞ËÆ¢ÈòÖÈöèÊú∫ËäÇÁÇπÔºåËÆ©‰Ω†‰ΩìÈ™åÊäΩÂç°ÁöÑ‰πêË∂£

## üöÄ Âø´ÈÄü‰∏äÊâã

### 1. ÂÆâË£Ö`Docker`Âíå`Docker compose`

- `Docker`ÂÆâË£ÖÊïôÁ®ãÔºö[https://docs.docker.com/engine/install/](https://docs.docker.com/engine/install/)
- `Docker compose`ÂÆâË£ÖÊïôÁ®ãÔºö[https://docs.docker.com/compose/install/](https://docs.docker.com/compose/install/)

### 2. ‰∏ãËΩΩÈ°πÁõÆ

```bash
git clone https://github.com/vvbbnn00/WARP-Clash-API.git
```

### 3. [ÂèØÈÄâ] ÈÖçÁΩÆ`SECRET_KEY`

Ëã•ÊÇ®ÈúÄË¶ÅÂú®ÂÖ¨ÁΩë‰∏äÈÉ®ÁΩ≤ËØ•È°πÁõÆÔºåÂª∫ËÆÆÊÇ®ÈÖçÁΩÆ`SECRET_KEY`‰∏é`PUBLIC_URL`„ÄÇÂú®È°πÁõÆÁõÆÂΩï‰∏ãÂàõÂª∫`.env.local`Êñá‰ª∂ÔºåÂÜôÂÖ•Â¶Ç‰∏ãÂÜÖÂÆπÔºö

```bash
SECRET_KEY=your_secret_key
```

ÂÖ≥‰∫éÁéØÂ¢ÉÂèòÈáèÁöÑÊõ¥Â§ö‰ø°ÊÅØÔºåËØ∑ÂèÇËÄÉ[ÁéØÂ¢ÉÂèòÈáè](#-ÁéØÂ¢ÉÂèòÈáè)„ÄÇ

### 4. ÁºñËØëÂπ∂ËøêË°å

```bash
docker-compose up -d
```

### 5. Ëé∑ÂèñËÆ¢ÈòÖÈìæÊé•

ËÆøÈóÆ`http://‰Ω†ÁöÑIP:21001`ÔºåËæìÂÖ•`SECRET_KEY`ÔºàËã•Ê≤°ÊúâÈÖçÁΩÆÔºåÂàôÂèØ‰ª•ÁïôÁ©∫ÔºâÔºåÂç≥ÂèØËé∑ÂèñËÆ¢ÈòÖÈìæÊé•„ÄÇ

**üéâ Â§ßÂäüÂëäÊàê**

## üåè ÊâãÂä®IPÈÄâ‰ºò

È°πÁõÆÊú¨Ë∫´ÂåÖÂê´‰∫Ü‰∏Ä‰∏™ÈÄâ‰ºòËøáÁöÑ`IP`ÂàóË°®Ôºå‰ΩÜÊòØÁî±‰∫é`WARP`ÁöÑ`IP`ÊòØÂä®ÊÄÅÁöÑÔºåÊâÄ‰ª•ÂèØËÉΩ‰ºöÂá∫Áé∞`IP`‰∏çÂèØÁî®ÁöÑÊÉÖÂÜµ„ÄÇËã•ÊÇ®ÈúÄË¶ÅÊâãÂä®ÈÄâ‰ºòÔºåÂèØ‰ª•ÈÅµÂæ™‰ª•‰∏ãÊ≠•È™§Ôºö

Ëã•ÊÇ®ÈÄöËøá`docker-compose`ÈÉ®ÁΩ≤ÔºåÂèØ‰ª•Âú®È°πÁõÆÁõÆÂΩï‰∏ãÈÄöËøá‰ª•‰∏ãÂëΩ‰ª§ÊâãÂä®ÊâßË°å`IP`ÈÄâ‰ºòÔºö

```bash
docker-compose exec warp-clash python3 app.py optimize
```

Âê¶ÂàôÔºåÂèØ‰ª•Âú®È°πÁõÆÁõÆÂΩï‰∏ãÊâßË°å‰ª•‰∏ãÂëΩ‰ª§Ôºö

```bash
python3 app.py optimize
```

## üîß ÁéØÂ¢ÉÂèòÈáè

Ê≤°ÈîôÔºåÊÇ®ÂèØ‰ª•ÈÄöËøáÁéØÂ¢ÉÂèòÈáèÊù•ÈÖçÁΩÆËØ•È°πÁõÆÔºåÂú®ÈÖçÁΩÆÊó∂ÔºåÂè™ÈúÄÊñ∞Âª∫‰∏Ä‰∏™`.env.local`Êñá‰ª∂ÔºåÂÜôÂÖ•ÊÇ®ÈúÄË¶ÅÁöÑÁéØÂ¢ÉÂèòÈáèÂç≥ÂèØ„ÄÇ

‰ª•‰∏ãÊòØÂèØÁî®ÁöÑÁéØÂ¢ÉÂèòÈáèÔºö

| ÂèòÈáèÂêç                 | ÈªòËÆ§ÂÄº                               | ËØ¥Êòé                                                                                                         |
|---------------------|-----------------------------------|------------------------------------------------------------------------------------------------------------|
| DELAY_THRESHOLD     | `500`                             | Âª∂ËøüÈòàÂÄºÔºåË∂ÖËøáËØ•ÈòàÂÄºÁöÑ`IP`Â∞ÜË¢´ÂâîÈô§                                                                                        |
| DO_GET_WARP_DATA    | `True`                            | ÊòØÂê¶Âà∑Âèñ`WARP+`ÊµÅÈáèÔºåËã•‰∏çÈúÄË¶ÅÂà∑ÂèñÊµÅÈáèÔºåÂàôËÆæÁΩÆ‰∏∫`False`Âç≥ÂèØ                                                                       |
| LOSS_THRESHOLD      | `10`                              | ‰∏¢ÂåÖÁéáÈòàÂÄºÔºåË∂ÖËøáËØ•ÈòàÂÄºÁöÑ`IP`Â∞ÜË¢´ÂâîÈô§                                                                                       |
| PROXY_POOL_URL      | `https://getproxy.bzpl.tech/get/` | IP‰ª£ÁêÜÊ±†Âú∞ÂùÄÔºåÁî®‰∫éÂà∑Âèñ`WARP+`ÊµÅÈáèÔºåÊÇ®ÂèØ‰ª•Ëá™Ë°åÊê≠Âª∫ÔºåÂèÇÁÖß[proxy_pool](https://github.com/jhao104/proxy_pool)                        |
| PUBLIC_URL          | `Êó†`                               | ÈÉ®ÁΩ≤Âú®ÂÖ¨ÁΩë‰∏äÊó∂ÔºåÂ°´ÂÜôÂÖ¨ÁΩë`IP`ÊàñÂüüÂêçÔºåÁî®‰∫éÁîüÊàêËÆ¢ÈòÖÈìæÊé•ÔºåÊØîÂ¶Ç `https://subs.zeabur.app`                                                  |
| RANDOM_COUNT        | `10`                              | ÊØèÊ¨°Êõ¥Êñ∞ËÆ¢ÈòÖÈöèÊú∫ËäÇÁÇπÁöÑÊï∞Èáè                                                                                              |
| REOPTIMIZE_INTERVAL | `-1`                              | ÈáçÊñ∞ÈÄâ‰ºòÁöÑÊó∂Èó¥Èó¥ÈöîÔºåÂçï‰Ωç‰∏∫ÁßíÔºåËã•Â∞è‰∫éÁ≠â‰∫é0ÔºåÂàô‰∏ç‰ºöÈáçÊñ∞ÈÄâ‰ºòÔºåÂê¶ÂàôÊØèÈöîËØ•Êó∂Èó¥Èó¥Èöî‰ºöÈáçÊñ∞ÈÄâ‰ºò‰∏ÄÊ¨°Ôºå‰∏çÂª∫ËÆÆÈó¥ÈöîËÆæÁΩÆËøáÁü≠„ÄÇ                                                  |                     
| REQUEST_RATE_LIMIT  | `0`                               | ÈôêÂà∂XÁßí‰∏ÄÊ¨°ËØ∑Ê±ÇÔºåËØ•ÂäüËÉΩ‰∏çÂ§™Á®≥ÂÆöÔºåÂª∫ËÆÆ‰∏çË¶ÅÂºÄÂêØ                                                                                    |
| SECRET_KEY          | `Êó†`                               | Áî®‰∫é‰øùÊä§ËÆ¢ÈòÖÈìæÊé•ÔºåËã•‰∏çÈÖçÁΩÆÔºåÂàô‰∏çÈúÄË¶ÅËæìÂÖ•`SECRET_KEY`Âç≥ÂèØËé∑ÂèñËÆ¢ÈòÖÈìæÊé•                                                                   |
| SHARE_SUBSCRIPTION  | `False`                           | Ëã•ÊÇ®ÁöÑÁ´ôÁÇπÊÉ≥Ë¶ÅÂêëÁ§æÂå∫ÂàÜ‰∫´ËÆ¢ÈòÖÔºå‰ΩÜ‰∏çÊÉ≥ËÆ©Ëá™Â∑±ÁöÑË¥¶Êà∑‰ø°ÊÅØË¢´ÂÖ¨ÂºÄÊàñ‰øÆÊîπÔºåÂèØ‰ª•ËÆæÁΩÆ‰∏∫`True`ÔºåÊ≠§Êó∂ÔºåËÆøÈóÆËÆ¢ÈòÖÈìæÊé•Êó∂Ôºå‰∏çÈúÄË¶ÅËæìÂÖ•`SECRET_KEY`Âç≥ÂèØËé∑ÂèñÔºåËÄåÂØπ‰∫éÂÖ∂‰ªñÁöÑÊìç‰ΩúÔºå‰ªçÁÑ∂ÈúÄË¶ÅËæìÂÖ•`SECRET_KEY`„ÄÇ |

## üß∞ ËøõÈò∂Êìç‰Ωú

### ÈáçÁΩÆË¥¶Êà∑ÁöÑ`PublicKey`Âíå`PrivateKey`

È°πÁõÆÊîØÊåÅÊÇ®ÈÄöËøáËØ∑Ê±Ç‰ª•‰∏ãÊé•Âè£Êù•ÈáçÁΩÆ`PublicKey`Âíå`PrivateKey`Ôºö

```bash
curl -X POST http://host:port/api/account/reset_key
```

ÈáçÁΩÆËøáÂêéÔºåÈúÄË¶ÅÈáçÊñ∞Ëé∑ÂèñËÆ¢ÈòÖÂÜÖÂÆπÔºåÂê¶ÂàôÂèØËÉΩÊó†Ê≥ï‰ΩøÁî®„ÄÇ

### ËÆæÁΩÆËá™Â∑±ÁöÑ`LicenseKey`

Ëã•ÊÇ®Â∑≤ÁªèÊã•Êúâ‰∫Ü`WARP+`ÁöÑ`LicenseKey`ÔºåÂèØ‰ª•ÈÄöËøá‰ª•‰∏ãÊé•Âè£Êù•ËÆæÁΩÆÔºö

```bash
curl -X POST http://host:port/api/account/update_license -H ""Content-Type: application/json"" -d ""{\""license_key\"": \""your_license_key\""}""
```

ËØ∑Ê≥®ÊÑèÔºåÂΩìÊÇ®ËÆæÁΩÆ‰∫Ü`LicenseKey`ÂêéÔºåÂÖ∂`PublicKey`Âíå`PrivateKey`Â∞Ü‰ºöË¢´ÈáçÁΩÆÔºåÈúÄË¶ÅÈáçÊñ∞Ëé∑ÂèñËÆ¢ÈòÖÂÜÖÂÆπ„ÄÇ

## üóÇÔ∏è ÂºïÁî®È°πÁõÆ

Êú¨È°πÁõÆÁöÑÂºÄÂèëÂèÇÁÖß‰∫Ü‰ª•‰∏ãÈ°πÁõÆÔºåÊÑüË∞¢Ëøô‰∫õÂºÄÊ∫êÈ°πÁõÆÁöÑ‰ΩúËÄÖÔºö

- [warp-script](https://gitlab.com/Misaka-blog/warp-script)
- [warp](https://replit.com/@aliilapro/warp)
- [wgcf](https://github.com/ViRb3/wgcf)
- [proxy_pool](https://github.com/jhao104/proxy_pool)
- [geolite2](https://dev.maxmind.com/geoip/geolite2-free-geolocation-data)

## üë• Á§æÂå∫ÈÉ®ÁΩ≤ÁöÑÂÆû‰æã

- [https://tofree.zeabur.app](https://tofree.zeabur.app)

‰∏Ä‰∏™ÂÖçË¥πÁöÑ WARP Clash API ÂÆû‰æãÔºåÁî±Á§æÂå∫ÈÉ®ÁΩ≤„ÄÇ",,,,0,,,
2024-02-25,https://github.com/vvbbnn00/WARP-Clash-API,https://raw.githubusercontent.com/ndleah/python-mini-project/main/README.md,"The text introduces ""Python Mini Projects,"" a repository on GitHub by ndleah designed to aid beginners in enhancing their Python programming skills through a collection of small projects. It motivates contributions and improvements from both novices and experts, aiming to foster a collaborative learning environment. Steps for contributing include starring the repo, forking it, cloning the fork to one's machine, creating a feature branch, and submitting pull requests. The repository features an array of projects, from simple games like Dice Rolling Simulator and Tic Tac Toe to practical utilities like a Smart Calculator and Email Slicer, showcasing the diversity and practicality of Python. Additionally, contributors are recognized and feedback is encouraged through LinkedIn and GitHub, emphasizing community and continuous development.","![Star Badge](https://img.shields.io/static/v1?label=%F0%9F%8C%9F&message=If%20Useful&style=style=flat&color=BC4E99)
![Open Source Love](https://badges.frapsoft.com/os/v1/open-source.svg?v=103)
[![View My Profile](https://img.shields.io/badge/View-My_Profile-green?logo=GitHub)](https://github.com/ndleah)
[![View Repositories](https://img.shields.io/badge/View-My_Repositories-blue?logo=GitHub)](https://github.com/ndleah?tab=repositories)

[![forthebadge](https://forthebadge.com/images/badges/powered-by-coffee.svg)](https://forthebadge.com)
[![forthebadge](https://forthebadge.com/images/badges/built-with-love.svg)](https://forthebadge.com)
[![forthebadge](https://forthebadge.com/images/badges/powered-by-black-magic.svg)](https://forthebadge.com)
[![forthebadge](https://forthebadge.com/images/badges/made-with-python.svg)](https://forthebadge.com)

# Python Mini Projects <img src=""https://i.pinimg.com/originals/d8/5d/f0/d85df08df1212c0f8b219e779c5ebc46.gif"" align=""right"" width=""120"" />

> A collection of easy Python small projects to help you improve your programming skills.

![Issues](https://img.shields.io/github/issues/ndleah/python-mini-project?style=social&logo=github)
![Pull Requests](https://img.shields.io/github/issues-pr/ndleah/python-mini-project?style=social&logo=github)
![Forks](https://img.shields.io/github/forks/ndleah/python-mini-project?style=social&logo=github)
![Stars](https://img.shields.io/github/stars/ndleah/python-mini-project?style=social&logo=github)
[![License](https://img.shields.io/github/license/ndleah/python-mini-project?style=social&logo=github)](https://github.com/ndleah/python-mini-project/blob/main/LICENSE)

<!-- omit in toc -->

## Table Of Contents

- [Aim Of The Project](#-aim-of-the-project)
- [Contributing](#-contributing)
- [README Template for scripts](#-readme-template-for-scripts)
- [Projects](#-projects)
- [Feedback](#-feedback)

## ![image](IMG/aiming.svg) Aim Of The Project

As a Python newbie, I understand the problems that people face when they first begin studying and attempting to understand various Data Science concepts, particularly Python. This project is designed for folks who are just getting started with Python principles and exploring GitHub as ""contributors.""

My goal is to build a common playground where everyone, from beginners to experts, can learn and share knowledge, and I hope you enjoy your stay here!

Let's ""folk-ing"" create amazing things together! üëâ

## ![image](IMG/game-ps.svg) Contributing

<details>
<summary>
Step 1: Star The Repo
</summary>

Star the repo by pressing the topmost-right button to start your wonderful journey

![star repo](https://docs.github.com/assets/images/help/stars/starring-a-repository.png)

</details>

---

<details>
<summary>
Step 2: Fork it
</summary>

On the [GitHub page for this repository](https://github.com/ndleah/python-mini-project), click on the Button ""**Fork**"".

![fork image](https://upload.wikimedia.org/wikipedia/commons/3/38/GitHub_Fork_Button.png)

</details>

---

<details>
<summary>
Step 3: Clone it
</summary>

- **Method 1:** GitHub Desktop

> ‚ö†Ô∏è **NOTE:** If you're not familiar with Git, using **GitHub Desktop Application** is a better start. If you choose this method, make sure to download it before continuing reading.
>
> ‚ùó‚ùó Access link to download [**here**](https://desktop.github.com).

Learn more about how to clone the remote respository on your local machine using **GitHub Desktop** [here](https://docs.github.com/en/desktop/contributing-and-collaborating-using-github-desktop/adding-and-cloning-repositories/cloning-and-forking-repositories-from-github-desktop#cloning-a-repository).

- **Method 2:** Git

Clone the forked repository. Open git bash and type:

```bash
git clone https://github.com/<your-github-username>/python-mini-project.git
```

> This makes a local copy of the repository in your machine.
>
> ‚ö†Ô∏è **Replace \<your-github-username\>!**

Learn more about [forking](https://help.github.com/en/github/getting-started-with-github/fork-a-repo) and [cloning a repo](https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/cloning-a-repository).

</details>

---

<details>
<summary>
Step 4: Create your feature branch 
</summary>

Always keep your local copy of the repository updated with the original repository.
Before making any changes and/or in an appropriate interval, follow the following steps:

- **Method 1:** GitHub Desktop

Learn more about how to create new branch [here](https://docs.github.com/en/desktop/contributing-and-collaborating-using-github-desktop/making-changes-in-a-branch/managing-branches#creating-a-branch) and how to fetch and pull origin from/to your local machine [here](https://docs.github.com/en/desktop/contributing-and-collaborating-using-github-desktop/keeping-your-local-repository-in-sync-with-github/syncing-your-branch).

Learn more about how to fetch and pull origin from/to your local machine using **GitHub Desktop** [here](https://docs.github.com/en/desktop/contributing-and-collaborating-using-github-desktop/keeping-your-local-repository-in-sync-with-github/syncing-your-branch).

- **Method 2:** Git

Run the following commands **_carefully_** to update your local repository

```sh
# If you cloned a while ago, get the latest changes from upstream
git checkout <master>
git pull upstream <master>

# Make a feature branch (Always check your current branch is up to date before creating a new branch from it to avoid merge conflicts)
git checkout -b <branch-name>

#
```

</details>

---

<details>
<summary>
Step 5: Ready, Set, Go...
</summary>

Once you have completed these steps, you are ready to start contributing to the project and creating **pull requests**.

- Create a folder in
  [projects directory](https://github.com/ndleah/python-mini-project) according to your project name.
  > The folder name should follow the following format ""Your_Project_Name_Here"". For example: Dice_Stimulator
- Write your code and add to the respective folder in the projects directory, locally.
- Don't forget to add a `README.md` in your folder, according to the
  [README_TEMPLATE.](https://github.com/Python-World/python-mini-projects/blob/master/README_TEMPLATE.md)

* **Method 1:** GitHub Desktop

Learn more how to pull request from your local machine using **GitHub Desktop** to the main repo [here](https://docs.github.com/en/desktop/contributing-and-collaborating-using-github-desktop/working-with-your-remote-repository-on-github-or-github-enterprise/viewing-a-pull-request-in-github-desktop).

- **Method 2:** Git

Add the changes with `git add`, `git commit`:

```bash
git add -A
git commit -m ""<your message>""
```

Push the code _to your repository_.

```bash
git push origin <branch-name>
```

</details>

---

<details>
<summary>
Step 6: Pull Request
</summary>

Go to the GitHub page of _your fork_, and **make a pull request**:

![pull request image](https://i.ytimg.com/vi/rgbCcBNZcdQ/maxresdefault.jpg)

Read more about pull requests on the [GitHub help pages](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/creating-a-pull-request).

Now wait, until _your Pull Request_ is approved! If there are any conflicts, you will get a notification.

</details>

<br>

## ![image](IMG/bookmark.svg) README Template for scripts

please make sure to add a `README.md` file that follow the same construction as this template for consistency.

[README Template](https://github.com/ndleah/python-mini-project/blob/master/README_TEMPLATE.md)

## ![image](IMG/like.svg) Projects

| SR No | Project                                                                                                               | Description                                                                                                                                                                                                                                                                                              | Author                                                     |
| ----- | --------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------- |
| 1     | [Dice Rolling Stimulator](https://github.com/ndleah/python-mini-project/tree/main/Dice_Rolling_Stimulator)            | This is a simple dice stimulator made using Python.                                                                                                                                                                                                                                                      | [Leah Nguyen](https://github.com/ndleah)                   |
| 2     | [Dictionary](https://github.com/ndleah/python-mini-project/tree/main/Dictionary)                                      | A dictionary stimulator by Python in which you can enter any words and you will get the definition of it as the output.                                                                                                                                                                                  | [Leah Nguyen](https://github.com/ndleah)                   |
| 3     | [Hangman Game](https://github.com/ndleah/python-mini-project/tree/main/Hangman_Game)                                  | A hangman game stimulator using Python in which the player have 10 attempts to guess the phrase before the men is hung.                                                                                                                                                                                  | [Leah Nguyen](https://github.com/ndleah)                   |
| 4     | [Tic Tac Toe](https://github.com/ndleah/python-mini-project/tree/main/Tic_Tac_Toe)                                    | A simple game of tic tac toe, built in python.                                                                                                                                                                                                                                                           | [Leah Nguyen](https://github.com/ndleah)                   |
| 5     | [Plotter](https://github.com/ndleah/python-mini-project/tree/main/Plotter)                                            | An automation program to plot data with different visualisations by user selections.                                                                                                                                                                                                                     | [Leah Nguyen](https://github.com/ndleah)                   |
| 6     | [Geographical Plot Using Folium](https://github.com/ndleah/python-mini-project/tree/main/Geo_Plot_Using_Folium)       | Using Folium library to create different map data visualization.                                                                                                                                                                                                                                         | [Leah Nguyen](https://github.com/ndleah)                   |
| 7     | [Caterpillar Game](https://github.com/ndleah/python-mini-project/tree/main/Caterpillar_Game)                          | A simple Caterpillar game built in python.                                                                                                                                                                                                                                                               | [Leah Nguyen](https://github.com/ndleah)                   |
| 8     | [Matchmaker Game](https://github.com/ndleah/python-mini-project/tree/main/Matchmaker)                                 | A simple Matchmaker game built by using python.                                                                                                                                                                                                                                                          | [Leah Nguyen](https://github.com/ndleah)                   |
| 9     | [Smart Calculator](https://github.com/ndleah/python-mini-project/tree/main/Smart_Calculator)                          | A smart calculator using for basic math equations, built by using python.                                                                                                                                                                                                                                | [Leah Nguyen](https://github.com/ndleah)                   |
| 10    | [Screenpet](https://github.com/ndleah/python-mini-project/tree/main/Screenpet)                                        | A cute screenpet having different reactions when interact on desktop.                                                                                                                                                                                                                                    | [Leah Nguyen](https://github.com/ndleah)                   |
| 11    | [Egg Catcher](https://github.com/ndleah/python-mini-project/tree/main/Egg_Catcher)                                    | Egg catcher game built in using Python.                                                                                                                                                                                                                                                                  | [Leah Nguyen](https://github.com/ndleah)                   |
| 12    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Number%20Guessing"">Number Guessing                   | Number Guessing Game                                                                                                                                                                                                                                                                                     | [Shruti Solani](https://github.com/ShrutiSolani)           |
| 13    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Madlibs"">Mad Libs                                    | Mad Libs Game                                                                                                                                                                                                                                                                                            | [Shruti Solani](https://github.com/ShrutiSolani)           |
| 14    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Caesar_Cipher"">Caesar Cipher                         | Simple Caesar Cipher encryptor and decryptor bulit with python                                                                                                                                                                                                                                           | [FH089](https://github.com/FH089)                          |
| 15    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Email%20Slicer"">Email Slicer                         | Email Slicer                                                                                                                                                                                                                                                                                             | [Shruti Solani](https://github.com/ShrutiSolani)           |
| 16    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Speaking_Dictionary"">Speaking Dictionary             | Python program that allows the user to find the meaning of an English word by speaking it directly to the device                                                                                                                                                                                         | [19lyaejin](https://github.com/19lyaejin)                  |
| 17    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Cat_command"">Cat Command                             | this project is a basic implementation of the linux cat command                                                                                                                                                                                                                                          | [Alexander Monterrosa](https://github.com/Alex108-lab)     |
| 18    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Sqlite-crud"">Sqlite-crud                             | A simple crud implemented in python using sqlite.                                                                                                                                                                                                                                                        | [Alexander Monterrosa](https://github.com/Alex108-lab)     |
| 19    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Binary_tree"">Binary Tree                             | Implementation of a binary tree in python                                                                                                                                                                                                                                                                | [Alexander Monterrosa](https://github.com/Alex108-lab)     |
| 20    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Socket_example"">Socket                               | Implementation of a socket in python                                                                                                                                                                                                                                                                     | [Alexander Monterrosa](https://github.com/Alex108-lab)     |
| 21    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Stack_structure"">Stack Structure                     | Implementation of a stack structure in python                                                                                                                                                                                                                                                            | [Alexander Monterrosa](https://github.com/Alex108-lab)     |
| 22    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Math_Game"">Math Game                                 | It's just a simple math game. Improve your math skills                                                                                                                                                                                                                                                   | [Pargorn Ruasijan (xNewz)](https://github.com/xNewz)       |
| 23    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Password%20Generator"">Password Generator             | Create secure passwords that are impossible to crack.                                                                                                                                                                                                                                                    | [Pargorn Ruasijan (xNewz)](https://github.com/xNewz)       |
| 24    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Demerge_pdfs"">Demerging PDF                          | Python program to convert a large pdf file to number of different sized pdf files without any change in the large file.                                                                                                                                                                                  | [Darpan-Balar](https://github.com/Darpan-Balar)            |
| 25    | <a href=""https://github.com/vivekthedev/python-mini-project/tree/main/QR%20Code%20Genrator"">QR Code Generator         | GUI with Tkinter to get convert text to a PNG QR Code.                                                                                                                                                                                                                                                   | [Vivek Kumar Singh](https://github.com/vivekthedev)        |
| 26    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Crud_in_flask"">Flask Crud                            | Crud using flask and sqlite3                                                                                                                                                                                                                                                                             | [Alexander Monterrosa](https://github.com/Alex108-lab)     |
| 27    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Sudoku_solver"">Sudoku solver                         | This program can generate and solve Sudoku boards.                                                                                                                                                                                                                                                       | [Dominik Meurer](https://github.com/DMeurer)               |
| 28    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Mail_Checker"">Mail Checker                           | Mail-Checker is a python script that lets you read your gmail subjects from particular gmail accounts directly from the terminal without having to login each time!                                                                                                                                      | [Siddharth Pradeep](https://github.com/thirt33n)           |
| 29    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Whatsapp_Bot"">Whatsapp Bot                           | Whatsapp Bot is a simple bot made using Python to send a WhatsApp message.                                                                                                                                                                                                                               | [Anish Lohiya](https://github.com/AnishLohiya)             |
| 30    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Youtube_video_download"">YouTube Video Downloader     | YouTube Video Downloader lets you download videos from YouTube.                                                                                                                                                                                                                                          | [Alexander Monterrosa](https://github.com/Alex108-lab)     |
| 31    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Face_Recognition"">Face Recognition                   | A Face Recognition Project developed using OpenCV Module in Python that displays a Blue Reactangle Frame around Faces.                                                                                                                                                                                   | [Anish Lohiya](https://github.com/AnishLohiya)             |
| 32    | <a href=""https://github.com/vivekthedev/python-mini-project/tree/main/Slideshare%20to%20PDF"">Slideshare to PDF        | Download any presentation from slideshare to a PDF form without any signup or login                                                                                                                                                                                                                      | [Vivek](https://github.com/vivekthedev)                    |
| 33    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Rock_Paper_Scissors_Spock"">Rock Paper Scissors Spock | Rock Paper Scissors Spock has extra steps to it which add a little spice and creativity over the generic Rock Paper Scissors game we all know and love. The player gets to choose between Rock, Paper, Scissor, Lizard or Spock. If they choose correctly, then the player wins. Have fun and good luck! | [Anokh1](https://github.com/Anokh1)                        |
| 34    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Port%20Scanner"">Port Scanner                         | It's a simple port scanner.                                                                                                                                                                                                                                                                              | [AniYengibaryan](https://github.com/AniYengibaryan)        |
| 35    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/TEXTVENTURE"">TextVenture                             | TextVenture is a short for Text Adventure. It is a game where you can play as a character and explore a world.                                                                                                                                                                                           | [RAO.exe](https://github.com/RAOexe)                       |
| 36    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Lazy_Pong"">Lazy pong                                 | This is a very simple Pong game made with python. It's so simple it doesnt even keep track of scores                                                                                                                                                                                                     | [Ben-Sicat](https://github.com/Ben-Sicat)                  |
| 37    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Minesweeper_game"">Minesweeper Game                   | The classic Minesweeper game in python.                                                                                                                                                                                                                                                                  | [`Subhadeep Das(Raven1233)`](https://github.com/Raven1233) |
| 38    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Zombie_Game"">Zombie Game                             | Simple Zombie Survival Quiz Game bulit with python                                                                                                                                                                                                                                                       | [jmeyu](https://github.com/jmeyu)                          |
| 39    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Music-Player"">Music Player                           | A simple music player in python which enables you to play, next, back, pause, resume the music                                                                                                                                                                                                           | [mr-shitij](https://github.com/mr-shitij)                  |
| 40    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Wordle_Aid"">Wordle Aid!                              | Use this to give you all the possible options for today's wordle based on the information available.                                                                                                                                                                                                     | [Timmy Churchill](https://github.com/Timmy-Churchill)      |
| 41    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Diff_Utility"">Diff Utility                           | A Diff Utility that takes in two file names as commandline arguments and compares them. The comparison is then shown as the output indication which file has changed where.                                                                                                                              | [Shreyas Sable](https://github.com/KILLinefficiency)       |
| 42    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Automated_Mailing"">Automated Mailing                 | A python script that reads email addresses from CSV file and automatically sends email.                                                                                                                                                                                                                  | [Pradhyuman Arora](https://github.com/pradhyumanarora)     |
| 43    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Chinese_FlashCard"">Chinese Flashcard                 | A tkinter application which displays chinese characters.                                                                                                                                                                                                                  | [CMagnac](https://github.com/CMagnac)    |
| 44    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Finding_Lanes"">Finding Lanes                | Detect road lanes using Python                                                                                                                                                                                                                  | [zmdlw](https://github.com/zmdlw)   |
| 45    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Star_Pyramid"">Star Pyramid Generator               | This is a simple code made with while loop and you can also use for loop fot this.   | <a href= ""https://github.com/hasalaonline"">Hasala Abhilasha    |
| 46    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Clip_Organizer"">Clip Organizer  | This script takes in a directory of video clips, and outputs one video clip featuring all of the input clips. | [Seth Treiman](https://github.com/sethtrei) |
| 47    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/infix_postfix_calculator"">Infix Postfix Calculator  | Infix Postfix Calculator | [xNewz](https://github.com/xNewz) |
| 48    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Speaking_Wikipedia"">Speaking Wikipedia | This tool enables users to convert summaries of Wikipedia pages into a speaking version. | [Yam Timor](https://github.com/yamtimor) |
| 49    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Face_Recognition"">Face Recognition | Face Recognition | [BlockmasterPlayz](https://github.com/BlockmasterPlayz) |
| 50    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Url_Shortener"">URL Shortener | A cli url shortener. | [dongjin2008](https://github.com/dongjin2008) |
| 51    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Simple_dice"">Simple Dice | This is a simple dice rolling application written in Python using the Tkinter library. | [tusuii](https://github.com/tusuii) |
| 52    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Encode_Morse.py"">Encode Morse | A program that allows the user to convert the text he/she inputs into the program into a voice audio that is played in real time.Morse encoder is a tool or program that converts text into Morse code, a system of representing letters and numbers using a combination of dots (.) and dashes (-). Morse Encoder is responsible for encrypting English into Morse code. | <a href=""https://github.com/JohnN310"">John Nguyen |
| 53    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/image_comparator"">Image Comparator | This is a tool for coparing two Images and getting their difference image as output | [Rajit99](https://github.com/Rajit99) |
| 54    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Currency_Converter"">Currency Converter | A program that converts currencies using CLI | [Yehuda Elyasaf](https://github.com/YehudaElyasaf) |
| 55    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/linked_lists"">Linked Lists | Implementation of various data structures using linked list. | [Alen Senson](https://github.com/AlenSenson) |
| 56    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Converter"">Converter | A simple converter app built in python. | [Alen Senson](https://github.com/AlenSenson) |
| 57    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Spinning%20Donut"">Spinning Donut | This project uses the Pygame library to create an animation of a spinning donut on a black background. | [Gideon Ogunbanjo](https://github.com/gideon-ogunbanjo) |
| 58    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Firebase_Authentication_Using_Flask"">Firebase Authentication | This project enables developers to implement secure user authentication features in their Flask applications with ease using Firebase Authentication | [MBSA INFINITY](https://github.com/MBSA-INFINITY) |
| 59    | <a href=""https://github.com/ndleah/python-mini-project/tree/main/Finance_Tracker"">Finance Tracker | A finance tracker application allows the user to keep track of their income and expenses, and visualize their balance through a pie chart.  | [Hina Ota](https://github.com/otahina) |
| 60    | <a href=""https://github.com/ndleah/python-mini-project/spam_bot/"">Spam bot | The Python Script that uses Pyautogui library to create a spam to that can be used to prank you friends  | [Prafull Sonawane](https://github.com/prafuel) |
| 61    | <a href=""https://github.com/ndleah/python-mini-project/lorem_in_python/"">Lorem Generator | The Python Script that used to create random paragraph based on how much you want like number of rows | [Prafull Sonawane](https://github.com/prafuel) |
| 62    | <a href=""https://github.com/ndleah/python-mini-project/minionGame/"">Minion Game | The Terminal playable sub string game that deal with count of vowel and constituent, score will be based on some rules given in scrip file. | [Prafull Sonawane](https://github.com/prafuel) |
| 63    | <a href=""https://github.com/ndleah/python-mini-project/Chess_Game/"">Chess Game | A simple chess game that you can play with your friend | [Prafull Sonawane](https://github.com/prafuel) |
| 64    | <a href=""https://github.com/ndleah/python-mini-project/IP_Tools/"">IP Tools | IP Locater extracts data of an IP address. IP Finder gets the IP of a domain name. | [Mohammad Bazargan](https://github.com/BazarganDev) |
| 65    | <a href=""https://github.com/ndleah/python-mini-project/Othello-Reversi-Game/""> Othello/Reversi | Othello is a board-game played on a 8x8 board, with 64 discs that are black on one side and white on the other.  | [Vikrant Singh Bhadouriya](https://www.github.com/TERNION-1121) |
| 66    | <a href=""https://github.com/ndleah/python-mini-project/Image_compressor/"">Image_compressor | The image resizer takes in an image and reduces it's disk size according to the quality you choose, the compressed image is saved in the folder of the orignal image | [Yuv-glitch](https://www.github.com/Yuv-glitch) |
| 67    | <a href=""https://github.com/ndleah/python-mini-project/csv_to_json/"">CSV To JSON | This script helps to convert a csv file to a json file. | [Rajit Gupta](https://www.github.com/Rajit99) |
| 68    | <a href=""https://github.com/ndleah/python-mini-project/images_to_pdf_converter/"">Images To Pdf Converter | Multiple images in Folder being converted in one pdf | [Nikhil Gupta](https://www.github.com/guptanik9) |
| 69    | <a href=""https://github.com/ndleah/python-mini-project/Download%20Audio/"">Download Audio | This is a python script that downloads audio files directly from youtube videos. | [Muhammad Abdullah]((https://github.com/Muhammad-Abdullah3)) |
| 70    | <a href=""https://github.com/ndleah/python-mini-project/Expense_Tracker/"">Expense Tracker | Expense Tracker is a Python application designed to help users keep track of their daily expenses | [Darshan Patil](https://github.com/darshan8850) |
| 71    | <a href=""https://github.com/ndleah/python-mini-project/Lazy_Pong/"">Lazy Pong | This is a simple implementation of the classic Pong game in Python using the Pygame library | [dar8900](https://github.com/dar8900) |

## ![image](IMG/like.svg) Our Contributors

<a href=""https://github.com/ndleah/python-mini-project/graphs/contributors"">
  <img src=""https://contrib.rocks/image?repo=ndleah/python-mini-project"" />
</a>

## ![image](IMG/muscle.svg) Feedback

If you have any feedback or ideas to improve this project, feel free to contact me via

<a href=""https://www.linkedin.com/in/ndleah/"">
  <img align=""left"" alt=""Reeha's Linkdein"" width=""22px"" src=""https://cdn.jsdelivr.net/npm/simple-icons@v3/icons/linkedin.svg"" />

</a>
<a href=""https://github.com/ndleah"">
  <img align=""left"" alt=""Reeha's Github"" width=""22px"" src=""https://cdn.jsdelivr.net/npm/simple-icons@v3/icons/github.svg"" />
</a>
",,https://docs.github.com/assets/images/help/stars/starring-a-repository.png; https://upload.wikimedia.org/wikipedia/commons/3/38/GitHub_Fork_Button.png; https://i.ytimg.com/vi/rgbCcBNZcdQ/maxresdefault.jpg,,0,https://docs.github.com/assets/images/help/stars/starring-a-repository.png; https://upload.wikimedia.org/wikipedia/commons/3/38/GitHub_Fork_Button.png; https://i.ytimg.com/vi/rgbCcBNZcdQ/maxresdefault.jpg,,
2024-02-25,https://github.com/ndleah/python-mini-project,https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/README.md,"The Stable Diffusion web UI is a browser interface designed for Stable Diffusion, utilizing the Gradio library. It features original txt2img and img2img modes, one-click installation (with prerequisites), and advanced functionalities including outpainting, inpainting, upscaling, and attention manipulation for text prompts. The UI supports various neural network integrations for image enhancement, live preview of image generation, customization of UI elements, and no limit on prompt tokens. It offers extensive model and script support, including Stable Diffusion 2.0, Alt-Diffusion, and tools for specific aesthetic generation. Installation guidance is provided for different systems, including Windows, Linux, and Apple Silicon. The project encourages community contributions and has detailed documentation on its wiki page. Credits include collaborations and code from multiple sources across the Stable Diffusion ecosystem.","# Stable Diffusion web UI
A browser interface based on Gradio library for Stable Diffusion.

![](screenshot.png)

## Features
[Detailed feature showcase with images](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features):
- Original txt2img and img2img modes
- One click install and run script (but you still must install python and git)
- Outpainting
- Inpainting
- Color Sketch
- Prompt Matrix
- Stable Diffusion Upscale
- Attention, specify parts of text that the model should pay more attention to
    - a man in a `((tuxedo))` - will pay more attention to tuxedo
    - a man in a `(tuxedo:1.21)` - alternative syntax
    - select text and press `Ctrl+Up` or `Ctrl+Down` (or `Command+Up` or `Command+Down` if you're on a MacOS) to automatically adjust attention to selected text (code contributed by anonymous user)
- Loopback, run img2img processing multiple times
- X/Y/Z plot, a way to draw a 3 dimensional plot of images with different parameters
- Textual Inversion
    - have as many embeddings as you want and use any names you like for them
    - use multiple embeddings with different numbers of vectors per token
    - works with half precision floating point numbers
    - train embeddings on 8GB (also reports of 6GB working)
- Extras tab with:
    - GFPGAN, neural network that fixes faces
    - CodeFormer, face restoration tool as an alternative to GFPGAN
    - RealESRGAN, neural network upscaler
    - ESRGAN, neural network upscaler with a lot of third party models
    - SwinIR and Swin2SR ([see here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/2092)), neural network upscalers
    - LDSR, Latent diffusion super resolution upscaling
- Resizing aspect ratio options
- Sampling method selection
    - Adjust sampler eta values (noise multiplier)
    - More advanced noise setting options
- Interrupt processing at any time
- 4GB video card support (also reports of 2GB working)
- Correct seeds for batches
- Live prompt token length validation
- Generation parameters
     - parameters you used to generate images are saved with that image
     - in PNG chunks for PNG, in EXIF for JPEG
     - can drag the image to PNG info tab to restore generation parameters and automatically copy them into UI
     - can be disabled in settings
     - drag and drop an image/text-parameters to promptbox
- Read Generation Parameters Button, loads parameters in promptbox to UI
- Settings page
- Running arbitrary python code from UI (must run with `--allow-code` to enable)
- Mouseover hints for most UI elements
- Possible to change defaults/mix/max/step values for UI elements via text config
- Tiling support, a checkbox to create images that can be tiled like textures
- Progress bar and live image generation preview
    - Can use a separate neural network to produce previews with almost none VRAM or compute requirement
- Negative prompt, an extra text field that allows you to list what you don't want to see in generated image
- Styles, a way to save part of prompt and easily apply them via dropdown later
- Variations, a way to generate same image but with tiny differences
- Seed resizing, a way to generate same image but at slightly different resolution
- CLIP interrogator, a button that tries to guess prompt from an image
- Prompt Editing, a way to change prompt mid-generation, say to start making a watermelon and switch to anime girl midway
- Batch Processing, process a group of files using img2img
- Img2img Alternative, reverse Euler method of cross attention control
- Highres Fix, a convenience option to produce high resolution pictures in one click without usual distortions
- Reloading checkpoints on the fly
- Checkpoint Merger, a tab that allows you to merge up to 3 checkpoints into one
- [Custom scripts](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Custom-Scripts) with many extensions from community
- [Composable-Diffusion](https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/), a way to use multiple prompts at once
     - separate prompts using uppercase `AND`
     - also supports weights for prompts: `a cat :1.2 AND a dog AND a penguin :2.2`
- No token limit for prompts (original stable diffusion lets you use up to 75 tokens)
- DeepDanbooru integration, creates danbooru style tags for anime prompts
- [xformers](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Xformers), major speed increase for select cards: (add `--xformers` to commandline args)
- via extension: [History tab](https://github.com/yfszzx/stable-diffusion-webui-images-browser): view, direct and delete images conveniently within the UI
- Generate forever option
- Training tab
     - hypernetworks and embeddings options
     - Preprocessing images: cropping, mirroring, autotagging using BLIP or deepdanbooru (for anime)
- Clip skip
- Hypernetworks
- Loras (same as Hypernetworks but more pretty)
- A separate UI where you can choose, with preview, which embeddings, hypernetworks or Loras to add to your prompt 
- Can select to load a different VAE from settings screen
- Estimated completion time in progress bar
- API
- Support for dedicated [inpainting model](https://github.com/runwayml/stable-diffusion#inpainting-with-stable-diffusion) by RunwayML
- via extension: [Aesthetic Gradients](https://github.com/AUTOMATIC1111/stable-diffusion-webui-aesthetic-gradients), a way to generate images with a specific aesthetic by using clip images embeds (implementation of [https://github.com/vicgalle/stable-diffusion-aesthetic-gradients](https://github.com/vicgalle/stable-diffusion-aesthetic-gradients))
- [Stable Diffusion 2.0](https://github.com/Stability-AI/stablediffusion) support - see [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#stable-diffusion-20) for instructions
- [Alt-Diffusion](https://arxiv.org/abs/2211.06679) support - see [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#alt-diffusion) for instructions
- Now without any bad letters!
- Load checkpoints in safetensors format
- Eased resolution restriction: generated image's dimensions must be a multiple of 8 rather than 64
- Now with a license!
- Reorder elements in the UI from settings screen
- [Segmind Stable Diffusion](https://huggingface.co/segmind/SSD-1B) support

## Installation and Running
Make sure the required [dependencies](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Dependencies) are met and follow the instructions available for:
- [NVidia](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-NVidia-GPUs) (recommended)
- [AMD](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-AMD-GPUs) GPUs.
- [Intel CPUs, Intel GPUs (both integrated and discrete)](https://github.com/openvinotoolkit/stable-diffusion-webui/wiki/Installation-on-Intel-Silicon) (external wiki page)

Alternatively, use online services (like Google Colab):

- [List of Online Services](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Online-Services)

### Installation on Windows 10/11 with NVidia-GPUs using release package
1. Download `sd.webui.zip` from [v1.0.0-pre](https://github.com/AUTOMATIC1111/stable-diffusion-webui/releases/tag/v1.0.0-pre) and extract its contents.
2. Run `update.bat`.
3. Run `run.bat`.
> For more details see [Install-and-Run-on-NVidia-GPUs](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-NVidia-GPUs)

### Automatic Installation on Windows
1. Install [Python 3.10.6](https://www.python.org/downloads/release/python-3106/) (Newer version of Python does not support torch), checking ""Add Python to PATH"".
2. Install [git](https://git-scm.com/download/win).
3. Download the stable-diffusion-webui repository, for example by running `git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git`.
4. Run `webui-user.bat` from Windows Explorer as normal, non-administrator, user.

### Automatic Installation on Linux
1. Install the dependencies:
```bash
# Debian-based:
sudo apt install wget git python3 python3-venv libgl1 libglib2.0-0
# Red Hat-based:
sudo dnf install wget git python3 gperftools-libs libglvnd-glx 
# openSUSE-based:
sudo zypper install wget git python3 libtcmalloc4 libglvnd
# Arch-based:
sudo pacman -S wget git python3
```
2. Navigate to the directory you would like the webui to be installed and execute the following command:
```bash
wget -q https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh
```
3. Run `webui.sh`.
4. Check `webui-user.sh` for options.
### Installation on Apple Silicon

Find the instructions [here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Installation-on-Apple-Silicon).

## Contributing
Here's how to add code to this repo: [Contributing](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing)

## Documentation

The documentation was moved from this README over to the project's [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki).

For the purposes of getting Google and other search engines to crawl the wiki, here's a link to the (not for humans) [crawlable wiki](https://github-wiki-see.page/m/AUTOMATIC1111/stable-diffusion-webui/wiki).

## Credits
Licenses for borrowed code can be found in `Settings -> Licenses` screen, and also in `html/licenses.html` file.

- Stable Diffusion - https://github.com/Stability-AI/stablediffusion, https://github.com/CompVis/taming-transformers
- k-diffusion - https://github.com/crowsonkb/k-diffusion.git
- GFPGAN - https://github.com/TencentARC/GFPGAN.git
- CodeFormer - https://github.com/sczhou/CodeFormer
- ESRGAN - https://github.com/xinntao/ESRGAN
- SwinIR - https://github.com/JingyunLiang/SwinIR
- Swin2SR - https://github.com/mv-lab/swin2sr
- LDSR - https://github.com/Hafiidz/latent-diffusion
- MiDaS - https://github.com/isl-org/MiDaS
- Ideas for optimizations - https://github.com/basujindal/stable-diffusion
- Cross Attention layer optimization - Doggettx - https://github.com/Doggettx/stable-diffusion, original idea for prompt editing.
- Cross Attention layer optimization - InvokeAI, lstein - https://github.com/invoke-ai/InvokeAI (originally http://github.com/lstein/stable-diffusion)
- Sub-quadratic Cross Attention layer optimization - Alex Birch (https://github.com/Birch-san/diffusers/pull/1), Amin Rezaei (https://github.com/AminRezaei0x443/memory-efficient-attention)
- Textual Inversion - Rinon Gal - https://github.com/rinongal/textual_inversion (we're not using his code, but we are using his ideas).
- Idea for SD upscale - https://github.com/jquesnelle/txt2imghd
- Noise generation for outpainting mk2 - https://github.com/parlance-zz/g-diffuser-bot
- CLIP interrogator idea and borrowing some code - https://github.com/pharmapsychotic/clip-interrogator
- Idea for Composable Diffusion - https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch
- xformers - https://github.com/facebookresearch/xformers
- DeepDanbooru - interrogator for anime diffusers https://github.com/KichangKim/DeepDanbooru
- Sampling in float32 precision from a float16 UNet - marunine for the idea, Birch-san for the example Diffusers implementation (https://github.com/Birch-san/diffusers-play/tree/92feee6)
- Instruct pix2pix - Tim Brooks (star), Aleksander Holynski (star), Alexei A. Efros (no star) - https://github.com/timothybrooks/instruct-pix2pix
- Security advice - RyotaK
- UniPC sampler - Wenliang Zhao - https://github.com/wl-zhao/UniPC
- TAESD - Ollin Boer Bohan - https://github.com/madebyollin/taesd
- LyCORIS - KohakuBlueleaf
- Restart sampling - lambertae - https://github.com/Newbeeer/diffusion_restart_sampling
- Hypertile - tfernd - https://github.com/tfernd/HyperTile
- Initial Gradio script - posted on 4chan by an Anonymous user. Thank you Anonymous user.
- (You)
",,https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/screenshot.png,,0,https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/screenshot.png,,
2024-02-25,https://github.com/AUTOMATIC1111/stable-diffusion-webui,https://raw.githubusercontent.com/google-deepmind/graphcast/main/README.md,"GraphCast is an innovative approach for medium-range global weather forecasting, detailed in a comprehensive package that includes example code and three pretrained models: the high-resolution GraphCast model trained on ERA5 data (1979-2017), a smaller low-resolution version (GraphCast_small) for reduced memory and compute requirements, and GraphCast_operational, which is fine-tuned on HRES data for operational use without needing precipitation inputs. These models and their components are accessible via a Google Cloud Bucket. To train the models fully, one needs to download the ERA5 dataset from ECMWF. The package also offers various utility and model files, demonstrating its implementation and usage, particularly through the `graphcast_demo.ipynb` Colaboratory notebook. Key dependencies include Chex, Dask, Haiku, JAX, and others. The package and models are available under the Apache License 2.0 and Creative Commons BY-NC-SA 4.0, respectively. It's important to note the copyright and licensing details pertaining to ECMWF's ERA5 and HRES data, as well as the disclaimer clarifying this as an unsupported Google product. The citation for the original GraphCast paper is provided for those leveraging this work.","# GraphCast: Learning skillful medium-range global weather forecasting

This package contains example code to run and train [GraphCast](https://arxiv.org/abs/2212.12794).
It also provides three pretrained models:

1.  `GraphCast`, the high-resolution model used in the GraphCast paper (0.25 degree
resolution, 37 pressure levels), trained on ERA5 data from 1979 to 2017,

2.  `GraphCast_small`, a smaller, low-resolution version of GraphCast (1 degree
resolution, 13 pressure levels, and a smaller mesh), trained on ERA5 data from
1979 to 2015, useful to run a model with lower memory and compute constraints,

3.  `GraphCast_operational`, a high-resolution model (0.25 degree resolution, 13
pressure levels) pre-trained on ERA5 data from 1979 to 2017 and fine-tuned on
HRES data from 2016 to 2021. This model can be initialized from HRES data (does
not require precipitation inputs).

The model weights, normalization statistics, and example inputs are available on [Google Cloud Bucket](https://console.cloud.google.com/storage/browser/dm_graphcast).

Full model training requires downloading the
[ERA5](https://www.ecmwf.int/en/forecasts/datasets/reanalysis-datasets/era5)
dataset, available from [ECMWF](https://www.ecmwf.int/). This can best be
accessed as Zarr from [Weatherbench2's ERA5 data](https://weatherbench2.readthedocs.io/en/latest/data-guide.html#era5) (see the 6h downsampled versions).

## Overview of files

The best starting point is to open `graphcast_demo.ipynb` in [Colaboratory](https://colab.research.google.com/github/deepmind/graphcast/blob/master/graphcast_demo.ipynb), which gives an
example of loading data, generating random weights or load a pre-trained
snapshot, generating predictions, computing the loss and computing gradients.
The one-step implementation of GraphCast architecture, is provided in
`graphcast.py`.

### Brief description of library files:

*   `autoregressive.py`: Wrapper used to run (and train) the one-step GraphCast
    to produce a sequence of predictions by auto-regressively feeding the
    outputs back as inputs at each step, in JAX a differentiable way.
*   `casting.py`: Wrapper used around GraphCast to make it work using
    BFloat16 precision.
*   `checkpoint.py`: Utils to serialize and deserialize trees.
*   `data_utils.py`: Utils for data preprocessing.
*   `deep_typed_graph_net.py`: General purpose deep graph neural network (GNN)
    that operates on `TypedGraph`'s where both inputs and outputs are flat
    vectors of features for each of the nodes and edges. `graphcast.py` uses
    three of these for the Grid2Mesh GNN, the Multi-mesh GNN and the Mesh2Grid
    GNN, respectively.
*   `graphcast.py`: The main GraphCast model architecture for one-step of
    predictions.
*   `grid_mesh_connectivity.py`: Tools for converting between regular grids on a
    sphere and triangular meshes.
*   `icosahedral_mesh.py`: Definition of an icosahedral multi-mesh.
*   `losses.py`: Loss computations, including latitude-weighting.
*   `model_utils.py`: Utilities to produce flat node and edge vector features
    from input grid data, and to manipulate the node output vectors back
    into a multilevel grid data.
*   `normalization.py`: Wrapper for the one-step GraphCast used to normalize
    inputs according to historical values, and targets according to historical
    time differences.
*   `predictor_base.py`: Defines the interface of the predictor, which GraphCast
    and all of the wrappers implement.
*   `rollout.py`: Similar to `autoregressive.py` but used only at inference time
    using a python loop to produce longer, but non-differentiable trajectories.
*   `solar_radiation.py`: Computes Top-Of-the-Atmosphere (TOA) incident solar
    radiation compatible with ERA5. This is used as a forcing variable and thus
    needs to be computed for target lead times in an operational setting.
*   `typed_graph.py`: Definition of `TypedGraph`'s.
*   `typed_graph_net.py`: Implementation of simple graph neural network
    building blocks defined over `TypedGraph`'s that can be combined to build
    deeper models.
*   `xarray_jax.py`: A wrapper to let JAX work with `xarray`s.
*   `xarray_tree.py`: An implementation of tree.map_structure that works with
    `xarray`s.


### Dependencies.

[Chex](https://github.com/deepmind/chex),
[Dask](https://github.com/dask/dask),
[Haiku](https://github.com/deepmind/dm-haiku),
[JAX](https://github.com/google/jax),
[JAXline](https://github.com/deepmind/jaxline),
[Jraph](https://github.com/deepmind/jraph),
[Numpy](https://numpy.org/),
[Pandas](https://pandas.pydata.org/),
[Python](https://www.python.org/),
[SciPy](https://scipy.org/),
[Tree](https://github.com/deepmind/tree),
[Trimesh](https://github.com/mikedh/trimesh) and
[XArray](https://github.com/pydata/xarray).


### License and attribution

The Colab notebook and the associated code are licensed under the Apache
License, Version 2.0. You may obtain a copy of the License at:
https://www.apache.org/licenses/LICENSE-2.0.

The model weights are made available for use under the terms of the Creative
Commons Attribution-NonCommercial-ShareAlike 4.0 International
(CC BY-NC-SA 4.0). You may obtain a copy of the License at:
https://creativecommons.org/licenses/by-nc-sa/4.0/.

The weights were trained on ECMWF's ERA5 and HRES data. The colab includes a few
examples of ERA5 and HRES data that can be used as inputs to the models.
ECMWF data product are subject to the following terms:

1. Copyright statement: Copyright ""¬© 2023 European Centre for Medium-Range Weather Forecasts (ECMWF)"".
2. Source www.ecmwf.int
3. Licence Statement: ECMWF data is published under a Creative Commons Attribution 4.0 International (CC BY 4.0). https://creativecommons.org/licenses/by/4.0/
4. Disclaimer: ECMWF does not accept any liability whatsoever for any error or omission in the data, their availability, or for any loss or damage arising from their use.

### Disclaimer

This is not an officially supported Google product.

Copyright 2023 DeepMind Technologies Limited.

### Citation

If you use this work, consider citing our [paper](https://arxiv.org/abs/2212.12794):

```latex
@article{lam2022graphcast,
      title={GraphCast: Learning skillful medium-range global weather forecasting},
      author={Remi Lam and Alvaro Sanchez-Gonzalez and Matthew Willson and Peter Wirnsberger and Meire Fortunato and Alexander Pritzel and Suman Ravuri and Timo Ewalds and Ferran Alet and Zach Eaton-Rosen and Weihua Hu and Alexander Merose and Stephan Hoyer and George Holland and Jacklynn Stott and Oriol Vinyals and Shakir Mohamed and Peter Battaglia},
      year={2022},
      eprint={2212.12794},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```
",,,,0,,,
2024-02-25,https://github.com/google-deepmind/graphcast,https://raw.githubusercontent.com/keras-team/keras/master/README.md,"Keras 3 is a versatile deep learning framework that supports TensorFlow, JAX, and PyTorch, designed to make machine learning more accessible. It can be installed via pip and requires the corresponding backend package (`tensorflow`, `jax`, or `torch`) for full functionality. Special instructions are provided for local installation, including GPU support. Users can configure their desired backend through an environment variable or a local config file but must do so before importing Keras 3. Keras 3 aims to be backward compatible with `tf.keras`, allowing seamless transitions between frameworks with minimal adjustments. It promotes flexibility, enabling users to utilize their preferred framework's benefits while ensuring their machine learning code is future-proof. This approach also opens up Keras‚Äô extensive feature set to PyTorch and JAX users, offering enhanced modeling and training capabilities that are well-documented and have been battle-tested, encouraging broader adoption and application in various projects.","# Keras 3: Deep Learning for Humans

Keras 3 is a multi-backend deep learning framework, with support for TensorFlow, JAX, and PyTorch.

## Installation

### Install with pip

Keras 3 is available on PyPI as `keras`. Note that Keras 2 remains available as the `tf-keras` package.

1. Install `keras`:

```
pip install keras --upgrade
```

2. Install backend package(s).

To use `keras`, you should also install the backend of choice: `tensorflow`, `jax`, or `torch`.
Note that `tensorflow` is required for using certain Keras 3 features: certain preprocessing layers
as well as `tf.data` pipelines.

### Local installation

#### Minimal installation

Keras 3 is compatible with Linux and MacOS systems. For Windows users, we recommend using WSL2 to run Keras.
To install a local development version:

1. Install dependencies:

```
pip install -r requirements.txt
```

2. Run installation command from the root directory.

```
python pip_build.py --install
```

#### Adding GPU support

The `requirements.txt` file will install a CPU-only version of TensorFlow, JAX, and PyTorch. For GPU support, we also
provide a separate `requirements-{backend}-cuda.txt` for TensorFlow, JAX, and PyTorch. These install all CUDA
dependencies via `pip` and expect a NVIDIA driver to be pre-installed. We recommend a clean python environment for each
backend to avoid CUDA version mismatches. As an example, here is how to create a Jax GPU environment with `conda`:

```shell
conda create -y -n keras-jax python=3.10
conda activate keras-jax
pip install -r requirements-jax-cuda.txt
python pip_build.py --install
```

## Configuring your backend

You can export the environment variable `KERAS_BACKEND` or you can edit your local config file at `~/.keras/keras.json`
to configure your backend. Available backend options are: `""tensorflow""`, `""jax""`, `""torch""`. Example:

```
export KERAS_BACKEND=""jax""
```

In Colab, you can do:

```python
import os
os.environ[""KERAS_BACKEND""] = ""jax""

import keras
```

**Note:** The backend must be configured before importing `keras`, and the backend cannot be changed after 
the package has been imported.

## Backwards compatibility

Keras 3 is intended to work as a drop-in replacement for `tf.keras` (when using the TensorFlow backend). Just take your
existing `tf.keras` code, make sure that your calls to `model.save()` are using the up-to-date `.keras` format, and you're
done.

If your `tf.keras` model does not include custom components, you can start running it on top of JAX or PyTorch immediately.

If it does include custom components (e.g. custom layers or a custom `train_step()`), it is usually possible to convert it
to a backend-agnostic implementation in just a few minutes.

In addition, Keras models can consume datasets in any format, regardless of the backend you're using:
you can train your models with your existing `tf.data.Dataset` pipelines or PyTorch `DataLoaders`.

## Why use Keras 3?

- Run your high-level Keras workflows on top of any framework -- benefiting at will from the advantages of each framework,
e.g. the scalability and performance of JAX or the production ecosystem options of TensorFlow.
- Write custom components (e.g. layers, models, metrics) that you can use in low-level workflows in any framework.
    - You can take a Keras model and train it in a training loop written from scratch in native TF, JAX, or PyTorch.
    - You can take a Keras model and use it as part of a PyTorch-native `Module` or as part of a JAX-native model function.
- Make your ML code future-proof by avoiding framework lock-in.
- As a PyTorch user: get access to power and usability of Keras, at last!
- As a JAX user: get access to a fully-featured, battle-tested, well-documented modeling and training library.


Read more in the [Keras 3 release announcement](https://keras.io/keras_3/).
",,,,0,,,
2024-02-25,https://github.com/keras-team/keras,https://raw.githubusercontent.com/NVIDIA/GenerativeAIExamples/main/README.md,"NVIDIA's Generative AI examples showcase the deployment, testing, and extension of state-of-the-art AI models that operate on NVIDIA's high-performance CUDA-X software stack and GPUs. These examples leverage resources from the NVIDIA NGC AI Development Catalog, requiring a free NGC developer account for access. The examples cover Retrieval Augmented Generation (RAG) for embedding multimodal data into a database for a chat interface with data, using NVIDIA GPU acceleration and popular LLM programming frameworks. Both developer and enterprise versions of RAG examples are provided, with developer versions running on a single VM and enterprise versions distributed across multiple VMs and GPUs, showing the integration with Kubernetes and Helm for orchestration. The documentation also includes resources for LLM development, observability tools, open source connectors for API endpoints, and encourages community feedback and contributions through GitHub. Issues, licenses, and third-party software are noted for user awareness.","# NVIDIA Generative AI Examples

## Introduction
State-of-the-art Generative AI examples that are easy to deploy, test, and extend. All examples run on the high performance NVIDIA CUDA-X software stack and NVIDIA GPUs.

## NVIDIA NGC
Generative AI Examples uses resources from the [NVIDIA NGC AI Development Catalog](https://ngc.nvidia.com).

Sign up for a [free NGC developer account](https://ngc.nvidia.com/signin) to access:

- GPU-optimized containers used in these examples
- Release notes and developer documentation

## Retrieval Augmented Generation (RAG)

A RAG pipeline embeds multimodal data --  such as documents, images, and video -- into a database connected to a LLM.  RAG lets users chat with their data!

### Developer RAG Examples

The developer RAG examples run on a single VM. They demonstrate how to combine NVIDIA GPU acceleration with popular LLM programming frameworks using NVIDIA's [open source connectors](#open-source-integrations). The examples are easy to deploy via [Docker Compose](https://docs.docker.com/compose/).

Examples support local and remote inference endpoints. If you have a GPU, you can inference locally via [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM). If you don't have a GPU, you can inference and embed remotely via [NVIDIA AI Foundations endpoints](https://www.nvidia.com/en-us/ai-data-science/foundation-models/).

| Model         | Embedding           | Framework        | Description               | Multi-GPU | TRT-LLM | NVIDIA AI Foundation | Triton | Vector Database |
|---------------|-----------------------|------------|-------------------------|-----------|------------|-------------|---------|--------|
| llama-2 | e5-large-v2 | Llamaindex | Canonical QA Chatbot | [YES](RetrievalAugmentedGeneration/README.md#3-qa-chatbot-multi-gpu----a100h100l40s)        | [YES](RetrievalAugmentedGeneration/README.md#2-qa-chatbot----a100h100l40s-gpu)       | No | YES     | Milvus/[PGVector]((RetrievalAugmentedGeneration/README.md#2-qa-chatbot----a100h100l40s-gpu))|
| mixtral_8x7b | nvolveqa_40k | Langchain | [Nvidia AI foundation based QA Chatbot](RetrievalAugmentedGeneration/README.md#1-qa-chatbot----nvidia-ai-foundation-inference-endpoint)  | No        | No       | YES | YES     | FAISS|
| llama-2 | all-MiniLM-L6-v2 | Llama Index | [QA Chatbot, GeForce, Windows](https://github.com/NVIDIA/trt-llm-rag-windows/tree/release/1.0)  | NO        | YES        | NO | NO     | FAISS |
| llama-2 | nvolveqa_40k | Langchain | [QA Chatbot, Task Decomposition Agent](./RetrievalAugmentedGeneration/README.md#5-qa-chatbot-with-task-decomposition-example----a100h100l40s) | No | No | YES | YES | FAISS
| mixtral_8x7b | nvolveqa_40k | Langchain | [Minimilastic example showcasing RAG using Nvidia AI foundation models](./examples/README.md#rag-in-5-minutes-example)  | No        | No       | YES | YES     | FAISS|



### Enterprise RAG Examples

The enterprise RAG examples run as microservies distributed across multiple VMs and GPUs. They show how RAG pipelines can be orchestrated with [Kubernetes](https://kubernetes.io/) and deployed with [Helm](https://helm.sh/).

Enterprise RAG examples include a [Kubernetes operator](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/) for LLM lifecycle management. It is compatible with the [NVIDIA GPU operator](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/gpu-operator) that automates GPU discovery and lifecycle management in a Kubernetes cluster.

Enterprise RAG examples also support local and remote inference via [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) and [NVIDIA AI Foundations endpoints](https://www.nvidia.com/en-us/ai-data-science/foundation-models/).

| Model         | Embedding           | Framework        | Description               | Multi-GPU | Multi-node | TRT-LLM | NVIDIA AI Foundation | Triton | Vector Database |
|---------------|-----------------------|------------|--------|-------------------------|-----------|------------|-------------|---------|--------|
| llama-2 | NV-Embed-QA-003 | Llamaindex | QA Chatbot, Helm, k8s  | NO        | NO | [YES](./docs/developer-llm-operator/)         | NO | YES     | Milvus|

## Tools

Example tools and tutorials to enhance LLM development and productivity when using NVIDIA RAG pipelines.

| Name | Description | Deployment | Tutorial |
|------|-------------|------|--------|
| Evaluation | Example open source RAG eval tool that uses synthetic data generation and LLM-as-a-judge |  [Docker compose file](./deploy/compose/docker-compose-evaluation.yaml) | [README](./docs/rag/evaluation.md) |]
| Observability | Observability serves as an efficient mechanism for both monitoring and debugging RAG pipelines. |  [Docker compose file](./deploy/compose/docker-compose-observability.yaml) | [README](./docs/rag/observability.md) |]

## Open Source Integrations

These are open source connectors for NVIDIA-hosted and self-hosted API endpoints. These open source connectors are maintained and tested by NVIDIA engineers.

| Name | Framework | Chat | Text Embedding | Python | Description |
|------|-----------|------|-----------|--------|-------------|
|[NVIDIA AI Foundation Endpoints](https://python.langchain.com/docs/integrations/providers/nvidia) | [Langchain](https://www.langchain.com/) |[YES](https://python.langchain.com/docs/integrations/chat/nvidia_ai_endpoints)|[YES](https://python.langchain.com/docs/integrations/text_embedding/nvidia_ai_endpoints)|[YES](https://pypi.org/project/langchain-nvidia-ai-endpoints/)|Easy access to NVIDIA hosted models. Supports chat, embedding, code generation, steerLM, multimodal, and RAG.|
|[NVIDIA Triton + TensorRT-LLM](https://github.com/langchain-ai/langchain/tree/master/libs/partners/nvidia-trt) | [Langchain](https://www.langchain.com/) |[YES](https://github.com/langchain-ai/langchain/blob/master/libs/partners/nvidia-trt/docs/llms.ipynb)|[YES](https://github.com/langchain-ai/langchain/blob/master/libs/partners/nvidia-trt/docs/llms.ipynb)|[YES](https://pypi.org/project/langchain-nvidia-trt/)|This connector allows Langchain to remotely interact with a Triton inference server over GRPC or HTTP tfor optimized LLM inference.|
|[NVIDIA Triton Inference Server](https://docs.llamaindex.ai/en/stable/examples/llm/nvidia_triton.html) | [LlamaIndex](https://www.llamaindex.ai/) |YES|YES|NO|Triton inference server provides API access to hosted LLM models over gRPC. |
|[NVIDIA TensorRT-LLM](https://docs.llamaindex.ai/en/stable/examples/llm/nvidia_tensorrt.html) | [LlamaIndex](https://www.llamaindex.ai/) |YES|YES|NO|TensorRT-LLM provides a Python API to build TensorRT engines with state-of-the-art optimizations for LLM inference on NVIDIA GPUs. |


## NVIDIA support
In each example README we indicate the level of support provided.

## Feedback / Contributions
We're posting these examples on GitHub to support the NVIDIA LLM community, facilitate feedback. We invite contributions via GitHub Issues or pull requests!

## Known issues
- In each of the READMEs, we indicate any known issues and encourage the community to provide feedback.
- The datasets provided as part of this project is under a different license for research and evaluation purposes.
- This project will download and install additional third-party open source software projects. Review the license terms of these open source projects before use.
",,,,0,,,
